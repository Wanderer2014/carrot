{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import torch as torch\n",
    "import torchvision\n",
    "import mongoengine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transforms_image = transforms.Compose([transforms.Resize(32),\n",
    "                                     transforms.CenterCrop(32),\n",
    "                                     transforms.ToTensor()])\n",
    "train_xray = torch.utils.data.DataLoader(datasets.ImageFolder('chest_xray/train', \n",
    "                                                                            transform=transforms_image),\n",
    "                                                        batch_size=20, shuffle=True)\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_xray)\n",
    "images, _ = dataiter.next() # _ for no labels\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {'train': transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])]),\n",
    "                   'valid': transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])]),\n",
    "                   'test': transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(), \n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])])\n",
    "                  }\n",
    "\n",
    "loaders = {'train': torch.utils.data.DataLoader(datasets.ImageFolder('chest_xray/train', \n",
    "                                                                            transform=image_transforms['train']),\n",
    "                                                        batch_size=128, shuffle=True),                    \n",
    "                   'valid': torch.utils.data.DataLoader(datasets.ImageFolder('chest_xray/val', \n",
    "                                                                             transform=image_transforms['valid']),\n",
    "                                                        batch_size=128, shuffle=True), \n",
    "                   'test': torch.utils.data.DataLoader(datasets.ImageFolder('chest_xray/test', \n",
    "                                                                            transform=image_transforms['test']),\n",
    "                                                        batch_size=128, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.c1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.c2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.c3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(in_features=6272, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=412)\n",
    "        self.fc4 = nn.Linear(in_features=412, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = F.relu(F.max_pool2d(self.c1(x), 3))\n",
    "        x = F.relu(F.max_pool2d(self.c2(x), 3))\n",
    "        x = F.relu(F.max_pool2d(self.c3(x), 3))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (c1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (c2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (c3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=6272, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=412, bias=True)\n",
       "  (fc4): Linear(in_features=412, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongoengine.connect('pytorchboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(mongoengine.Document):\n",
    "    date_time = mongoengine.DateTimeField(required=True)\n",
    "    epoch = mongoengine.IntField()\n",
    "    train_loss = mongoengine.FloatField()\n",
    "    test_loss = mongoengine.FloatField()\n",
    "    train_accuracy = mongoengine.FloatField()\n",
    "    test_accuracy = mongoengine.FloatField()\n",
    "    gradients = mongoengine.BinaryField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.690815269947052\n",
      "Loss:  0.6600399017333984\n",
      "Loss:  0.6058178544044495\n",
      "Loss:  0.5687176585197449\n",
      "Loss:  0.5879955291748047\n",
      "Loss:  0.6990935206413269\n",
      "Loss:  0.6121244430541992\n",
      "Loss:  0.5873472094535828\n",
      "Loss:  0.5841731429100037\n",
      "Loss:  0.6099420189857483\n",
      "Loss:  0.6115288734436035\n",
      "Loss:  0.5610635280609131\n",
      "Loss:  0.5293806791305542\n",
      "Loss:  0.5181272029876709\n",
      "Loss:  0.4222904145717621\n",
      "Loss:  0.4986377954483032\n",
      "Loss:  0.49659264087677\n",
      "Loss:  0.35898709297180176\n",
      "Loss:  0.34549081325531006\n",
      "Loss:  0.2923855781555176\n",
      "Loss:  0.2591489255428314\n",
      "Loss:  0.3127569854259491\n",
      "Loss:  0.27009105682373047\n",
      "Loss:  0.31707146763801575\n",
      "Loss:  0.10536050796508789\n",
      "Loss:  0.1279219388961792\n",
      "Loss:  0.1831696778535843\n",
      "Loss:  0.1586543172597885\n",
      "Loss:  0.17579516768455505\n",
      "Loss:  0.19460196793079376\n",
      "Loss:  0.09793245047330856\n",
      "Loss:  0.3241034746170044\n",
      "Loss:  0.11397755891084671\n",
      "Loss:  0.11833880096673965\n",
      "Loss:  0.24868477880954742\n",
      "Loss:  0.2993070185184479\n",
      "Loss:  0.12104088068008423\n",
      "Loss:  0.27452850341796875\n",
      "Loss:  0.16466499865055084\n",
      "Loss:  0.18589386343955994\n",
      "Loss:  0.17172634601593018\n",
      "Epoch:  1\n",
      "Loss:  0.21578475832939148\n",
      "Loss:  0.11284991353750229\n",
      "Loss:  0.20891834795475006\n",
      "Loss:  0.2806216776371002\n",
      "Loss:  0.21416433155536652\n",
      "Loss:  0.17794068157672882\n",
      "Loss:  0.20446155965328217\n",
      "Loss:  0.24797303974628448\n",
      "Loss:  0.19615282118320465\n",
      "Loss:  0.17109712958335876\n",
      "Loss:  0.21097244322299957\n",
      "Loss:  0.22184301912784576\n",
      "Loss:  0.13843534886837006\n",
      "Loss:  0.14276646077632904\n",
      "Loss:  0.23335187137126923\n",
      "Loss:  0.2756403684616089\n",
      "Loss:  0.21516577899456024\n",
      "Loss:  0.1467345505952835\n",
      "Loss:  0.24151279032230377\n",
      "Loss:  0.2273104339838028\n",
      "Loss:  0.1985807716846466\n",
      "Loss:  0.23686575889587402\n",
      "Loss:  0.23218056559562683\n",
      "Loss:  0.26049456000328064\n",
      "Loss:  0.20710599422454834\n",
      "Loss:  0.18359436094760895\n",
      "Loss:  0.2309618890285492\n",
      "Loss:  0.20663714408874512\n",
      "Loss:  0.24867208302021027\n",
      "Loss:  0.19587746262550354\n",
      "Loss:  0.19542472064495087\n",
      "Loss:  0.2743200361728668\n",
      "Loss:  0.28287574648857117\n",
      "Loss:  0.19331729412078857\n",
      "Loss:  0.1665460169315338\n",
      "Loss:  0.1299508959054947\n",
      "Loss:  0.20190712809562683\n",
      "Loss:  0.19382137060165405\n",
      "Loss:  0.19606594741344452\n",
      "Loss:  0.17944423854351044\n",
      "Loss:  0.26867586374282837\n",
      "Training accuracy: 91%\n",
      "Loss:  0.4734582304954529\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.18443775177001953\n",
      "Loss:  0.19355328381061554\n",
      "Loss:  0.1966855376958847\n",
      "Loss:  0.1656341850757599\n",
      "Loss:  0.17612150311470032\n",
      "Loss:  0.20959541201591492\n",
      "Loss:  0.1563388705253601\n",
      "Loss:  0.15318579971790314\n",
      "Loss:  0.12807875871658325\n",
      "Loss:  0.17088660597801208\n",
      "Loss:  0.16464602947235107\n",
      "Loss:  0.16053779423236847\n",
      "Loss:  0.20175758004188538\n",
      "Loss:  0.12602615356445312\n",
      "Loss:  0.11493983119726181\n",
      "Loss:  0.11105787754058838\n",
      "Loss:  0.14604519307613373\n",
      "Loss:  0.11719324439764023\n",
      "Loss:  0.10631534457206726\n",
      "Loss:  0.10487781465053558\n",
      "Loss:  0.07402127981185913\n",
      "Loss:  0.10970278084278107\n",
      "Loss:  0.09524665027856827\n",
      "Loss:  0.09139955043792725\n",
      "Loss:  0.13803933560848236\n",
      "Loss:  0.10499463230371475\n",
      "Loss:  0.14643874764442444\n",
      "Loss:  0.18787813186645508\n",
      "Loss:  0.09667624533176422\n",
      "Loss:  0.06270335614681244\n",
      "Loss:  0.08363175392150879\n",
      "Loss:  0.1460292786359787\n",
      "Loss:  0.19138559699058533\n",
      "Loss:  0.11589588224887848\n",
      "Loss:  0.08400728553533554\n",
      "Loss:  0.11355593055486679\n",
      "Loss:  0.16984395682811737\n",
      "Loss:  0.10457607358694077\n",
      "Loss:  0.1296098530292511\n",
      "Loss:  0.1689060628414154\n",
      "Loss:  0.11443977802991867\n",
      "Epoch:  2\n",
      "Loss:  0.1504354178905487\n",
      "Loss:  0.0678425282239914\n",
      "Loss:  0.09714566171169281\n",
      "Loss:  0.1307590752840042\n",
      "Loss:  0.07757625728845596\n",
      "Loss:  0.06398384273052216\n",
      "Loss:  0.17963586747646332\n",
      "Loss:  0.07594368606805801\n",
      "Loss:  0.09290743619203568\n",
      "Loss:  0.08692345768213272\n",
      "Loss:  0.115046925842762\n",
      "Loss:  0.10207682102918625\n",
      "Loss:  0.08304478228092194\n",
      "Loss:  0.07953600585460663\n",
      "Loss:  0.04324910789728165\n",
      "Loss:  0.13417388498783112\n",
      "Loss:  0.0685681626200676\n",
      "Loss:  0.09370497614145279\n",
      "Loss:  0.11221307516098022\n",
      "Loss:  0.0797421783208847\n",
      "Loss:  0.06232685595750809\n",
      "Loss:  0.10680098086595535\n",
      "Loss:  0.04170741140842438\n",
      "Loss:  0.07603251188993454\n",
      "Loss:  0.12254694104194641\n",
      "Loss:  0.07873900234699249\n",
      "Loss:  0.11516258865594864\n",
      "Loss:  0.0791199803352356\n",
      "Loss:  0.12754689157009125\n",
      "Loss:  0.08545602858066559\n",
      "Loss:  0.16157706081867218\n",
      "Loss:  0.20415329933166504\n",
      "Loss:  0.13530904054641724\n",
      "Loss:  0.07261015474796295\n",
      "Loss:  0.09614439308643341\n",
      "Loss:  0.0788518413901329\n",
      "Loss:  0.08510445803403854\n",
      "Loss:  0.10724521428346634\n",
      "Loss:  0.07312224060297012\n",
      "Loss:  0.15768074989318848\n",
      "Loss:  0.15474991500377655\n",
      "Training accuracy: 96%\n",
      "Loss:  0.9722505211830139\n",
      "Validation accuracy: 62%\n",
      "Loss:  0.06292958557605743\n",
      "Loss:  0.14304082095623016\n",
      "Loss:  0.16814596951007843\n",
      "Loss:  0.18481281399726868\n",
      "Loss:  0.1159309595823288\n",
      "Loss:  0.09396639466285706\n",
      "Loss:  0.1297866404056549\n",
      "Loss:  0.08818209916353226\n",
      "Loss:  0.041890501976013184\n",
      "Loss:  0.08368217200040817\n",
      "Loss:  0.11536791920661926\n",
      "Loss:  0.1263086050748825\n",
      "Loss:  0.0580865740776062\n",
      "Loss:  0.07737210392951965\n",
      "Loss:  0.18230018019676208\n",
      "Loss:  0.13253721594810486\n",
      "Loss:  0.1191958338022232\n",
      "Loss:  0.1068066954612732\n",
      "Loss:  0.2038554847240448\n",
      "Loss:  0.192000150680542\n",
      "Loss:  0.10149583220481873\n",
      "Loss:  0.0492313951253891\n",
      "Loss:  0.12716007232666016\n",
      "Loss:  0.18691375851631165\n",
      "Loss:  0.13279780745506287\n",
      "Loss:  0.06481407582759857\n",
      "Loss:  0.11717794835567474\n",
      "Loss:  0.07521422207355499\n",
      "Loss:  0.13071122765541077\n",
      "Loss:  0.08146548271179199\n",
      "Loss:  0.07951560616493225\n",
      "Loss:  0.11485495418310165\n",
      "Loss:  0.10637487471103668\n",
      "Loss:  0.09385654330253601\n",
      "Loss:  0.09494023770093918\n",
      "Loss:  0.13500724732875824\n",
      "Loss:  0.09672613441944122\n",
      "Loss:  0.10840897262096405\n",
      "Loss:  0.06595642119646072\n",
      "Loss:  0.1235557273030281\n",
      "Loss:  0.1870618611574173\n",
      "Epoch:  3\n",
      "Loss:  0.07125170528888702\n",
      "Loss:  0.08240719884634018\n",
      "Loss:  0.05685759335756302\n",
      "Loss:  0.12139991670846939\n",
      "Loss:  0.13380900025367737\n",
      "Loss:  0.14277520775794983\n",
      "Loss:  0.10495976358652115\n",
      "Loss:  0.07160071283578873\n",
      "Loss:  0.11863577365875244\n",
      "Loss:  0.047898441553115845\n",
      "Loss:  0.0696629136800766\n",
      "Loss:  0.1032373383641243\n",
      "Loss:  0.07784488052129745\n",
      "Loss:  0.10275551676750183\n",
      "Loss:  0.09843175113201141\n",
      "Loss:  0.10625537484884262\n",
      "Loss:  0.08972013741731644\n",
      "Loss:  0.15079449117183685\n",
      "Loss:  0.06256023794412613\n",
      "Loss:  0.04172900691628456\n",
      "Loss:  0.06298470497131348\n",
      "Loss:  0.06623348593711853\n",
      "Loss:  0.04958587884902954\n",
      "Loss:  0.10128837823867798\n",
      "Loss:  0.06340950727462769\n",
      "Loss:  0.07221543788909912\n",
      "Loss:  0.07099447399377823\n",
      "Loss:  0.07763925939798355\n",
      "Loss:  0.04303376004099846\n",
      "Loss:  0.08343443274497986\n",
      "Loss:  0.1104271411895752\n",
      "Loss:  0.12322446703910828\n",
      "Loss:  0.12138867378234863\n",
      "Loss:  0.05691078305244446\n",
      "Loss:  0.07621399313211441\n",
      "Loss:  0.05882164090871811\n",
      "Loss:  0.12699508666992188\n",
      "Loss:  0.0724446177482605\n",
      "Loss:  0.10444765537977219\n",
      "Loss:  0.06019875034689903\n",
      "Loss:  0.07860726863145828\n",
      "Training accuracy: 97%\n",
      "Loss:  1.0519076585769653\n",
      "Validation accuracy: 62%\n",
      "Loss:  0.05463065579533577\n",
      "Loss:  0.07956631481647491\n",
      "Loss:  0.11842986941337585\n",
      "Loss:  0.0767490565776825\n",
      "Loss:  0.10025829076766968\n",
      "Loss:  0.0840376764535904\n",
      "Loss:  0.14785663783550262\n",
      "Loss:  0.07254120707511902\n",
      "Loss:  0.09340094029903412\n",
      "Loss:  0.13916000723838806\n",
      "Loss:  0.09621644765138626\n",
      "Loss:  0.04201893508434296\n",
      "Loss:  0.05442802980542183\n",
      "Loss:  0.07681842148303986\n",
      "Loss:  0.09252239763736725\n",
      "Loss:  0.06508973240852356\n",
      "Loss:  0.05397689342498779\n",
      "Loss:  0.09824517369270325\n",
      "Loss:  0.05462062358856201\n",
      "Loss:  0.015900470316410065\n",
      "Loss:  0.12936700880527496\n",
      "Loss:  0.09746301174163818\n",
      "Loss:  0.08460275083780289\n",
      "Loss:  0.13192754983901978\n",
      "Loss:  0.0547192208468914\n",
      "Loss:  0.11226204037666321\n",
      "Loss:  0.07624448835849762\n",
      "Loss:  0.054747603833675385\n",
      "Loss:  0.21133235096931458\n",
      "Loss:  0.06492400169372559\n",
      "Loss:  0.03343139588832855\n",
      "Loss:  0.0783962681889534\n",
      "Loss:  0.07729139178991318\n",
      "Loss:  0.09139341115951538\n",
      "Loss:  0.07997709512710571\n",
      "Loss:  0.1270018219947815\n",
      "Loss:  0.04443710297346115\n",
      "Loss:  0.04608869552612305\n",
      "Loss:  0.06963791698217392\n",
      "Loss:  0.09502504020929337\n",
      "Loss:  0.0680709034204483\n",
      "Epoch:  4\n",
      "Loss:  0.06997272372245789\n",
      "Loss:  0.1273195445537567\n",
      "Loss:  0.03179752454161644\n",
      "Loss:  0.07334594428539276\n",
      "Loss:  0.066330686211586\n",
      "Loss:  0.08207909762859344\n",
      "Loss:  0.08614448457956314\n",
      "Loss:  0.04125593975186348\n",
      "Loss:  0.05815908685326576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.10142648220062256\n",
      "Loss:  0.07327601313591003\n",
      "Loss:  0.07987751811742783\n",
      "Loss:  0.03749379888176918\n",
      "Loss:  0.07004240155220032\n",
      "Loss:  0.048152193427085876\n",
      "Loss:  0.04066367447376251\n",
      "Loss:  0.02899644337594509\n",
      "Loss:  0.08487792313098907\n",
      "Loss:  0.08093991130590439\n",
      "Loss:  0.08044816553592682\n",
      "Loss:  0.060229621827602386\n",
      "Loss:  0.08329282701015472\n",
      "Loss:  0.029746931046247482\n",
      "Loss:  0.046289920806884766\n",
      "Loss:  0.15348529815673828\n",
      "Loss:  0.16709725558757782\n",
      "Loss:  0.06224343553185463\n",
      "Loss:  0.043776735663414\n",
      "Loss:  0.10468903928995132\n",
      "Loss:  0.0585368350148201\n",
      "Loss:  0.08731862902641296\n",
      "Loss:  0.07704617828130722\n",
      "Loss:  0.03824698179960251\n",
      "Loss:  0.07691588997840881\n",
      "Loss:  0.07043243199586868\n",
      "Loss:  0.08910216391086578\n",
      "Loss:  0.07399719953536987\n",
      "Loss:  0.12218251824378967\n",
      "Loss:  0.08791318535804749\n",
      "Loss:  0.06333420425653458\n",
      "Loss:  0.11145421117544174\n",
      "Training accuracy: 97%\n",
      "Loss:  1.2363003492355347\n",
      "Validation accuracy: 62%\n",
      "Loss:  0.08145422488451004\n",
      "Loss:  0.13544033467769623\n",
      "Loss:  0.07506845891475677\n",
      "Loss:  0.18124347925186157\n",
      "Loss:  0.048942919820547104\n",
      "Loss:  0.12160758674144745\n",
      "Loss:  0.0800597220659256\n",
      "Loss:  0.05125521123409271\n",
      "Loss:  0.058632075786590576\n",
      "Loss:  0.0714084804058075\n",
      "Loss:  0.05285729840397835\n",
      "Loss:  0.03757218271493912\n",
      "Loss:  0.06422176212072372\n",
      "Loss:  0.09097948670387268\n",
      "Loss:  0.03838709741830826\n",
      "Loss:  0.056950975209474564\n",
      "Loss:  0.09535837173461914\n",
      "Loss:  0.09910683333873749\n",
      "Loss:  0.15195457637310028\n",
      "Loss:  0.025564515963196754\n",
      "Loss:  0.08697453886270523\n",
      "Loss:  0.08289635181427002\n",
      "Loss:  0.06865585595369339\n",
      "Loss:  0.0915447399020195\n",
      "Loss:  0.07666594535112381\n",
      "Loss:  0.12973488867282867\n",
      "Loss:  0.08035264164209366\n",
      "Loss:  0.06350981444120407\n",
      "Loss:  0.05075426399707794\n",
      "Loss:  0.0857110545039177\n",
      "Loss:  0.08608405292034149\n",
      "Loss:  0.055137909948825836\n",
      "Loss:  0.03460216149687767\n",
      "Loss:  0.11116444319486618\n",
      "Loss:  0.03145058453083038\n",
      "Loss:  0.10016748309135437\n",
      "Loss:  0.05140350013971329\n",
      "Loss:  0.07528885453939438\n",
      "Loss:  0.0684867799282074\n",
      "Loss:  0.11307454854249954\n",
      "Loss:  0.025144198909401894\n",
      "Epoch:  5\n",
      "Loss:  0.02857583574950695\n",
      "Loss:  0.017514707520604134\n",
      "Loss:  0.07401856780052185\n",
      "Loss:  0.08990810811519623\n",
      "Loss:  0.07459460943937302\n",
      "Loss:  0.15163291990756989\n",
      "Loss:  0.12554492056369781\n",
      "Loss:  0.11987587064504623\n",
      "Loss:  0.10684636235237122\n",
      "Loss:  0.05686083063483238\n",
      "Loss:  0.027998244389891624\n",
      "Loss:  0.05052899569272995\n",
      "Loss:  0.03156255558133125\n",
      "Loss:  0.07891237735748291\n",
      "Loss:  0.02944767102599144\n",
      "Loss:  0.1483660340309143\n",
      "Loss:  0.0568404495716095\n",
      "Loss:  0.10036048293113708\n",
      "Loss:  0.02124730311334133\n",
      "Loss:  0.06564904004335403\n",
      "Loss:  0.03759195655584335\n",
      "Loss:  0.09719713032245636\n",
      "Loss:  0.08611439168453217\n",
      "Loss:  0.09349215775728226\n",
      "Loss:  0.06816653907299042\n",
      "Loss:  0.052675046026706696\n",
      "Loss:  0.04766116291284561\n",
      "Loss:  0.027790138497948647\n",
      "Loss:  0.09345195442438126\n",
      "Loss:  0.05409345030784607\n",
      "Loss:  0.058809272944927216\n",
      "Loss:  0.0867566466331482\n",
      "Loss:  0.10435380786657333\n",
      "Loss:  0.06596224755048752\n",
      "Loss:  0.04881937801837921\n",
      "Loss:  0.033217668533325195\n",
      "Loss:  0.06187666207551956\n",
      "Loss:  0.07054626941680908\n",
      "Loss:  0.08099815249443054\n",
      "Loss:  0.08294358849525452\n",
      "Loss:  0.03449030593037605\n",
      "Training accuracy: 98%\n",
      "Loss:  0.7652578353881836\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.06219532713294029\n",
      "Loss:  0.0909857451915741\n",
      "Loss:  0.02944808080792427\n",
      "Loss:  0.06369024515151978\n",
      "Loss:  0.07160401344299316\n",
      "Loss:  0.0885869637131691\n",
      "Loss:  0.03191601485013962\n",
      "Loss:  0.032438330352306366\n",
      "Loss:  0.09022250771522522\n",
      "Loss:  0.04318098723888397\n",
      "Loss:  0.06838399171829224\n",
      "Loss:  0.11019403487443924\n",
      "Loss:  0.05358172208070755\n",
      "Loss:  0.05980534851551056\n",
      "Loss:  0.027649084106087685\n",
      "Loss:  0.03403807058930397\n",
      "Loss:  0.09702078998088837\n",
      "Loss:  0.04684118553996086\n",
      "Loss:  0.029825711622834206\n",
      "Loss:  0.04837169498205185\n",
      "Loss:  0.05575601011514664\n",
      "Loss:  0.05592717230319977\n",
      "Loss:  0.07047216594219208\n",
      "Loss:  0.09168680012226105\n",
      "Loss:  0.13667789101600647\n",
      "Loss:  0.07944165170192719\n",
      "Loss:  0.048529356718063354\n",
      "Loss:  0.13772238790988922\n",
      "Loss:  0.0701441690325737\n",
      "Loss:  0.11179548501968384\n",
      "Loss:  0.0498519167304039\n",
      "Loss:  0.09896793961524963\n",
      "Loss:  0.12395671010017395\n",
      "Loss:  0.06733531504869461\n",
      "Loss:  0.07964876294136047\n",
      "Loss:  0.09115735441446304\n",
      "Loss:  0.14202573895454407\n",
      "Loss:  0.029438257217407227\n",
      "Loss:  0.07548780739307404\n",
      "Loss:  0.054535239934921265\n",
      "Loss:  0.058479923754930496\n",
      "Epoch:  6\n",
      "Loss:  0.11200617253780365\n",
      "Loss:  0.06145492196083069\n",
      "Loss:  0.09032025933265686\n",
      "Loss:  0.0762539654970169\n",
      "Loss:  0.07686468213796616\n",
      "Loss:  0.059912897646427155\n",
      "Loss:  0.06597664952278137\n",
      "Loss:  0.08379009366035461\n",
      "Loss:  0.09126985818147659\n",
      "Loss:  0.06298114359378815\n",
      "Loss:  0.07314211130142212\n",
      "Loss:  0.07703496515750885\n",
      "Loss:  0.09543528407812119\n",
      "Loss:  0.04112790897488594\n",
      "Loss:  0.07644471526145935\n",
      "Loss:  0.06696809828281403\n",
      "Loss:  0.0588921494781971\n",
      "Loss:  0.08773432672023773\n",
      "Loss:  0.0790676698088646\n",
      "Loss:  0.08503800630569458\n",
      "Loss:  0.08628950268030167\n",
      "Loss:  0.0935995802283287\n",
      "Loss:  0.053539421409368515\n",
      "Loss:  0.09122101217508316\n",
      "Loss:  0.0894642174243927\n",
      "Loss:  0.08152655512094498\n",
      "Loss:  0.07573273777961731\n",
      "Loss:  0.06155829876661301\n",
      "Loss:  0.0699647068977356\n",
      "Loss:  0.0851646214723587\n",
      "Loss:  0.04892142117023468\n",
      "Loss:  0.07481881976127625\n",
      "Loss:  0.07008317112922668\n",
      "Loss:  0.07758716493844986\n",
      "Loss:  0.08633986115455627\n",
      "Loss:  0.050254303961992264\n",
      "Loss:  0.09271898865699768\n",
      "Loss:  0.037241023033857346\n",
      "Loss:  0.06140311062335968\n",
      "Loss:  0.07967058569192886\n",
      "Loss:  0.09497785568237305\n",
      "Training accuracy: 97%\n",
      "Loss:  0.5905585289001465\n",
      "Validation accuracy: 81%\n",
      "Loss:  0.07203187048435211\n",
      "Loss:  0.041507538408041\n",
      "Loss:  0.0638006180524826\n",
      "Loss:  0.06644286960363388\n",
      "Loss:  0.07023631036281586\n",
      "Loss:  0.0458938404917717\n",
      "Loss:  0.028077319264411926\n",
      "Loss:  0.16342216730117798\n",
      "Loss:  0.08398952335119247\n",
      "Loss:  0.11165104806423187\n",
      "Loss:  0.17735673487186432\n",
      "Loss:  0.2475607991218567\n",
      "Loss:  0.14257125556468964\n",
      "Loss:  0.051798753440380096\n",
      "Loss:  0.09859788417816162\n",
      "Loss:  0.14974156022071838\n",
      "Loss:  0.09173813462257385\n",
      "Loss:  0.03653750568628311\n",
      "Loss:  0.07242637872695923\n",
      "Loss:  0.07251819968223572\n",
      "Loss:  0.10649103671312332\n",
      "Loss:  0.06177967041730881\n",
      "Loss:  0.07610949128866196\n",
      "Loss:  0.0634850338101387\n",
      "Loss:  0.05527069419622421\n",
      "Loss:  0.03830963373184204\n",
      "Loss:  0.07587897777557373\n",
      "Loss:  0.02837226539850235\n",
      "Loss:  0.09329494833946228\n",
      "Loss:  0.04363323748111725\n",
      "Loss:  0.05096052587032318\n",
      "Loss:  0.05587941035628319\n",
      "Loss:  0.052998632192611694\n",
      "Loss:  0.08577287942171097\n",
      "Loss:  0.02384454384446144\n",
      "Loss:  0.04332250356674194\n",
      "Loss:  0.0617232508957386\n",
      "Loss:  0.06349111348390579\n",
      "Loss:  0.05274614691734314\n",
      "Loss:  0.07762061059474945\n",
      "Loss:  0.09604556113481522\n",
      "Epoch:  7\n",
      "Loss:  0.04006753861904144\n",
      "Loss:  0.03051423281431198\n",
      "Loss:  0.06963664293289185\n",
      "Loss:  0.06515977531671524\n",
      "Loss:  0.028711725026369095\n",
      "Loss:  0.07126802951097488\n",
      "Loss:  0.09539076685905457\n",
      "Loss:  0.026885392144322395\n",
      "Loss:  0.06331373751163483\n",
      "Loss:  0.030448373407125473\n",
      "Loss:  0.02063971944153309\n",
      "Loss:  0.036661546677351\n",
      "Loss:  0.02723357081413269\n",
      "Loss:  0.025426363572478294\n",
      "Loss:  0.02504822611808777\n",
      "Loss:  0.032410066574811935\n",
      "Loss:  0.022103287279605865\n",
      "Loss:  0.12287136912345886\n",
      "Loss:  0.03648986294865608\n",
      "Loss:  0.03176868334412575\n",
      "Loss:  0.045629795640707016\n",
      "Loss:  0.06973034888505936\n",
      "Loss:  0.04279358312487602\n",
      "Loss:  0.024726247414946556\n",
      "Loss:  0.027564184740185738\n",
      "Loss:  0.040636833757162094\n",
      "Loss:  0.07087109982967377\n",
      "Loss:  0.037489280104637146\n",
      "Loss:  0.10841123014688492\n",
      "Loss:  0.06828659027814865\n",
      "Loss:  0.0691111609339714\n",
      "Loss:  0.046892356127500534\n",
      "Loss:  0.05273227393627167\n",
      "Loss:  0.02512604370713234\n",
      "Loss:  0.033265188336372375\n",
      "Loss:  0.02603337913751602\n",
      "Loss:  0.060357216745615005\n",
      "Loss:  0.07288206368684769\n",
      "Loss:  0.10957139730453491\n",
      "Loss:  0.062479738146066666\n",
      "Loss:  0.043328214436769485\n",
      "Training accuracy: 98%\n",
      "Loss:  0.8443880677223206\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.06575131416320801\n",
      "Loss:  0.10596698522567749\n",
      "Loss:  0.020473279058933258\n",
      "Loss:  0.04172821342945099\n",
      "Loss:  0.10234837979078293\n",
      "Loss:  0.02754155732691288\n",
      "Loss:  0.1592341810464859\n",
      "Loss:  0.027596402913331985\n",
      "Loss:  0.06509869545698166\n",
      "Loss:  0.05238621309399605\n",
      "Loss:  0.059655919671058655\n",
      "Loss:  0.06219383329153061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.09407815337181091\n",
      "Loss:  0.042327072471380234\n",
      "Loss:  0.04340040311217308\n",
      "Loss:  0.027246907353401184\n",
      "Loss:  0.03340549394488335\n",
      "Loss:  0.04556620866060257\n",
      "Loss:  0.02191336825489998\n",
      "Loss:  0.023262247443199158\n",
      "Loss:  0.0456402525305748\n",
      "Loss:  0.012691096402704716\n",
      "Loss:  0.05103008449077606\n",
      "Loss:  0.018117103725671768\n",
      "Loss:  0.10578465461730957\n",
      "Loss:  0.015025006607174873\n",
      "Loss:  0.07171940803527832\n",
      "Loss:  0.09381824731826782\n",
      "Loss:  0.09780833125114441\n",
      "Loss:  0.0628981739282608\n",
      "Loss:  0.030395079404115677\n",
      "Loss:  0.06150955706834793\n",
      "Loss:  0.06326456367969513\n",
      "Loss:  0.03114447370171547\n",
      "Loss:  0.03163960576057434\n",
      "Loss:  0.03382866829633713\n",
      "Loss:  0.03658842295408249\n",
      "Loss:  0.04795818030834198\n",
      "Loss:  0.05861479789018631\n",
      "Loss:  0.016973912715911865\n",
      "Loss:  0.0333792120218277\n",
      "Epoch:  8\n",
      "Loss:  0.044707026332616806\n",
      "Loss:  0.07078660279512405\n",
      "Loss:  0.09923850744962692\n",
      "Loss:  0.06520316004753113\n",
      "Loss:  0.03024628758430481\n",
      "Loss:  0.09774424135684967\n",
      "Loss:  0.03366570547223091\n",
      "Loss:  0.053999826312065125\n",
      "Loss:  0.027560777962207794\n",
      "Loss:  0.053891029208898544\n",
      "Loss:  0.03840351104736328\n",
      "Loss:  0.06456707417964935\n",
      "Loss:  0.048773784190416336\n",
      "Loss:  0.10226837545633316\n",
      "Loss:  0.11061418801546097\n",
      "Loss:  0.005818408913910389\n",
      "Loss:  0.04119809344410896\n",
      "Loss:  0.08831243962049484\n",
      "Loss:  0.09579244256019592\n",
      "Loss:  0.020146697759628296\n",
      "Loss:  0.05053548514842987\n",
      "Loss:  0.032224610447883606\n",
      "Loss:  0.03553264960646629\n",
      "Loss:  0.028980225324630737\n",
      "Loss:  0.020706918090581894\n",
      "Loss:  0.03782607987523079\n",
      "Loss:  0.016927402466535568\n",
      "Loss:  0.09241669625043869\n",
      "Loss:  0.044166915118694305\n",
      "Loss:  0.06254351139068604\n",
      "Loss:  0.08484179526567459\n",
      "Loss:  0.09346295893192291\n",
      "Loss:  0.04296239838004112\n",
      "Loss:  0.04841068387031555\n",
      "Loss:  0.017687808722257614\n",
      "Loss:  0.025187935680150986\n",
      "Loss:  0.09146750718355179\n",
      "Loss:  0.019401418045163155\n",
      "Loss:  0.04379776865243912\n",
      "Loss:  0.015263661742210388\n",
      "Loss:  0.023041347041726112\n",
      "Training accuracy: 98%\n",
      "Loss:  0.5215792655944824\n",
      "Validation accuracy: 81%\n",
      "Loss:  0.02398909442126751\n",
      "Loss:  0.08983387053012848\n",
      "Loss:  0.013561456464231014\n",
      "Loss:  0.05529166758060455\n",
      "Loss:  0.07539454102516174\n",
      "Loss:  0.013414917513728142\n",
      "Loss:  0.0368616059422493\n",
      "Loss:  0.1147167980670929\n",
      "Loss:  0.03754335641860962\n",
      "Loss:  0.04795859754085541\n",
      "Loss:  0.03785116225481033\n",
      "Loss:  0.04671638831496239\n",
      "Loss:  0.05989235267043114\n",
      "Loss:  0.060735031962394714\n",
      "Loss:  0.037033893167972565\n",
      "Loss:  0.03605993837118149\n",
      "Loss:  0.047627076506614685\n",
      "Loss:  0.02469681017100811\n",
      "Loss:  0.024068264290690422\n",
      "Loss:  0.049752186983823776\n",
      "Loss:  0.03948378935456276\n",
      "Loss:  0.018509456887841225\n",
      "Loss:  0.027717696502804756\n",
      "Loss:  0.05579984933137894\n",
      "Loss:  0.030697546899318695\n",
      "Loss:  0.05392878130078316\n",
      "Loss:  0.026189226657152176\n",
      "Loss:  0.05012306198477745\n",
      "Loss:  0.0655323788523674\n",
      "Loss:  0.018666531890630722\n",
      "Loss:  0.10233858972787857\n",
      "Loss:  0.050079166889190674\n",
      "Loss:  0.013149453327059746\n",
      "Loss:  0.027143100276589394\n",
      "Loss:  0.054487526416778564\n",
      "Loss:  0.06739777326583862\n",
      "Loss:  0.06469330936670303\n",
      "Loss:  0.05635356530547142\n",
      "Loss:  0.01959317922592163\n",
      "Loss:  0.13284540176391602\n",
      "Loss:  0.08164829015731812\n",
      "Epoch:  9\n",
      "Loss:  0.024117831140756607\n",
      "Loss:  0.03499552607536316\n",
      "Loss:  0.015897613018751144\n",
      "Loss:  0.02155623957514763\n",
      "Loss:  0.031822480261325836\n",
      "Loss:  0.06670574843883514\n",
      "Loss:  0.034466296434402466\n",
      "Loss:  0.056434448808431625\n",
      "Loss:  0.0186802726238966\n",
      "Loss:  0.031812120229005814\n",
      "Loss:  0.04518028348684311\n",
      "Loss:  0.02695572003722191\n",
      "Loss:  0.02995998039841652\n",
      "Loss:  0.054399676620960236\n",
      "Loss:  0.03668701648712158\n",
      "Loss:  0.010625950992107391\n",
      "Loss:  0.029371798038482666\n",
      "Loss:  0.03656487911939621\n",
      "Loss:  0.02697666920721531\n",
      "Loss:  0.029377132654190063\n",
      "Loss:  0.02221628464758396\n",
      "Loss:  0.04542805999517441\n",
      "Loss:  0.012567219324409962\n",
      "Loss:  0.02326500229537487\n",
      "Loss:  0.032308969646692276\n",
      "Loss:  0.07295767217874527\n",
      "Loss:  0.09767796099185944\n",
      "Loss:  0.030253103002905846\n",
      "Loss:  0.012166233733296394\n",
      "Loss:  0.027222441509366035\n",
      "Loss:  0.02327480912208557\n",
      "Loss:  0.05582341179251671\n",
      "Loss:  0.033922284841537476\n",
      "Loss:  0.03569680452346802\n",
      "Loss:  0.008938783779740334\n",
      "Loss:  0.028653934597969055\n",
      "Loss:  0.017512744292616844\n",
      "Loss:  0.022200485691428185\n",
      "Loss:  0.029001440852880478\n",
      "Loss:  0.025704065337777138\n",
      "Loss:  0.03944158926606178\n",
      "Training accuracy: 99%\n",
      "Loss:  0.7787160277366638\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.04779191315174103\n",
      "Loss:  0.03759022802114487\n",
      "Loss:  0.06620823591947556\n",
      "Loss:  0.03688492998480797\n",
      "Loss:  0.021112851798534393\n",
      "Loss:  0.025950146839022636\n",
      "Loss:  0.0569680817425251\n",
      "Loss:  0.06497325003147125\n",
      "Loss:  0.05808950215578079\n",
      "Loss:  0.04730410873889923\n",
      "Loss:  0.08925250917673111\n",
      "Loss:  0.028826864436268806\n",
      "Loss:  0.0847804993391037\n",
      "Loss:  0.04124591872096062\n",
      "Loss:  0.032439637929201126\n",
      "Loss:  0.11275959014892578\n",
      "Loss:  0.0315198078751564\n",
      "Loss:  0.045054543763399124\n",
      "Loss:  0.060897111892700195\n",
      "Loss:  0.06459642201662064\n",
      "Loss:  0.03332584351301193\n",
      "Loss:  0.04004841670393944\n",
      "Loss:  0.11953805387020111\n",
      "Loss:  0.01121899951249361\n",
      "Loss:  0.03387388959527016\n",
      "Loss:  0.024168897420167923\n",
      "Loss:  0.02696016989648342\n",
      "Loss:  0.05488956719636917\n",
      "Loss:  0.02213231287896633\n",
      "Loss:  0.009586981497704983\n",
      "Loss:  0.025312546640634537\n",
      "Loss:  0.09298107028007507\n",
      "Loss:  0.04620630294084549\n",
      "Loss:  0.02144879661500454\n",
      "Loss:  0.05156746134161949\n",
      "Loss:  0.028225325047969818\n",
      "Loss:  0.025368954986333847\n",
      "Loss:  0.007908506318926811\n",
      "Loss:  0.06279666721820831\n",
      "Loss:  0.03823187202215195\n",
      "Loss:  0.04134678840637207\n",
      "Epoch:  10\n",
      "Loss:  0.01316300593316555\n",
      "Loss:  0.05330289527773857\n",
      "Loss:  0.01142208743840456\n",
      "Loss:  0.013142931275069714\n",
      "Loss:  0.023183533921837807\n",
      "Loss:  0.035705920308828354\n",
      "Loss:  0.026847902685403824\n",
      "Loss:  0.026940463110804558\n",
      "Loss:  0.04007094353437424\n",
      "Loss:  0.0058873500674963\n",
      "Loss:  0.008313584141433239\n",
      "Loss:  0.0245361290872097\n",
      "Loss:  0.031764306128025055\n",
      "Loss:  0.06372019648551941\n",
      "Loss:  0.0076487199403345585\n",
      "Loss:  0.016540605574846268\n",
      "Loss:  0.02534588798880577\n",
      "Loss:  0.04572856053709984\n",
      "Loss:  0.048952702432870865\n",
      "Loss:  0.010329975746572018\n",
      "Loss:  0.018441397696733475\n",
      "Loss:  0.02147466130554676\n",
      "Loss:  0.04530368000268936\n",
      "Loss:  0.015935145318508148\n",
      "Loss:  0.014879664406180382\n",
      "Loss:  0.044819943606853485\n",
      "Loss:  0.007243969477713108\n",
      "Loss:  0.015609651803970337\n",
      "Loss:  0.024855056777596474\n",
      "Loss:  0.016627348959445953\n",
      "Loss:  0.0624740794301033\n",
      "Loss:  0.03988578915596008\n",
      "Loss:  0.02325250953435898\n",
      "Loss:  0.060949623584747314\n",
      "Loss:  0.03428344801068306\n",
      "Loss:  0.028618618845939636\n",
      "Loss:  0.035300157964229584\n",
      "Loss:  0.04730921611189842\n",
      "Loss:  0.02956985868513584\n",
      "Loss:  0.05115177854895592\n",
      "Loss:  0.011304291896522045\n",
      "Training accuracy: 99%\n",
      "Loss:  0.8077678084373474\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.018499786034226418\n",
      "Loss:  0.03242027014493942\n",
      "Loss:  0.009247472509741783\n",
      "Loss:  0.014850151725113392\n",
      "Loss:  0.024368489161133766\n",
      "Loss:  0.03989989310503006\n",
      "Loss:  0.01632130891084671\n",
      "Loss:  0.04208621382713318\n",
      "Loss:  0.08160357922315598\n",
      "Loss:  0.012094616889953613\n",
      "Loss:  0.007338933181017637\n",
      "Loss:  0.0037405346520245075\n",
      "Loss:  0.02525179833173752\n",
      "Loss:  0.00998738780617714\n",
      "Loss:  0.017314504832029343\n",
      "Loss:  0.056049197912216187\n",
      "Loss:  0.05840175598859787\n",
      "Loss:  0.024228697642683983\n",
      "Loss:  0.005121622700244188\n",
      "Loss:  0.024787940084934235\n",
      "Loss:  0.06499980390071869\n",
      "Loss:  0.013515731319785118\n",
      "Loss:  0.017285890877246857\n",
      "Loss:  0.04729397967457771\n",
      "Loss:  0.017865890637040138\n",
      "Loss:  0.041075557470321655\n",
      "Loss:  0.023099597543478012\n",
      "Loss:  0.051268238574266434\n",
      "Loss:  0.027098221704363823\n",
      "Loss:  0.02428572252392769\n",
      "Loss:  0.011708427220582962\n",
      "Loss:  0.022513123229146004\n",
      "Loss:  0.03660663217306137\n",
      "Loss:  0.07087097316980362\n",
      "Loss:  0.03540823236107826\n",
      "Loss:  0.05526115372776985\n",
      "Loss:  0.030457548797130585\n",
      "Loss:  0.06381003558635712\n",
      "Loss:  0.043697286397218704\n",
      "Loss:  0.013629748485982418\n",
      "Loss:  0.08278372883796692\n",
      "Epoch:  11\n",
      "Loss:  0.009512556716799736\n",
      "Loss:  0.061144448816776276\n",
      "Loss:  0.0283588208258152\n",
      "Loss:  0.05968252569437027\n",
      "Loss:  0.03442556411027908\n",
      "Loss:  0.01222712267190218\n",
      "Loss:  0.025435592979192734\n",
      "Loss:  0.023792244493961334\n",
      "Loss:  0.035212442278862\n",
      "Loss:  0.020977646112442017\n",
      "Loss:  0.00736774830147624\n",
      "Loss:  0.02303866669535637\n",
      "Loss:  0.11282281577587128\n",
      "Loss:  0.0111237782984972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.01956394873559475\n",
      "Loss:  0.009516088292002678\n",
      "Loss:  0.04472615569829941\n",
      "Loss:  0.028274662792682648\n",
      "Loss:  0.03500628098845482\n",
      "Loss:  0.05932319536805153\n",
      "Loss:  0.022689171135425568\n",
      "Loss:  0.0657065212726593\n",
      "Loss:  0.038102224469184875\n",
      "Loss:  0.04133106395602226\n",
      "Loss:  0.08584664762020111\n",
      "Loss:  0.09631258249282837\n",
      "Loss:  0.01814018376171589\n",
      "Loss:  0.05081263184547424\n",
      "Loss:  0.09404801577329636\n",
      "Loss:  0.030605634674429893\n",
      "Loss:  0.0256260484457016\n",
      "Loss:  0.02718154899775982\n",
      "Loss:  0.0023263394832611084\n",
      "Loss:  0.031416118144989014\n",
      "Loss:  0.05603518337011337\n",
      "Loss:  0.02800767868757248\n",
      "Loss:  0.04214823991060257\n",
      "Loss:  0.004241535905748606\n",
      "Loss:  0.05823595076799393\n",
      "Loss:  0.05688323825597763\n",
      "Loss:  0.02920708991587162\n",
      "Training accuracy: 99%\n",
      "Loss:  1.9411195516586304\n",
      "Validation accuracy: 62%\n",
      "Loss:  0.07460454851388931\n",
      "Loss:  0.07439897954463959\n",
      "Loss:  0.02922758087515831\n",
      "Loss:  0.011995935812592506\n",
      "Loss:  0.012116254307329655\n",
      "Loss:  0.06844072043895721\n",
      "Loss:  0.08468509465456009\n",
      "Loss:  0.023104002699255943\n",
      "Loss:  0.029116015881299973\n",
      "Loss:  0.03309890255331993\n",
      "Loss:  0.062265362590551376\n",
      "Loss:  0.059854548424482346\n",
      "Loss:  0.012453906238079071\n",
      "Loss:  0.031098593026399612\n",
      "Loss:  0.025650283321738243\n",
      "Loss:  0.06065300479531288\n",
      "Loss:  0.01746186800301075\n",
      "Loss:  0.02905324101448059\n",
      "Loss:  0.017904777079820633\n",
      "Loss:  0.02548247016966343\n",
      "Loss:  0.02081214264035225\n",
      "Loss:  0.0068666888400912285\n",
      "Loss:  0.020408913493156433\n",
      "Loss:  0.005829146131873131\n",
      "Loss:  0.008834163658320904\n",
      "Loss:  0.040230944752693176\n",
      "Loss:  0.05590475723147392\n",
      "Loss:  0.05224769189953804\n",
      "Loss:  0.016367867588996887\n",
      "Loss:  0.0304334107786417\n",
      "Loss:  0.022316845133900642\n",
      "Loss:  0.014343132264912128\n",
      "Loss:  0.07133115082979202\n",
      "Loss:  0.01522140670567751\n",
      "Loss:  0.05804892256855965\n",
      "Loss:  0.02289387583732605\n",
      "Loss:  0.01957136206328869\n",
      "Loss:  0.030471347272396088\n",
      "Loss:  0.012679731473326683\n",
      "Loss:  0.008699100464582443\n",
      "Loss:  0.02595333568751812\n",
      "Epoch:  12\n",
      "Loss:  0.04267941415309906\n",
      "Loss:  0.020446712151169777\n",
      "Loss:  0.016884399577975273\n",
      "Loss:  0.029196199029684067\n",
      "Loss:  0.027376139536499977\n",
      "Loss:  0.031518109142780304\n",
      "Loss:  0.009951679036021233\n",
      "Loss:  0.043220654129981995\n",
      "Loss:  0.008632649667561054\n",
      "Loss:  0.023636769503355026\n",
      "Loss:  0.03773657977581024\n",
      "Loss:  0.017923172563314438\n",
      "Loss:  0.0028138495981693268\n",
      "Loss:  0.024602726101875305\n",
      "Loss:  0.02276398427784443\n",
      "Loss:  0.016853932291269302\n",
      "Loss:  0.007809078320860863\n",
      "Loss:  0.011121824383735657\n",
      "Loss:  0.021019432693719864\n",
      "Loss:  0.008838760666549206\n",
      "Loss:  0.02004913240671158\n",
      "Loss:  0.017182033509016037\n",
      "Loss:  0.01039399765431881\n",
      "Loss:  0.006563441827893257\n",
      "Loss:  0.025658268481492996\n",
      "Loss:  0.010282011702656746\n",
      "Loss:  0.05112820863723755\n",
      "Loss:  0.03676476329565048\n",
      "Loss:  0.009576753713190556\n",
      "Loss:  0.024029303342103958\n",
      "Loss:  0.009115158580243587\n",
      "Loss:  0.011295298114418983\n",
      "Loss:  0.028797686100006104\n",
      "Loss:  0.013371751643717289\n",
      "Loss:  0.011378645896911621\n",
      "Loss:  0.01777404360473156\n",
      "Loss:  0.025353984907269478\n",
      "Loss:  0.008867166936397552\n",
      "Loss:  0.04510759562253952\n",
      "Loss:  0.007287908811122179\n",
      "Loss:  0.005493288394063711\n",
      "Training accuracy: 99%\n",
      "Loss:  0.7523142099380493\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.015627499669790268\n",
      "Loss:  0.029835466295480728\n",
      "Loss:  0.0069236960262060165\n",
      "Loss:  0.016543982550501823\n",
      "Loss:  0.006668127607554197\n",
      "Loss:  0.012894824147224426\n",
      "Loss:  0.00561058335006237\n",
      "Loss:  0.013189313933253288\n",
      "Loss:  0.01479249820113182\n",
      "Loss:  0.009156095795333385\n",
      "Loss:  0.021842781454324722\n",
      "Loss:  0.05258385092020035\n",
      "Loss:  0.0040046898648142815\n",
      "Loss:  0.007993961684405804\n",
      "Loss:  0.05293863266706467\n",
      "Loss:  0.026029931381344795\n",
      "Loss:  0.030419377610087395\n",
      "Loss:  0.01021649781614542\n",
      "Loss:  0.0296495221555233\n",
      "Loss:  0.03708406537771225\n",
      "Loss:  0.034926887601614\n",
      "Loss:  0.011517542414367199\n",
      "Loss:  0.003679100424051285\n",
      "Loss:  0.010962804779410362\n",
      "Loss:  0.09241184592247009\n",
      "Loss:  0.026606585830450058\n",
      "Loss:  0.014740749262273312\n",
      "Loss:  0.08025703579187393\n",
      "Loss:  0.02666616067290306\n",
      "Loss:  0.009478744119405746\n",
      "Loss:  0.020742475986480713\n",
      "Loss:  0.044395919889211655\n",
      "Loss:  0.049564070999622345\n",
      "Loss:  0.08326873183250427\n",
      "Loss:  0.06002696976065636\n",
      "Loss:  0.03432881459593773\n",
      "Loss:  0.007409298792481422\n",
      "Loss:  0.03187389299273491\n",
      "Loss:  0.01336994580924511\n",
      "Loss:  0.029457638040184975\n",
      "Loss:  0.020476512610912323\n",
      "Epoch:  13\n",
      "Loss:  0.0041436124593019485\n",
      "Loss:  0.01647176221013069\n",
      "Loss:  0.01928366906940937\n",
      "Loss:  0.01979789510369301\n",
      "Loss:  0.009206297807395458\n",
      "Loss:  0.027571454644203186\n",
      "Loss:  0.023470431566238403\n",
      "Loss:  0.01595480553805828\n",
      "Loss:  0.008155817165970802\n",
      "Loss:  0.008968153968453407\n",
      "Loss:  0.008714208379387856\n",
      "Loss:  0.00876054260879755\n",
      "Loss:  0.014063127338886261\n",
      "Loss:  0.008840566501021385\n",
      "Loss:  0.02037562057375908\n",
      "Loss:  0.0356254056096077\n",
      "Loss:  0.025582268834114075\n",
      "Loss:  0.024838082492351532\n",
      "Loss:  0.007960626855492592\n",
      "Loss:  0.010805411264300346\n",
      "Loss:  0.014051965437829494\n",
      "Loss:  0.016357311978936195\n",
      "Loss:  0.008346217684447765\n",
      "Loss:  0.02195568010210991\n",
      "Loss:  0.004531271755695343\n",
      "Loss:  0.019374124705791473\n",
      "Loss:  0.019581003114581108\n",
      "Loss:  0.005758434534072876\n",
      "Loss:  0.02396566793322563\n",
      "Loss:  0.02303994819521904\n",
      "Loss:  0.01538901962339878\n",
      "Loss:  0.021122632548213005\n",
      "Loss:  0.013475301675498486\n",
      "Loss:  0.04242362827062607\n",
      "Loss:  0.017366958782076836\n",
      "Loss:  0.006245573982596397\n",
      "Loss:  0.007317630108445883\n",
      "Loss:  0.005608694162219763\n",
      "Loss:  0.02082674205303192\n",
      "Loss:  0.01740993559360504\n",
      "Loss:  0.053893521428108215\n",
      "Training accuracy: 100%\n",
      "Loss:  0.768336296081543\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.006051837932318449\n",
      "Loss:  0.010002117604017258\n",
      "Loss:  0.020698830485343933\n",
      "Loss:  0.03528952598571777\n",
      "Loss:  0.011485880240797997\n",
      "Loss:  0.013590560294687748\n",
      "Loss:  0.04338251054286957\n",
      "Loss:  0.008790047839283943\n",
      "Loss:  0.030970310792326927\n",
      "Loss:  0.05818074941635132\n",
      "Loss:  0.011989643797278404\n",
      "Loss:  0.0017462735995650291\n",
      "Loss:  0.02350487746298313\n",
      "Loss:  0.05082036182284355\n",
      "Loss:  0.044338516891002655\n",
      "Loss:  0.015787307173013687\n",
      "Loss:  0.06724076718091965\n",
      "Loss:  0.028258992359042168\n",
      "Loss:  0.004671350587159395\n",
      "Loss:  0.005102942697703838\n",
      "Loss:  0.027208134531974792\n",
      "Loss:  0.020977361127734184\n",
      "Loss:  0.007611420936882496\n",
      "Loss:  0.014133516699075699\n",
      "Loss:  0.021763287484645844\n",
      "Loss:  0.0034703847486525774\n",
      "Loss:  0.04568018764257431\n",
      "Loss:  0.010294236242771149\n",
      "Loss:  0.04474532604217529\n",
      "Loss:  0.014843693934381008\n",
      "Loss:  0.03347111865878105\n",
      "Loss:  0.042765725404024124\n",
      "Loss:  0.026175597682595253\n",
      "Loss:  0.041913360357284546\n",
      "Loss:  0.007645753212273121\n",
      "Loss:  0.02012431062757969\n",
      "Loss:  0.014731505885720253\n",
      "Loss:  0.012395413592457771\n",
      "Loss:  0.010160156525671482\n",
      "Loss:  0.04458251968026161\n",
      "Loss:  0.09462498873472214\n",
      "Epoch:  14\n",
      "Loss:  0.01830308511853218\n",
      "Loss:  0.017834773287177086\n",
      "Loss:  0.013593053445219994\n",
      "Loss:  0.05208864063024521\n",
      "Loss:  0.006066322326660156\n",
      "Loss:  0.0654199868440628\n",
      "Loss:  0.011221309192478657\n",
      "Loss:  0.046242933720350266\n",
      "Loss:  0.00401543453335762\n",
      "Loss:  0.004663345403969288\n",
      "Loss:  0.026046529412269592\n",
      "Loss:  0.04864240437746048\n",
      "Loss:  0.008651578798890114\n",
      "Loss:  0.04531627148389816\n",
      "Loss:  0.014927370473742485\n",
      "Loss:  0.054873086512088776\n",
      "Loss:  0.03440795838832855\n",
      "Loss:  0.03317083418369293\n",
      "Loss:  0.005713997408747673\n",
      "Loss:  0.04063904657959938\n",
      "Loss:  0.027711782604455948\n",
      "Loss:  0.006704601924866438\n",
      "Loss:  0.005546842701733112\n",
      "Loss:  0.005175817757844925\n",
      "Loss:  0.01931929960846901\n",
      "Loss:  0.06053505092859268\n",
      "Loss:  0.015755489468574524\n",
      "Loss:  0.008708939887583256\n",
      "Loss:  0.008297157473862171\n",
      "Loss:  0.010906352661550045\n",
      "Loss:  0.03496682643890381\n",
      "Loss:  0.016269749030470848\n",
      "Loss:  0.028247598558664322\n",
      "Loss:  0.0198832955211401\n",
      "Loss:  0.00749761750921607\n",
      "Loss:  0.035247836261987686\n",
      "Loss:  0.009433514438569546\n",
      "Loss:  0.033709753304719925\n",
      "Loss:  0.025200936943292618\n",
      "Loss:  0.01900186389684677\n",
      "Loss:  0.08337948471307755\n",
      "Training accuracy: 99%\n",
      "Loss:  0.9535805583000183\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.015499645844101906\n",
      "Loss:  0.0213791336864233\n",
      "Loss:  0.08859867602586746\n",
      "Loss:  0.02161124162375927\n",
      "Loss:  0.011033185757696629\n",
      "Loss:  0.021715400740504265\n",
      "Loss:  0.09173057228326797\n",
      "Loss:  0.011581323109567165\n",
      "Loss:  0.019884143024683\n",
      "Loss:  0.04588032141327858\n",
      "Loss:  0.007033701986074448\n",
      "Loss:  0.011568909510970116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0710144191980362\n",
      "Loss:  0.044141627848148346\n",
      "Loss:  0.026464173570275307\n",
      "Loss:  0.008570042438805103\n",
      "Loss:  0.03805001825094223\n",
      "Loss:  0.007033067289739847\n",
      "Loss:  0.007108527701348066\n",
      "Loss:  0.03464557230472565\n",
      "Loss:  0.022198379039764404\n",
      "Loss:  0.014107857830822468\n",
      "Loss:  0.003549790009856224\n",
      "Loss:  0.006914067547768354\n",
      "Loss:  0.04467951878905296\n",
      "Loss:  0.012625264003872871\n",
      "Loss:  0.006718796212226152\n",
      "Loss:  0.009731071069836617\n",
      "Loss:  0.002003086730837822\n",
      "Loss:  0.0158823374658823\n",
      "Loss:  0.006571365986019373\n",
      "Loss:  0.0038622424472123384\n",
      "Loss:  0.007559056859463453\n",
      "Loss:  0.002497671637684107\n",
      "Loss:  0.011494666337966919\n",
      "Loss:  0.005774777382612228\n",
      "Loss:  0.00677972286939621\n",
      "Loss:  0.0013834396377205849\n",
      "Loss:  0.004472583532333374\n",
      "Loss:  0.0073319412767887115\n",
      "Loss:  0.01666858047246933\n",
      "Epoch:  15\n",
      "Loss:  0.0035926662385463715\n",
      "Loss:  0.0007908279076218605\n",
      "Loss:  0.0018798206001520157\n",
      "Loss:  0.0022776182740926743\n",
      "Loss:  0.0042345887050032616\n",
      "Loss:  0.0029494110494852066\n",
      "Loss:  0.01611325703561306\n",
      "Loss:  0.003647244069725275\n",
      "Loss:  0.004860036540776491\n",
      "Loss:  0.0055919187143445015\n",
      "Loss:  0.0010147914290428162\n",
      "Loss:  0.009728648699820042\n",
      "Loss:  0.018084321171045303\n",
      "Loss:  0.036510027945041656\n",
      "Loss:  0.0078119877725839615\n",
      "Loss:  0.0014320751652121544\n",
      "Loss:  0.018090909346938133\n",
      "Loss:  0.0012875087559223175\n",
      "Loss:  0.0026933234184980392\n",
      "Loss:  0.00405322527512908\n",
      "Loss:  0.009835370816290379\n",
      "Loss:  0.0016635172069072723\n",
      "Loss:  0.0008165203034877777\n",
      "Loss:  0.02977786399424076\n",
      "Loss:  0.008751798421144485\n",
      "Loss:  0.0031679109670221806\n",
      "Loss:  0.008467808365821838\n",
      "Loss:  0.025340164080262184\n",
      "Loss:  0.011595468968153\n",
      "Loss:  0.009136185981333256\n",
      "Loss:  0.0053812190890312195\n",
      "Loss:  0.0012575890868902206\n",
      "Loss:  0.001434946432709694\n",
      "Loss:  0.021096426993608475\n",
      "Loss:  0.0011881748214364052\n",
      "Loss:  0.002369442954659462\n",
      "Loss:  0.0008966447785496712\n",
      "Loss:  0.002133917063474655\n",
      "Loss:  0.0037582097575068474\n",
      "Loss:  0.0033485337626188993\n",
      "Loss:  0.0010345032205805182\n",
      "Training accuracy: 100%\n",
      "Loss:  1.0674444437026978\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.0018761064857244492\n",
      "Loss:  0.007143819704651833\n",
      "Loss:  0.006805969402194023\n",
      "Loss:  0.016639653593301773\n",
      "Loss:  0.02694377303123474\n",
      "Loss:  0.012697344645857811\n",
      "Loss:  0.032027412205934525\n",
      "Loss:  0.00397392688319087\n",
      "Loss:  0.0017573735676705837\n",
      "Loss:  0.006439864169806242\n",
      "Loss:  0.040744028985500336\n",
      "Loss:  0.003359041642397642\n",
      "Loss:  0.0018813763745129108\n",
      "Loss:  0.020354868844151497\n",
      "Loss:  0.06642510741949081\n",
      "Loss:  0.001596011221408844\n",
      "Loss:  0.004186981823295355\n",
      "Loss:  0.004912934266030788\n",
      "Loss:  0.0017916783690452576\n",
      "Loss:  0.008926904760301113\n",
      "Loss:  0.00677321245893836\n",
      "Loss:  0.005435589235275984\n",
      "Loss:  0.007645432837307453\n",
      "Loss:  0.006212423089891672\n",
      "Loss:  0.005159447900950909\n",
      "Loss:  0.010092516429722309\n",
      "Loss:  0.004199191927909851\n",
      "Loss:  0.004510140512138605\n",
      "Loss:  0.0005892682820558548\n",
      "Loss:  0.0030534248799085617\n",
      "Loss:  0.027882112190127373\n",
      "Loss:  0.006434172857552767\n",
      "Loss:  0.004761015996336937\n",
      "Loss:  0.0013245576992630959\n",
      "Loss:  0.003663231385871768\n",
      "Loss:  0.0017813928425312042\n",
      "Loss:  0.0009540300816297531\n",
      "Loss:  0.005508735775947571\n",
      "Loss:  0.0027689123526215553\n",
      "Loss:  0.012360487133264542\n",
      "Loss:  0.006053479854017496\n",
      "Epoch:  16\n",
      "Loss:  0.0048873545601964\n",
      "Loss:  0.0004644719883799553\n",
      "Loss:  0.0003242725506424904\n",
      "Loss:  0.0018554916605353355\n",
      "Loss:  0.024548986926674843\n",
      "Loss:  0.006341850385069847\n",
      "Loss:  0.0011456850916147232\n",
      "Loss:  0.0011712294071912766\n",
      "Loss:  0.0009583467617630959\n",
      "Loss:  0.0012747393921017647\n",
      "Loss:  0.003971060737967491\n",
      "Loss:  0.001111774705350399\n",
      "Loss:  0.00045523885637521744\n",
      "Loss:  0.0006071338430047035\n",
      "Loss:  0.0008235080167651176\n",
      "Loss:  0.0006433995440602303\n",
      "Loss:  0.0011026188731193542\n",
      "Loss:  0.005292210262268782\n",
      "Loss:  0.0049630748108029366\n",
      "Loss:  0.03574061766266823\n",
      "Loss:  0.003217983990907669\n",
      "Loss:  0.0013630804605782032\n",
      "Loss:  0.0030507855117321014\n",
      "Loss:  0.00041976477950811386\n",
      "Loss:  0.0013098632916808128\n",
      "Loss:  0.0006521772593259811\n",
      "Loss:  0.01918170414865017\n",
      "Loss:  0.0005783652886748314\n",
      "Loss:  0.0003699054941534996\n",
      "Loss:  0.00015564076602458954\n",
      "Loss:  0.0009120493195950985\n",
      "Loss:  0.0003610430285334587\n",
      "Loss:  0.0035295793786644936\n",
      "Loss:  0.0002146586775779724\n",
      "Loss:  0.010074688121676445\n",
      "Loss:  0.0015268800780177116\n",
      "Loss:  0.0016706278547644615\n",
      "Loss:  0.0027418413665145636\n",
      "Loss:  0.0005848510190844536\n",
      "Loss:  0.0002811215817928314\n",
      "Loss:  0.0008356521720997989\n",
      "Training accuracy: 100%\n",
      "Loss:  1.1937922239303589\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.0002909470349550247\n",
      "Loss:  0.0031197951175272465\n",
      "Loss:  0.011989781633019447\n",
      "Loss:  0.0015229559503495693\n",
      "Loss:  0.003745666705071926\n",
      "Loss:  0.02405690774321556\n",
      "Loss:  0.002402298152446747\n",
      "Loss:  0.000997200608253479\n",
      "Loss:  0.008413870818912983\n",
      "Loss:  0.0015954868867993355\n",
      "Loss:  0.00020472146570682526\n",
      "Loss:  0.002047901973128319\n",
      "Loss:  0.001518078614026308\n",
      "Loss:  0.002858900697901845\n",
      "Loss:  0.012155753560364246\n",
      "Loss:  0.001346454955637455\n",
      "Loss:  0.004014431498944759\n",
      "Loss:  0.0013386085629463196\n",
      "Loss:  0.0014984840527176857\n",
      "Loss:  0.002370249480009079\n",
      "Loss:  0.0018810974434018135\n",
      "Loss:  0.030797742307186127\n",
      "Loss:  0.00355019373819232\n",
      "Loss:  0.02609788253903389\n",
      "Loss:  0.001356460154056549\n",
      "Loss:  0.00289740739390254\n",
      "Loss:  0.0051630279049277306\n",
      "Loss:  0.0028583412058651447\n",
      "Loss:  0.001854104921221733\n",
      "Loss:  0.026102731004357338\n",
      "Loss:  0.002021502237766981\n",
      "Loss:  0.011783444322645664\n",
      "Loss:  0.05024326965212822\n",
      "Loss:  0.0032004753593355417\n",
      "Loss:  0.0008193580433726311\n",
      "Loss:  0.028773315250873566\n",
      "Loss:  0.08319833874702454\n",
      "Loss:  0.005689196288585663\n",
      "Loss:  0.0016397321596741676\n",
      "Loss:  0.03751051425933838\n",
      "Loss:  0.06748684495687485\n",
      "Epoch:  17\n",
      "Loss:  0.012241651304066181\n",
      "Loss:  0.019037583842873573\n",
      "Loss:  0.0010566161945462227\n",
      "Loss:  0.0004635760560631752\n",
      "Loss:  0.015270263887941837\n",
      "Loss:  0.007948779501020908\n",
      "Loss:  0.05383125692605972\n",
      "Loss:  0.015613507479429245\n",
      "Loss:  0.008912553079426289\n",
      "Loss:  0.002114385599270463\n",
      "Loss:  0.006674820091575384\n",
      "Loss:  0.022810721769928932\n",
      "Loss:  0.009995164349675179\n",
      "Loss:  0.00038864463567733765\n",
      "Loss:  0.0002378523349761963\n",
      "Loss:  0.01162854302674532\n",
      "Loss:  0.0030553375836461782\n",
      "Loss:  0.0005048131570219994\n",
      "Loss:  0.002177329733967781\n",
      "Loss:  0.0005403608083724976\n",
      "Loss:  0.0029433388262987137\n",
      "Loss:  0.0027402611449360847\n",
      "Loss:  0.0035483292303979397\n",
      "Loss:  0.0019884561188519\n",
      "Loss:  0.0524178221821785\n",
      "Loss:  0.003670953679829836\n",
      "Loss:  0.004395931027829647\n",
      "Loss:  0.0012704813852906227\n",
      "Loss:  0.005468685179948807\n",
      "Loss:  0.03156968206167221\n",
      "Loss:  0.0002737734466791153\n",
      "Loss:  0.0012565962970256805\n",
      "Loss:  0.001054626889526844\n",
      "Loss:  0.007935532368719578\n",
      "Loss:  0.0008107386529445648\n",
      "Loss:  0.006313742138445377\n",
      "Loss:  0.00253814784809947\n",
      "Loss:  0.014478120021522045\n",
      "Loss:  0.002274226862937212\n",
      "Loss:  0.035372015088796616\n",
      "Loss:  0.0002710720000322908\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8867075443267822\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.016952335834503174\n",
      "Loss:  0.016880463808774948\n",
      "Loss:  0.06781323999166489\n",
      "Loss:  0.01988956891000271\n",
      "Loss:  0.011721175163984299\n",
      "Loss:  0.032488081604242325\n",
      "Loss:  0.02652115374803543\n",
      "Loss:  0.05058009549975395\n",
      "Loss:  0.018006835132837296\n",
      "Loss:  0.003463256638497114\n",
      "Loss:  0.0018219919875264168\n",
      "Loss:  0.018357548862695694\n",
      "Loss:  0.029706213623285294\n",
      "Loss:  0.007106607314199209\n",
      "Loss:  0.04101305082440376\n",
      "Loss:  0.008576914668083191\n",
      "Loss:  0.00337209552526474\n",
      "Loss:  0.03653720021247864\n",
      "Loss:  0.039953723549842834\n",
      "Loss:  0.03453245386481285\n",
      "Loss:  0.01414386834949255\n",
      "Loss:  0.009342574514448643\n",
      "Loss:  0.012168499641120434\n",
      "Loss:  0.026496555656194687\n",
      "Loss:  0.016711875796318054\n",
      "Loss:  0.007927758619189262\n",
      "Loss:  0.041651204228401184\n",
      "Loss:  0.01269394252449274\n",
      "Loss:  0.017288673669099808\n",
      "Loss:  0.006949415430426598\n",
      "Loss:  0.005900755058974028\n",
      "Loss:  0.02391155995428562\n",
      "Loss:  0.007982593961060047\n",
      "Loss:  0.0015953248366713524\n",
      "Loss:  0.027349403128027916\n",
      "Loss:  0.008053944446146488\n",
      "Loss:  0.010193548165261745\n",
      "Loss:  0.005811765789985657\n",
      "Loss:  0.011119491420686245\n",
      "Loss:  0.016844920814037323\n",
      "Loss:  0.01001927349716425\n",
      "Epoch:  18\n",
      "Loss:  0.014385491609573364\n",
      "Loss:  0.0010417578741908073\n",
      "Loss:  0.002133562695235014\n",
      "Loss:  0.004121408797800541\n",
      "Loss:  0.013249053619801998\n",
      "Loss:  0.006812171079218388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0017598890699446201\n",
      "Loss:  0.009269881062209606\n",
      "Loss:  0.0009909477084875107\n",
      "Loss:  0.0013847444206476212\n",
      "Loss:  0.0023081242106854916\n",
      "Loss:  0.0030908845365047455\n",
      "Loss:  0.008316545747220516\n",
      "Loss:  0.003674074076116085\n",
      "Loss:  0.0060050394386053085\n",
      "Loss:  0.0025823877658694983\n",
      "Loss:  0.0015873382799327374\n",
      "Loss:  0.021039877086877823\n",
      "Loss:  0.0006525330245494843\n",
      "Loss:  0.0035317479632794857\n",
      "Loss:  0.0051255617290735245\n",
      "Loss:  0.0019579255022108555\n",
      "Loss:  0.004248568322509527\n",
      "Loss:  0.027626177296042442\n",
      "Loss:  0.0011146273463964462\n",
      "Loss:  0.0007016249001026154\n",
      "Loss:  0.002574182115495205\n",
      "Loss:  0.0016714558005332947\n",
      "Loss:  0.0008882991969585419\n",
      "Loss:  0.0014452803879976273\n",
      "Loss:  0.0010108556598424911\n",
      "Loss:  0.016532890498638153\n",
      "Loss:  0.004182689823210239\n",
      "Loss:  0.012255745008587837\n",
      "Loss:  0.0035891595762223005\n",
      "Loss:  0.0012666583061218262\n",
      "Loss:  0.0012202169746160507\n",
      "Loss:  0.0005467487499117851\n",
      "Loss:  0.003707721596583724\n",
      "Loss:  0.03171316161751747\n",
      "Loss:  0.0017958544194698334\n",
      "Training accuracy: 100%\n",
      "Loss:  1.0165528059005737\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.0016548149287700653\n",
      "Loss:  0.0010748598724603653\n",
      "Loss:  0.0018475488759577274\n",
      "Loss:  0.003965871408581734\n",
      "Loss:  0.0026632463559508324\n",
      "Loss:  0.011613101698458195\n",
      "Loss:  0.0036974039394408464\n",
      "Loss:  0.03904779627919197\n",
      "Loss:  0.004771981853991747\n",
      "Loss:  0.0012626135721802711\n",
      "Loss:  0.00017239153385162354\n",
      "Loss:  0.0004940694198012352\n",
      "Loss:  0.0069295261055231094\n",
      "Loss:  0.0001711566001176834\n",
      "Loss:  0.0005139978602528572\n",
      "Loss:  0.00234634755179286\n",
      "Loss:  0.03304925560951233\n",
      "Loss:  0.00132707878947258\n",
      "Loss:  0.001758121419698\n",
      "Loss:  0.0006866920739412308\n",
      "Loss:  0.006009197328239679\n",
      "Loss:  0.001924147829413414\n",
      "Loss:  0.00905183982104063\n",
      "Loss:  0.0064553674310445786\n",
      "Loss:  0.0005019735544919968\n",
      "Loss:  0.0007722824811935425\n",
      "Loss:  0.004249123856425285\n",
      "Loss:  0.0030513438396155834\n",
      "Loss:  0.0017687489744275808\n",
      "Loss:  0.001441660337150097\n",
      "Loss:  0.014533201232552528\n",
      "Loss:  0.004951719660311937\n",
      "Loss:  0.008900314569473267\n",
      "Loss:  0.023185841739177704\n",
      "Loss:  0.0018082507885992527\n",
      "Loss:  0.0009410069324076176\n",
      "Loss:  0.0072473809123039246\n",
      "Loss:  0.0007548732683062553\n",
      "Loss:  0.04950710013508797\n",
      "Loss:  0.028753748163580894\n",
      "Loss:  0.005179896485060453\n",
      "Epoch:  19\n",
      "Loss:  0.009655656293034554\n",
      "Loss:  0.0672154426574707\n",
      "Loss:  0.02774905227124691\n",
      "Loss:  0.016294850036501884\n",
      "Loss:  0.04874943941831589\n",
      "Loss:  0.043967798352241516\n",
      "Loss:  0.05810032784938812\n",
      "Loss:  0.013257928192615509\n",
      "Loss:  0.038598936051130295\n",
      "Loss:  0.0426994264125824\n",
      "Loss:  0.00046957843005657196\n",
      "Loss:  0.007394404616206884\n",
      "Loss:  0.06725085526704788\n",
      "Loss:  0.014312166720628738\n",
      "Loss:  0.002224690280854702\n",
      "Loss:  0.025606194511055946\n",
      "Loss:  0.03154486045241356\n",
      "Loss:  0.009355362504720688\n",
      "Loss:  0.010447444394230843\n",
      "Loss:  0.023553531616926193\n",
      "Loss:  0.028845680877566338\n",
      "Loss:  0.00623977929353714\n",
      "Loss:  0.0026480304077267647\n",
      "Loss:  0.014323062263429165\n",
      "Loss:  0.03618782013654709\n",
      "Loss:  0.011202098801732063\n",
      "Loss:  0.022703221067786217\n",
      "Loss:  0.037780601531267166\n",
      "Loss:  0.05497318506240845\n",
      "Loss:  0.007388232741504908\n",
      "Loss:  0.01894543319940567\n",
      "Loss:  0.00830845721065998\n",
      "Loss:  0.00870155543088913\n",
      "Loss:  0.028947578743100166\n",
      "Loss:  0.01594702899456024\n",
      "Loss:  0.02794833667576313\n",
      "Loss:  0.029204048216342926\n",
      "Loss:  0.0032662246376276016\n",
      "Loss:  0.019705956801772118\n",
      "Loss:  0.02220260538160801\n",
      "Loss:  0.046105921268463135\n",
      "Training accuracy: 99%\n",
      "Loss:  2.5076358318328857\n",
      "Validation accuracy: 62%\n",
      "Loss:  0.001455790363252163\n",
      "Loss:  0.025058453902602196\n",
      "Loss:  0.022001925855875015\n",
      "Loss:  0.0023812875151634216\n",
      "Loss:  0.05228479206562042\n",
      "Loss:  0.01747729443013668\n",
      "Loss:  0.0014457614161074162\n",
      "Loss:  0.006633325945585966\n",
      "Loss:  0.010734640061855316\n",
      "Loss:  0.001794152893126011\n",
      "Loss:  0.0016962564550340176\n",
      "Loss:  0.0013065598905086517\n",
      "Loss:  0.010912485420703888\n",
      "Loss:  0.0012518884614109993\n",
      "Loss:  0.005140918772667646\n",
      "Loss:  0.01122969388961792\n",
      "Loss:  0.0003316737711429596\n",
      "Loss:  0.01097414456307888\n",
      "Loss:  0.0011989898048341274\n",
      "Loss:  0.03203604370355606\n",
      "Loss:  0.0047189900651574135\n",
      "Loss:  0.0045842695981264114\n",
      "Loss:  0.018734781071543694\n",
      "Loss:  0.004904464352875948\n",
      "Loss:  0.025233784690499306\n",
      "Loss:  0.002708896528929472\n",
      "Loss:  0.0020410013385117054\n",
      "Loss:  0.0059485952369868755\n",
      "Loss:  0.00035856664180755615\n",
      "Loss:  0.00011385604739189148\n",
      "Loss:  0.011455529369413853\n",
      "Loss:  0.001515338197350502\n",
      "Loss:  0.022719252854585648\n",
      "Loss:  0.012322783470153809\n",
      "Loss:  0.0008958969265222549\n",
      "Loss:  0.010783460922539234\n",
      "Loss:  0.001988364849239588\n",
      "Loss:  0.008454770781099796\n",
      "Loss:  0.007404864300042391\n",
      "Loss:  0.003932399209588766\n",
      "Loss:  0.01641835831105709\n",
      "Epoch:  20\n",
      "Loss:  0.021662481129169464\n",
      "Loss:  0.00840727612376213\n",
      "Loss:  0.0009638182818889618\n",
      "Loss:  0.0006914753466844559\n",
      "Loss:  0.0007365169003605843\n",
      "Loss:  0.0014371462166309357\n",
      "Loss:  0.0005456525832414627\n",
      "Loss:  0.003581300377845764\n",
      "Loss:  0.00704787066206336\n",
      "Loss:  0.002379590878263116\n",
      "Loss:  0.0013801418244838715\n",
      "Loss:  0.00043414346873760223\n",
      "Loss:  0.006106354296207428\n",
      "Loss:  0.001855185255408287\n",
      "Loss:  0.0008670836687088013\n",
      "Loss:  0.0008348207920789719\n",
      "Loss:  0.0010549556463956833\n",
      "Loss:  0.0003737937659025192\n",
      "Loss:  0.001443713903427124\n",
      "Loss:  0.0005020322278141975\n",
      "Loss:  0.001105298288166523\n",
      "Loss:  0.0013194689527153969\n",
      "Loss:  0.0013766949996352196\n",
      "Loss:  0.0019351299852132797\n",
      "Loss:  0.0012482143938541412\n",
      "Loss:  0.0008687181398272514\n",
      "Loss:  0.0005901521071791649\n",
      "Loss:  0.0008760392665863037\n",
      "Loss:  0.0016405503265559673\n",
      "Loss:  0.00486477417871356\n",
      "Loss:  0.001199459657073021\n",
      "Loss:  0.0011925343424081802\n",
      "Loss:  0.0006423359736800194\n",
      "Loss:  0.0007364042103290558\n",
      "Loss:  0.012318452820181847\n",
      "Loss:  0.011596067808568478\n",
      "Loss:  0.0002933871001005173\n",
      "Loss:  0.00033054593950510025\n",
      "Loss:  0.0008274838328361511\n",
      "Loss:  0.004613080527633429\n",
      "Loss:  0.00840217899531126\n",
      "Training accuracy: 100%\n",
      "Loss:  0.9645912647247314\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.001072453334927559\n",
      "Loss:  0.0010897768661379814\n",
      "Loss:  0.0013124700635671616\n",
      "Loss:  0.0029401471838355064\n",
      "Loss:  0.003812682582065463\n",
      "Loss:  0.006209535989910364\n",
      "Loss:  0.009759178385138512\n",
      "Loss:  0.001692347228527069\n",
      "Loss:  0.004933607764542103\n",
      "Loss:  0.008314674720168114\n",
      "Loss:  0.006616153754293919\n",
      "Loss:  0.004295665770769119\n",
      "Loss:  0.004061164800077677\n",
      "Loss:  0.0017082151025533676\n",
      "Loss:  0.00021947268396615982\n",
      "Loss:  0.003646926488727331\n",
      "Loss:  0.03606242686510086\n",
      "Loss:  0.0004590386524796486\n",
      "Loss:  0.00019776076078414917\n",
      "Loss:  0.002848986769095063\n",
      "Loss:  0.01850554347038269\n",
      "Loss:  0.02100975066423416\n",
      "Loss:  0.00724678672850132\n",
      "Loss:  0.0018204753287136555\n",
      "Loss:  0.021791180595755577\n",
      "Loss:  0.04770608991384506\n",
      "Loss:  0.008162439800798893\n",
      "Loss:  0.0026395991444587708\n",
      "Loss:  0.0034189214929938316\n",
      "Loss:  0.0048960428684949875\n",
      "Loss:  0.00536276726052165\n",
      "Loss:  0.001074865460395813\n",
      "Loss:  0.005951579660177231\n",
      "Loss:  0.004421291872859001\n",
      "Loss:  0.0005361363291740417\n",
      "Loss:  0.0012368736788630486\n",
      "Loss:  0.006693867966532707\n",
      "Loss:  0.00265494454652071\n",
      "Loss:  0.009218119084835052\n",
      "Loss:  0.000772334635257721\n",
      "Loss:  0.0007510284776799381\n",
      "Epoch:  21\n",
      "Loss:  0.0035447359550744295\n",
      "Loss:  0.004875665996223688\n",
      "Loss:  0.0007069148123264313\n",
      "Loss:  0.010740266181528568\n",
      "Loss:  0.0028323624283075333\n",
      "Loss:  0.000463021919131279\n",
      "Loss:  0.004386079031974077\n",
      "Loss:  0.0007177926599979401\n",
      "Loss:  0.004485775250941515\n",
      "Loss:  0.004226259887218475\n",
      "Loss:  0.00048158690333366394\n",
      "Loss:  0.00519234873354435\n",
      "Loss:  0.004345362074673176\n",
      "Loss:  0.0028129005804657936\n",
      "Loss:  0.0017881589010357857\n",
      "Loss:  0.007590566761791706\n",
      "Loss:  0.0017802226357161999\n",
      "Loss:  0.005391858518123627\n",
      "Loss:  0.0006920080631971359\n",
      "Loss:  0.0007780026644468307\n",
      "Loss:  0.001327304169535637\n",
      "Loss:  0.007192226592451334\n",
      "Loss:  0.0008061174303293228\n",
      "Loss:  0.0003166906535625458\n",
      "Loss:  0.03948483616113663\n",
      "Loss:  0.0003227926790714264\n",
      "Loss:  0.004327400121837854\n",
      "Loss:  0.00042484235018491745\n",
      "Loss:  0.006191950291395187\n",
      "Loss:  0.01197267696261406\n",
      "Loss:  0.003599825082346797\n",
      "Loss:  0.0006398111581802368\n",
      "Loss:  0.009938971139490604\n",
      "Loss:  0.004064404405653477\n",
      "Loss:  0.005026455968618393\n",
      "Loss:  0.008223413489758968\n",
      "Loss:  0.001932252198457718\n",
      "Loss:  0.00175526924431324\n",
      "Loss:  0.0008040405809879303\n",
      "Loss:  0.011117243207991123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0010416097939014435\n",
      "Training accuracy: 100%\n",
      "Loss:  0.4545381963253021\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.0010308893397450447\n",
      "Loss:  0.00843752920627594\n",
      "Loss:  0.007780564017593861\n",
      "Loss:  0.0012201741337776184\n",
      "Loss:  0.00023452844470739365\n",
      "Loss:  0.0027661658823490143\n",
      "Loss:  0.0009152349084615707\n",
      "Loss:  0.03229444473981857\n",
      "Loss:  0.0018808520399034023\n",
      "Loss:  0.00035011954605579376\n",
      "Loss:  0.0009452644735574722\n",
      "Loss:  0.002212654799222946\n",
      "Loss:  0.01088695041835308\n",
      "Loss:  0.010005286894738674\n",
      "Loss:  0.00019957497715950012\n",
      "Loss:  0.000372578389942646\n",
      "Loss:  0.0016341060400009155\n",
      "Loss:  0.0005612941458821297\n",
      "Loss:  0.040236249566078186\n",
      "Loss:  0.0009071556851267815\n",
      "Loss:  0.00014678016304969788\n",
      "Loss:  0.003544146427884698\n",
      "Loss:  0.014965444803237915\n",
      "Loss:  0.004792953375726938\n",
      "Loss:  0.0005327304825186729\n",
      "Loss:  0.0016884184442460537\n",
      "Loss:  0.032172299921512604\n",
      "Loss:  0.0024179918691515923\n",
      "Loss:  0.0004335697740316391\n",
      "Loss:  0.00047829654067754745\n",
      "Loss:  0.0024560592137277126\n",
      "Loss:  0.0031294263899326324\n",
      "Loss:  0.012352677062153816\n",
      "Loss:  0.00034404173493385315\n",
      "Loss:  0.0025223626289516687\n",
      "Loss:  0.006106235086917877\n",
      "Loss:  0.0023559019900858402\n",
      "Loss:  0.0009410735219717026\n",
      "Loss:  0.00015984848141670227\n",
      "Loss:  0.0002724137157201767\n",
      "Loss:  0.00032370039843954146\n",
      "Epoch:  22\n",
      "Loss:  0.0013446994125843048\n",
      "Loss:  0.0011529196053743362\n",
      "Loss:  0.0006257398054003716\n",
      "Loss:  0.0005146516487002373\n",
      "Loss:  0.0002501942217350006\n",
      "Loss:  0.0015231100842356682\n",
      "Loss:  0.0027966632042080164\n",
      "Loss:  0.0012308619916439056\n",
      "Loss:  0.0005919924005866051\n",
      "Loss:  0.001656835898756981\n",
      "Loss:  0.005346791353076696\n",
      "Loss:  0.010366061702370644\n",
      "Loss:  0.00033868756145238876\n",
      "Loss:  0.0007001012563705444\n",
      "Loss:  0.0005316641181707382\n",
      "Loss:  0.0010910523124039173\n",
      "Loss:  0.0004784790799021721\n",
      "Loss:  0.0017901747487485409\n",
      "Loss:  0.0018841717392206192\n",
      "Loss:  0.0031588829588145018\n",
      "Loss:  0.0004871673882007599\n",
      "Loss:  0.0020418460480868816\n",
      "Loss:  0.004041817970573902\n",
      "Loss:  0.0005467552691698074\n",
      "Loss:  0.0017405776306986809\n",
      "Loss:  0.0020140758715569973\n",
      "Loss:  0.0011132443323731422\n",
      "Loss:  0.0011773481965065002\n",
      "Loss:  0.00212731771171093\n",
      "Loss:  0.0011869571171700954\n",
      "Loss:  0.000525694340467453\n",
      "Loss:  0.00012464635074138641\n",
      "Loss:  0.00039506517350673676\n",
      "Loss:  0.00037111900746822357\n",
      "Loss:  0.015821725130081177\n",
      "Loss:  0.0006012236699461937\n",
      "Loss:  0.00012400001287460327\n",
      "Loss:  0.0033493456430733204\n",
      "Loss:  0.00033181440085172653\n",
      "Loss:  0.00039687473326921463\n",
      "Loss:  0.0015050582587718964\n",
      "Training accuracy: 100%\n",
      "Loss:  1.39094078540802\n",
      "Validation accuracy: 81%\n",
      "Loss:  0.002278605941683054\n",
      "Loss:  0.01415996439754963\n",
      "Loss:  0.007386107929050922\n",
      "Loss:  0.001013103872537613\n",
      "Loss:  0.0013574683107435703\n",
      "Loss:  0.0007158080115914345\n",
      "Loss:  0.0006201751530170441\n",
      "Loss:  0.0005932850763201714\n",
      "Loss:  0.0011884020641446114\n",
      "Loss:  0.0008476441726088524\n",
      "Loss:  0.00018598604947328568\n",
      "Loss:  8.006393909454346e-05\n",
      "Loss:  0.0001477450132369995\n",
      "Loss:  0.0012112199328839779\n",
      "Loss:  0.002101536840200424\n",
      "Loss:  0.00010799616575241089\n",
      "Loss:  0.029297830536961555\n",
      "Loss:  0.0009269658476114273\n",
      "Loss:  0.005213775206357241\n",
      "Loss:  0.00043514184653759\n",
      "Loss:  3.758072853088379e-05\n",
      "Loss:  0.024748068302869797\n",
      "Loss:  0.005002054385840893\n",
      "Loss:  0.0004642205312848091\n",
      "Loss:  0.0020194125827401876\n",
      "Loss:  0.0006131399422883987\n",
      "Loss:  0.000851859338581562\n",
      "Loss:  0.003261979203671217\n",
      "Loss:  0.0008398164063692093\n",
      "Loss:  0.003129896242171526\n",
      "Loss:  0.0020900643430650234\n",
      "Loss:  0.0006521875038743019\n",
      "Loss:  0.0009693512693047523\n",
      "Loss:  0.002439147559925914\n",
      "Loss:  0.0003588693216443062\n",
      "Loss:  0.0034722296986728907\n",
      "Loss:  0.0008485671132802963\n",
      "Loss:  0.001724035944789648\n",
      "Loss:  0.0001255292445421219\n",
      "Loss:  0.0006635310128331184\n",
      "Loss:  0.00036220005131326616\n",
      "Epoch:  23\n",
      "Loss:  0.0004947623237967491\n",
      "Loss:  0.0009926208294928074\n",
      "Loss:  0.0001415461301803589\n",
      "Loss:  0.0005958825349807739\n",
      "Loss:  0.0011103004217147827\n",
      "Loss:  0.000932878814637661\n",
      "Loss:  0.0005902666598558426\n",
      "Loss:  0.0005804132670164108\n",
      "Loss:  0.00043532997369766235\n",
      "Loss:  0.0001967763528227806\n",
      "Loss:  0.0003648633137345314\n",
      "Loss:  0.0007281098514795303\n",
      "Loss:  0.0006515095010399818\n",
      "Loss:  0.0002728421241044998\n",
      "Loss:  0.00045164022594690323\n",
      "Loss:  0.0008122194558382034\n",
      "Loss:  0.0002612601965665817\n",
      "Loss:  0.00021931715309619904\n",
      "Loss:  0.0009355023503303528\n",
      "Loss:  0.0002658730372786522\n",
      "Loss:  0.00037738773971796036\n",
      "Loss:  0.0009772274643182755\n",
      "Loss:  0.0015961513854563236\n",
      "Loss:  0.00020567886531352997\n",
      "Loss:  0.0009567867964506149\n",
      "Loss:  0.0001867450773715973\n",
      "Loss:  0.00013344362378120422\n",
      "Loss:  0.0011298609897494316\n",
      "Loss:  0.00013034231960773468\n",
      "Loss:  9.351782500743866e-05\n",
      "Loss:  0.002048846334218979\n",
      "Loss:  0.00015291385352611542\n",
      "Loss:  0.00011736899614334106\n",
      "Loss:  0.0030972782988101244\n",
      "Loss:  0.0006779171526432037\n",
      "Loss:  0.001968745607882738\n",
      "Loss:  0.0013402337208390236\n",
      "Loss:  0.0004282398149371147\n",
      "Loss:  0.00022743456065654755\n",
      "Loss:  0.0004195794463157654\n",
      "Loss:  0.0007902234792709351\n",
      "Training accuracy: 100%\n",
      "Loss:  1.4061696529388428\n",
      "Validation accuracy: 75%\n",
      "Loss:  0.00035198405385017395\n",
      "Loss:  0.00022511929273605347\n",
      "Loss:  0.00030573271214962006\n",
      "Loss:  0.0002733580768108368\n",
      "Loss:  0.00017483532428741455\n",
      "Loss:  0.00011975131928920746\n",
      "Loss:  0.0002809036523103714\n",
      "Loss:  0.0003935331478714943\n",
      "Loss:  5.972757935523987e-05\n",
      "Loss:  6.18714839220047e-05\n",
      "Loss:  0.00016969814896583557\n",
      "Loss:  0.0004404541105031967\n",
      "Loss:  0.00014208629727363586\n",
      "Loss:  0.0005561187863349915\n",
      "Loss:  0.0010609785094857216\n",
      "Loss:  0.00019654445350170135\n",
      "Loss:  0.00010353513062000275\n",
      "Loss:  0.0004315916448831558\n",
      "Loss:  3.754720091819763e-05\n",
      "Loss:  0.00012982077896595\n",
      "Loss:  0.0009069009684026241\n",
      "Loss:  0.001405500341206789\n",
      "Loss:  0.00011840835213661194\n",
      "Loss:  6.820261478424072e-05\n",
      "Loss:  8.710473775863647e-05\n",
      "Loss:  0.0001005362719297409\n",
      "Loss:  7.3995441198349e-05\n",
      "Loss:  0.00013412721455097198\n",
      "Loss:  0.0007920609787106514\n",
      "Loss:  0.00013372302055358887\n",
      "Loss:  0.000903034582734108\n",
      "Loss:  0.0027303590904921293\n",
      "Loss:  9.050033986568451e-05\n",
      "Loss:  9.144656360149384e-05\n",
      "Loss:  0.001423688605427742\n",
      "Loss:  0.00015405751764774323\n",
      "Loss:  0.00010513700544834137\n",
      "Loss:  0.0002651624381542206\n",
      "Loss:  7.738731801509857e-05\n",
      "Loss:  0.0006897812709212303\n",
      "Loss:  0.0002918516693171114\n",
      "Epoch:  24\n",
      "Loss:  5.2712857723236084e-05\n",
      "Loss:  0.0018554339185357094\n",
      "Loss:  0.000340181402862072\n",
      "Loss:  0.00014580972492694855\n",
      "Loss:  0.001350131817162037\n",
      "Loss:  0.0003714645281434059\n",
      "Loss:  0.00037419330328702927\n",
      "Loss:  0.00029767677187919617\n",
      "Loss:  0.00016961153596639633\n",
      "Loss:  0.0013218177482485771\n",
      "Loss:  0.00010075606405735016\n",
      "Loss:  6.334111094474792e-05\n",
      "Loss:  0.0001157708466053009\n",
      "Loss:  0.00013094395399093628\n",
      "Loss:  0.0001467689871788025\n",
      "Loss:  0.00028285011649131775\n",
      "Loss:  9.920261800289154e-05\n",
      "Loss:  0.00039234384894371033\n",
      "Loss:  0.0003110859543085098\n",
      "Loss:  0.000954359769821167\n",
      "Loss:  0.0002041347324848175\n",
      "Loss:  0.00011621788144111633\n",
      "Loss:  0.0003557773306965828\n",
      "Loss:  7.431395351886749e-05\n",
      "Loss:  0.0006730658933520317\n",
      "Loss:  0.0005748486146330833\n",
      "Loss:  0.00012550875544548035\n",
      "Loss:  8.464604616165161e-05\n",
      "Loss:  0.00013689696788787842\n",
      "Loss:  0.00011649355292320251\n",
      "Loss:  0.00031258631497621536\n",
      "Loss:  0.00024156272411346436\n",
      "Loss:  0.0001223348081111908\n",
      "Loss:  0.0006028786301612854\n",
      "Loss:  0.00012503564357757568\n",
      "Loss:  0.00015609897673130035\n",
      "Loss:  0.000412890687584877\n",
      "Loss:  0.00010718032717704773\n",
      "Loss:  0.0002891095355153084\n",
      "Loss:  0.00011679530143737793\n",
      "Loss:  3.0281642466434278e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9790688753128052\n",
      "Validation accuracy: 69%\n",
      "Loss:  0.0005433456972241402\n",
      "Loss:  7.139705121517181e-05\n",
      "Loss:  0.00010780245065689087\n",
      "Loss:  0.00024279113858938217\n",
      "Loss:  0.00015498697757720947\n",
      "Loss:  6.0360878705978394e-05\n",
      "Loss:  0.00025572627782821655\n",
      "Loss:  0.0002661095932126045\n",
      "Loss:  3.718957304954529e-05\n",
      "Loss:  0.00013428367674350739\n",
      "Loss:  5.1995739340782166e-05\n",
      "Loss:  6.663613021373749e-05\n",
      "Loss:  6.219744682312012e-05\n",
      "Loss:  0.0001374613493680954\n",
      "Loss:  6.826035678386688e-05\n",
      "Loss:  4.890747368335724e-05\n",
      "Loss:  0.0010622553527355194\n",
      "Loss:  2.6866793632507324e-05\n",
      "Loss:  3.919564187526703e-05\n",
      "Loss:  4.0801241993904114e-05\n",
      "Loss:  2.746284008026123e-05\n",
      "Loss:  3.141351044178009e-05\n",
      "Loss:  0.0001930398866534233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.552731454372406e-05\n",
      "Loss:  0.0002487068995833397\n",
      "Loss:  2.6535242795944214e-05\n",
      "Loss:  0.0005501070991158485\n",
      "Loss:  5.9079378843307495e-05\n",
      "Loss:  8.076988160610199e-05\n",
      "Loss:  0.00017965584993362427\n",
      "Loss:  1.6320496797561646e-05\n",
      "Loss:  0.00018293783068656921\n",
      "Loss:  0.00015872810035943985\n",
      "Loss:  3.526173532009125e-05\n",
      "Loss:  0.0001424737274646759\n",
      "Loss:  7.813796401023865e-05\n",
      "Loss:  0.00017258338630199432\n",
      "Loss:  0.00021136179566383362\n",
      "Loss:  8.06804746389389e-05\n",
      "Loss:  7.467903196811676e-05\n",
      "Loss:  3.481904786895029e-05\n",
      "Epoch:  25\n",
      "Loss:  7.223151624202728e-05\n",
      "Loss:  7.554516196250916e-05\n",
      "Loss:  6.363168358802795e-05\n",
      "Loss:  9.231269359588623e-05\n",
      "Loss:  3.997981548309326e-05\n",
      "Loss:  1.2954697012901306e-05\n",
      "Loss:  4.945322871208191e-05\n",
      "Loss:  1.9399449229240417e-05\n",
      "Loss:  6.760656833648682e-05\n",
      "Loss:  1.358427107334137e-05\n",
      "Loss:  0.00020284578204154968\n",
      "Loss:  5.978532135486603e-05\n",
      "Loss:  4.5048072934150696e-05\n",
      "Loss:  0.0001704879105091095\n",
      "Loss:  4.623830318450928e-05\n",
      "Loss:  4.579685628414154e-05\n",
      "Loss:  5.117245018482208e-05\n",
      "Loss:  5.8341771364212036e-05\n",
      "Loss:  3.690645098686218e-05\n",
      "Loss:  0.00011489540338516235\n",
      "Loss:  0.00010342895984649658\n",
      "Loss:  0.00011088326573371887\n",
      "Loss:  7.2196125984191895e-06\n",
      "Loss:  0.00014001131057739258\n",
      "Loss:  0.000136595219373703\n",
      "Loss:  8.840486407279968e-05\n",
      "Loss:  3.307312726974487e-05\n",
      "Loss:  6.26780092716217e-05\n",
      "Loss:  7.527507841587067e-05\n",
      "Loss:  6.405077874660492e-05\n",
      "Loss:  3.088265657424927e-05\n",
      "Loss:  9.297952055931091e-05\n",
      "Loss:  0.0001356080174446106\n",
      "Loss:  6.458349525928497e-05\n",
      "Loss:  2.2433698177337646e-05\n",
      "Loss:  3.0975788831710815e-05\n",
      "Loss:  1.712515950202942e-05\n",
      "Loss:  0.00013843271881341934\n",
      "Loss:  2.980045974254608e-05\n",
      "Loss:  3.73348593711853e-05\n",
      "Loss:  3.879765790770762e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7362366914749146\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.1420961022377014e-05\n",
      "Loss:  8.873268961906433e-05\n",
      "Loss:  5.054846405982971e-05\n",
      "Loss:  9.440071880817413e-05\n",
      "Loss:  2.763979136943817e-05\n",
      "Loss:  6.404891610145569e-05\n",
      "Loss:  0.0001089293509721756\n",
      "Loss:  8.858740329742432e-05\n",
      "Loss:  4.9540773034095764e-05\n",
      "Loss:  8.068233728408813e-05\n",
      "Loss:  4.207715392112732e-05\n",
      "Loss:  1.6648322343826294e-05\n",
      "Loss:  3.168359398841858e-05\n",
      "Loss:  0.00016803480684757233\n",
      "Loss:  2.1053478121757507e-05\n",
      "Loss:  1.1702999472618103e-05\n",
      "Loss:  8.068978786468506e-06\n",
      "Loss:  8.843839168548584e-06\n",
      "Loss:  2.2372230887413025e-05\n",
      "Loss:  2.4372711777687073e-05\n",
      "Loss:  5.1802024245262146e-05\n",
      "Loss:  1.926906406879425e-05\n",
      "Loss:  2.0893290638923645e-05\n",
      "Loss:  2.4279579520225525e-05\n",
      "Loss:  9.577162563800812e-05\n",
      "Loss:  7.603131234645844e-05\n",
      "Loss:  1.6558915376663208e-05\n",
      "Loss:  3.446638584136963e-05\n",
      "Loss:  0.00013802386820316315\n",
      "Loss:  2.6166439056396484e-05\n",
      "Loss:  8.262414485216141e-05\n",
      "Loss:  0.00014390796422958374\n",
      "Loss:  5.475431680679321e-05\n",
      "Loss:  5.171261727809906e-05\n",
      "Loss:  1.7717480659484863e-05\n",
      "Loss:  0.00012911111116409302\n",
      "Loss:  2.6170164346694946e-05\n",
      "Loss:  3.773719072341919e-05\n",
      "Loss:  4.767812788486481e-05\n",
      "Loss:  9.386613965034485e-05\n",
      "Loss:  4.345923662185669e-05\n",
      "Epoch:  26\n",
      "Loss:  2.0503997802734375e-05\n",
      "Loss:  1.6335397958755493e-05\n",
      "Loss:  2.693571150302887e-05\n",
      "Loss:  4.0471553802490234e-05\n",
      "Loss:  0.0001130867749452591\n",
      "Loss:  3.604032099246979e-05\n",
      "Loss:  1.3010576367378235e-05\n",
      "Loss:  1.2990087270736694e-05\n",
      "Loss:  1.558661460876465e-05\n",
      "Loss:  9.009987115859985e-05\n",
      "Loss:  2.7177855372428894e-05\n",
      "Loss:  0.00012807920575141907\n",
      "Loss:  2.3443251848220825e-05\n",
      "Loss:  2.1051615476608276e-05\n",
      "Loss:  1.6430392861366272e-05\n",
      "Loss:  3.040209412574768e-05\n",
      "Loss:  2.6676803827285767e-05\n",
      "Loss:  2.100318670272827e-05\n",
      "Loss:  7.361173629760742e-06\n",
      "Loss:  1.1576339602470398e-05\n",
      "Loss:  2.069026231765747e-05\n",
      "Loss:  4.4990330934524536e-05\n",
      "Loss:  0.0001160595566034317\n",
      "Loss:  5.889125168323517e-05\n",
      "Loss:  5.772523581981659e-05\n",
      "Loss:  7.648766040802002e-05\n",
      "Loss:  4.743412137031555e-05\n",
      "Loss:  2.870894968509674e-05\n",
      "Loss:  7.643178105354309e-05\n",
      "Loss:  3.394857048988342e-05\n",
      "Loss:  0.00012410804629325867\n",
      "Loss:  9.328126907348633e-06\n",
      "Loss:  7.407739758491516e-06\n",
      "Loss:  3.721378743648529e-05\n",
      "Loss:  9.753741323947906e-05\n",
      "Loss:  5.3374096751213074e-05\n",
      "Loss:  9.696930646896362e-05\n",
      "Loss:  3.478117287158966e-05\n",
      "Loss:  3.1812116503715515e-05\n",
      "Loss:  5.951337516307831e-05\n",
      "Loss:  1.862645149230957e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.6744880676269531\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.156810998916626e-05\n",
      "Loss:  3.266334533691406e-05\n",
      "Loss:  0.00017263367772102356\n",
      "Loss:  4.601292312145233e-05\n",
      "Loss:  2.419017255306244e-05\n",
      "Loss:  3.875419497489929e-05\n",
      "Loss:  2.0012259483337402e-05\n",
      "Loss:  7.614493370056152e-06\n",
      "Loss:  1.938268542289734e-05\n",
      "Loss:  8.333846926689148e-05\n",
      "Loss:  0.00015700235962867737\n",
      "Loss:  1.627206802368164e-05\n",
      "Loss:  2.5743618607521057e-05\n",
      "Loss:  1.043081283569336e-05\n",
      "Loss:  4.453212022781372e-05\n",
      "Loss:  3.267824649810791e-05\n",
      "Loss:  4.419498145580292e-05\n",
      "Loss:  1.2967735528945923e-05\n",
      "Loss:  1.4021992683410645e-05\n",
      "Loss:  4.998035728931427e-05\n",
      "Loss:  3.725104033946991e-05\n",
      "Loss:  2.7138739824295044e-05\n",
      "Loss:  2.4743378162384033e-05\n",
      "Loss:  4.858523607254028e-05\n",
      "Loss:  1.1200085282325745e-05\n",
      "Loss:  1.8835067749023438e-05\n",
      "Loss:  3.204122185707092e-05\n",
      "Loss:  1.2110918760299683e-05\n",
      "Loss:  2.3670494556427002e-05\n",
      "Loss:  1.381710171699524e-05\n",
      "Loss:  7.139146327972412e-05\n",
      "Loss:  5.1429495215415955e-05\n",
      "Loss:  6.273947656154633e-05\n",
      "Loss:  8.67694616317749e-05\n",
      "Loss:  2.631545066833496e-05\n",
      "Loss:  1.7464160919189453e-05\n",
      "Loss:  2.277083694934845e-05\n",
      "Loss:  3.5569071769714355e-05\n",
      "Loss:  0.00012994185090065002\n",
      "Loss:  2.672336995601654e-05\n",
      "Loss:  3.107637166976929e-05\n",
      "Epoch:  27\n",
      "Loss:  2.0701438188552856e-05\n",
      "Loss:  4.4889748096466064e-06\n",
      "Loss:  2.442672848701477e-05\n",
      "Loss:  3.87243926525116e-06\n",
      "Loss:  2.3232772946357727e-05\n",
      "Loss:  3.4747645258903503e-05\n",
      "Loss:  5.459599196910858e-05\n",
      "Loss:  1.0676681995391846e-05\n",
      "Loss:  3.746524453163147e-05\n",
      "Loss:  3.0221417546272278e-05\n",
      "Loss:  0.000110626220703125\n",
      "Loss:  5.386769771575928e-06\n",
      "Loss:  1.4726072549819946e-05\n",
      "Loss:  2.8714537620544434e-05\n",
      "Loss:  1.0851770639419556e-05\n",
      "Loss:  1.2265518307685852e-05\n",
      "Loss:  6.25215470790863e-05\n",
      "Loss:  9.236857295036316e-05\n",
      "Loss:  7.832981646060944e-05\n",
      "Loss:  1.638941466808319e-05\n",
      "Loss:  2.251751720905304e-05\n",
      "Loss:  2.41156667470932e-05\n",
      "Loss:  2.5773420929908752e-05\n",
      "Loss:  3.666616976261139e-05\n",
      "Loss:  9.585171937942505e-06\n",
      "Loss:  2.619996666908264e-05\n",
      "Loss:  8.453242480754852e-05\n",
      "Loss:  7.545575499534607e-05\n",
      "Loss:  3.5235658288002014e-05\n",
      "Loss:  3.6973506212234497e-06\n",
      "Loss:  0.00014384649693965912\n",
      "Loss:  2.5019049644470215e-05\n",
      "Loss:  4.170462489128113e-05\n",
      "Loss:  1.239776611328125e-05\n",
      "Loss:  1.8944963812828064e-05\n",
      "Loss:  2.8409063816070557e-05\n",
      "Loss:  6.591901183128357e-06\n",
      "Loss:  2.9671937227249146e-05\n",
      "Loss:  3.0899420380592346e-05\n",
      "Loss:  2.485141158103943e-05\n",
      "Loss:  8.447220170637593e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7246601581573486\n",
      "Validation accuracy: 81%\n",
      "Loss:  6.928481161594391e-05\n",
      "Loss:  8.169561624526978e-06\n",
      "Loss:  2.6788562536239624e-05\n",
      "Loss:  1.6028061509132385e-05\n",
      "Loss:  2.788938581943512e-05\n",
      "Loss:  5.452893674373627e-05\n",
      "Loss:  7.027946412563324e-05\n",
      "Loss:  1.0114163160324097e-05\n",
      "Loss:  4.3330714106559753e-05\n",
      "Loss:  9.864568710327148e-06\n",
      "Loss:  3.5993754863739014e-05\n",
      "Loss:  2.2236257791519165e-05\n",
      "Loss:  8.398294448852539e-05\n",
      "Loss:  2.3193657398223877e-05\n",
      "Loss:  1.2405216693878174e-05\n",
      "Loss:  1.2196600437164307e-05\n",
      "Loss:  2.113915979862213e-05\n",
      "Loss:  3.059208393096924e-05\n",
      "Loss:  1.0028481483459473e-05\n",
      "Loss:  0.00015487335622310638\n",
      "Loss:  3.0104070901870728e-05\n",
      "Loss:  4.1676685214042664e-05\n",
      "Loss:  1.3742595911026001e-05\n",
      "Loss:  7.331371307373047e-05\n",
      "Loss:  1.7369166016578674e-05\n",
      "Loss:  2.979673445224762e-05\n",
      "Loss:  1.0866671800613403e-05\n",
      "Loss:  2.637505531311035e-05\n",
      "Loss:  2.4849548935890198e-05\n",
      "Loss:  8.571892976760864e-06\n",
      "Loss:  2.7779489755630493e-05\n",
      "Loss:  5.289912223815918e-06\n",
      "Loss:  3.7942081689834595e-05\n",
      "Loss:  3.696605563163757e-05\n",
      "Loss:  4.048086702823639e-05\n",
      "Loss:  3.7260353565216064e-05\n",
      "Loss:  1.8499791622161865e-05\n",
      "Loss:  2.2757798433303833e-05\n",
      "Loss:  6.156787276268005e-05\n",
      "Loss:  1.163966953754425e-05\n",
      "Loss:  2.7393301934353076e-05\n",
      "Epoch:  28\n",
      "Loss:  2.4741515517234802e-05\n",
      "Loss:  2.298690378665924e-05\n",
      "Loss:  6.941147148609161e-05\n",
      "Loss:  6.036832928657532e-06\n",
      "Loss:  2.7181580662727356e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.838329136371613e-05\n",
      "Loss:  3.376416862010956e-05\n",
      "Loss:  3.9381906390190125e-05\n",
      "Loss:  3.218837082386017e-05\n",
      "Loss:  1.270882785320282e-05\n",
      "Loss:  7.324293255805969e-05\n",
      "Loss:  6.755813956260681e-06\n",
      "Loss:  1.3379380106925964e-05\n",
      "Loss:  1.1453405022621155e-05\n",
      "Loss:  4.988163709640503e-05\n",
      "Loss:  1.560710370540619e-05\n",
      "Loss:  3.102794289588928e-05\n",
      "Loss:  5.5655837059021e-06\n",
      "Loss:  5.187466740608215e-05\n",
      "Loss:  2.954155206680298e-06\n",
      "Loss:  4.283711314201355e-05\n",
      "Loss:  1.881830394268036e-05\n",
      "Loss:  3.8662925362586975e-05\n",
      "Loss:  1.2880191206932068e-05\n",
      "Loss:  6.5656378865242e-05\n",
      "Loss:  2.1513551473617554e-05\n",
      "Loss:  3.653764724731445e-05\n",
      "Loss:  2.339296042919159e-05\n",
      "Loss:  4.040636122226715e-05\n",
      "Loss:  2.989731729030609e-05\n",
      "Loss:  1.2045726180076599e-05\n",
      "Loss:  6.975606083869934e-06\n",
      "Loss:  4.961155354976654e-05\n",
      "Loss:  2.2960826754570007e-05\n",
      "Loss:  1.4990568161010742e-05\n",
      "Loss:  1.298077404499054e-05\n",
      "Loss:  2.372823655605316e-05\n",
      "Loss:  7.392279803752899e-05\n",
      "Loss:  1.9511207938194275e-05\n",
      "Loss:  1.1876225471496582e-05\n",
      "Loss:  1.2236337170179468e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7217947244644165\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.5972182154655457e-05\n",
      "Loss:  5.541369318962097e-06\n",
      "Loss:  3.0973926186561584e-05\n",
      "Loss:  7.539987564086914e-06\n",
      "Loss:  8.322298526763916e-06\n",
      "Loss:  1.0849907994270325e-05\n",
      "Loss:  3.9540231227874756e-05\n",
      "Loss:  3.7863850593566895e-05\n",
      "Loss:  2.7837231755256653e-05\n",
      "Loss:  1.0667368769645691e-05\n",
      "Loss:  8.771941065788269e-05\n",
      "Loss:  3.2434239983558655e-05\n",
      "Loss:  1.8753111362457275e-05\n",
      "Loss:  4.4405460357666016e-06\n",
      "Loss:  1.9537284970283508e-05\n",
      "Loss:  3.368780016899109e-05\n",
      "Loss:  2.7433037757873535e-05\n",
      "Loss:  1.2582167983055115e-05\n",
      "Loss:  3.099814057350159e-05\n",
      "Loss:  7.275491952896118e-06\n",
      "Loss:  1.728534698486328e-05\n",
      "Loss:  6.312131881713867e-05\n",
      "Loss:  9.303353726863861e-05\n",
      "Loss:  8.52346420288086e-06\n",
      "Loss:  7.024034857749939e-06\n",
      "Loss:  3.688037395477295e-06\n",
      "Loss:  7.013417780399323e-05\n",
      "Loss:  4.297122359275818e-06\n",
      "Loss:  2.8327107429504395e-05\n",
      "Loss:  1.2764707207679749e-05\n",
      "Loss:  4.7050416469573975e-05\n",
      "Loss:  6.770715117454529e-06\n",
      "Loss:  1.12876296043396e-05\n",
      "Loss:  6.245449185371399e-06\n",
      "Loss:  1.802109181880951e-05\n",
      "Loss:  1.9311904907226562e-05\n",
      "Loss:  7.605738937854767e-05\n",
      "Loss:  4.275888204574585e-05\n",
      "Loss:  7.517635822296143e-06\n",
      "Loss:  4.2980536818504333e-05\n",
      "Loss:  8.504340803483501e-05\n",
      "Epoch:  29\n",
      "Loss:  1.4390796422958374e-05\n",
      "Loss:  5.463138222694397e-06\n",
      "Loss:  2.8368085622787476e-05\n",
      "Loss:  1.4554709196090698e-05\n",
      "Loss:  5.2522867918014526e-05\n",
      "Loss:  2.290494740009308e-05\n",
      "Loss:  6.84000551700592e-05\n",
      "Loss:  7.204711437225342e-06\n",
      "Loss:  1.5832483768463135e-05\n",
      "Loss:  1.514144241809845e-05\n",
      "Loss:  4.837289452552795e-06\n",
      "Loss:  7.482245564460754e-06\n",
      "Loss:  6.068311631679535e-05\n",
      "Loss:  5.206838250160217e-05\n",
      "Loss:  1.7175450921058655e-05\n",
      "Loss:  2.8504058718681335e-05\n",
      "Loss:  1.0877847671508789e-05\n",
      "Loss:  3.227405250072479e-05\n",
      "Loss:  1.7337501049041748e-05\n",
      "Loss:  7.214024662971497e-06\n",
      "Loss:  1.4390796422958374e-05\n",
      "Loss:  3.795698285102844e-05\n",
      "Loss:  1.6106292605400085e-05\n",
      "Loss:  2.7548521757125854e-05\n",
      "Loss:  4.479661583900452e-06\n",
      "Loss:  3.8370490074157715e-06\n",
      "Loss:  3.0005350708961487e-05\n",
      "Loss:  8.051842451095581e-05\n",
      "Loss:  7.491558790206909e-06\n",
      "Loss:  6.593950092792511e-05\n",
      "Loss:  9.728595614433289e-06\n",
      "Loss:  1.122988760471344e-05\n",
      "Loss:  2.3186206817626953e-05\n",
      "Loss:  6.7427754402160645e-06\n",
      "Loss:  3.7729740142822266e-05\n",
      "Loss:  5.09992241859436e-06\n",
      "Loss:  1.2442469596862793e-05\n",
      "Loss:  2.4788081645965576e-05\n",
      "Loss:  8.388422429561615e-05\n",
      "Loss:  1.6655772924423218e-05\n",
      "Loss:  1.753618380462285e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7560166120529175\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.5147030353546143e-05\n",
      "Loss:  1.6335397958755493e-05\n",
      "Loss:  2.802908420562744e-05\n",
      "Loss:  1.817941665649414e-05\n",
      "Loss:  6.107613444328308e-06\n",
      "Loss:  1.331232488155365e-05\n",
      "Loss:  2.1152198314666748e-05\n",
      "Loss:  1.8015503883361816e-05\n",
      "Loss:  3.826618194580078e-05\n",
      "Loss:  1.1408701539039612e-05\n",
      "Loss:  1.0691583156585693e-05\n",
      "Loss:  7.569789886474609e-06\n",
      "Loss:  1.5234574675559998e-05\n",
      "Loss:  1.4342367649078369e-05\n",
      "Loss:  5.1002949476242065e-05\n",
      "Loss:  6.616115570068359e-06\n",
      "Loss:  8.095055818557739e-05\n",
      "Loss:  4.5511871576309204e-05\n",
      "Loss:  6.062537431716919e-05\n",
      "Loss:  1.1214986443519592e-05\n",
      "Loss:  3.4788623452186584e-05\n",
      "Loss:  6.124377250671387e-06\n",
      "Loss:  1.2315809726715088e-05\n",
      "Loss:  4.396401345729828e-05\n",
      "Loss:  7.014721632003784e-06\n",
      "Loss:  2.7820467948913574e-05\n",
      "Loss:  1.8855556845664978e-05\n",
      "Loss:  8.085742592811584e-06\n",
      "Loss:  3.265775740146637e-05\n",
      "Loss:  5.058199167251587e-05\n",
      "Loss:  1.6462057828903198e-05\n",
      "Loss:  1.3029202818870544e-05\n",
      "Loss:  6.0830265283584595e-05\n",
      "Loss:  1.1272728443145752e-05\n",
      "Loss:  9.993091225624084e-06\n",
      "Loss:  2.4102628231048584e-06\n",
      "Loss:  3.680400550365448e-05\n",
      "Loss:  3.732368350028992e-05\n",
      "Loss:  7.202848792076111e-06\n",
      "Loss:  8.11740756034851e-06\n",
      "Loss:  4.5003991544945166e-05\n",
      "Epoch:  30\n",
      "Loss:  5.498528480529785e-06\n",
      "Loss:  1.4415010809898376e-05\n",
      "Loss:  7.729977369308472e-06\n",
      "Loss:  5.880370736122131e-06\n",
      "Loss:  4.9427151679992676e-05\n",
      "Loss:  2.5464221835136414e-05\n",
      "Loss:  2.152658998966217e-05\n",
      "Loss:  5.4817646741867065e-06\n",
      "Loss:  6.671994924545288e-06\n",
      "Loss:  3.11434268951416e-06\n",
      "Loss:  9.929761290550232e-06\n",
      "Loss:  2.1280720829963684e-05\n",
      "Loss:  1.1296942830085754e-05\n",
      "Loss:  1.3841316103935242e-05\n",
      "Loss:  1.9220635294914246e-05\n",
      "Loss:  7.681548595428467e-06\n",
      "Loss:  2.2640451788902283e-05\n",
      "Loss:  1.777149736881256e-05\n",
      "Loss:  1.0060146450996399e-05\n",
      "Loss:  0.0001011677086353302\n",
      "Loss:  3.124400973320007e-05\n",
      "Loss:  9.12696123123169e-06\n",
      "Loss:  5.7680532336235046e-05\n",
      "Loss:  1.535564661026001e-05\n",
      "Loss:  2.0362436771392822e-05\n",
      "Loss:  7.407739758491516e-06\n",
      "Loss:  4.6309083700180054e-05\n",
      "Loss:  4.388391971588135e-06\n",
      "Loss:  4.59868460893631e-05\n",
      "Loss:  7.27921724319458e-06\n",
      "Loss:  1.6072764992713928e-05\n",
      "Loss:  1.92299485206604e-05\n",
      "Loss:  2.3717060685157776e-05\n",
      "Loss:  1.5949830412864685e-05\n",
      "Loss:  2.1880492568016052e-05\n",
      "Loss:  4.989653825759888e-05\n",
      "Loss:  8.381903171539307e-06\n",
      "Loss:  5.1626935601234436e-05\n",
      "Loss:  5.755573511123657e-06\n",
      "Loss:  1.5098601579666138e-05\n",
      "Loss:  2.205620148743037e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7779982089996338\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.882257103919983e-05\n",
      "Loss:  1.1261552572250366e-05\n",
      "Loss:  3.7997961044311523e-06\n",
      "Loss:  8.643046021461487e-05\n",
      "Loss:  2.127140760421753e-05\n",
      "Loss:  2.1122395992279053e-06\n",
      "Loss:  2.9031187295913696e-05\n",
      "Loss:  2.057291567325592e-05\n",
      "Loss:  1.3787299394607544e-05\n",
      "Loss:  1.66967511177063e-05\n",
      "Loss:  1.892074942588806e-05\n",
      "Loss:  1.3045966625213623e-05\n",
      "Loss:  2.8314068913459778e-05\n",
      "Loss:  1.3474375009536743e-05\n",
      "Loss:  1.0030344128608704e-05\n",
      "Loss:  7.452443242073059e-06\n",
      "Loss:  7.661059498786926e-06\n",
      "Loss:  1.3509765267372131e-05\n",
      "Loss:  2.965889871120453e-05\n",
      "Loss:  2.9636546969413757e-05\n",
      "Loss:  9.912997484207153e-06\n",
      "Loss:  3.203749656677246e-06\n",
      "Loss:  5.1738694310188293e-05\n",
      "Loss:  1.0795891284942627e-05\n",
      "Loss:  1.5271827578544617e-05\n",
      "Loss:  2.7643516659736633e-05\n",
      "Loss:  1.1185184121131897e-05\n",
      "Loss:  2.0934268832206726e-05\n",
      "Loss:  7.878988981246948e-06\n",
      "Loss:  5.928799510002136e-05\n",
      "Loss:  7.729977369308472e-06\n",
      "Loss:  1.0943040251731873e-05\n",
      "Loss:  1.85258686542511e-05\n",
      "Loss:  4.346296191215515e-05\n",
      "Loss:  1.2049451470375061e-05\n",
      "Loss:  3.121607005596161e-05\n",
      "Loss:  1.271069049835205e-05\n",
      "Loss:  5.9176236391067505e-06\n",
      "Loss:  1.0108575224876404e-05\n",
      "Loss:  4.945509135723114e-05\n",
      "Loss:  1.4175971045915503e-05\n",
      "Epoch:  31\n",
      "Loss:  8.130446076393127e-06\n",
      "Loss:  4.89354133605957e-05\n",
      "Loss:  5.1019713282585144e-05\n",
      "Loss:  1.9762665033340454e-05\n",
      "Loss:  8.169561624526978e-06\n",
      "Loss:  1.6676262021064758e-05\n",
      "Loss:  6.057322025299072e-06\n",
      "Loss:  1.41877681016922e-05\n",
      "Loss:  1.0475516319274902e-05\n",
      "Loss:  1.1434778571128845e-05\n",
      "Loss:  2.0563602447509766e-06\n",
      "Loss:  1.710467040538788e-05\n",
      "Loss:  6.491318345069885e-06\n",
      "Loss:  8.11554491519928e-06\n",
      "Loss:  3.571808338165283e-05\n",
      "Loss:  2.1414831280708313e-05\n",
      "Loss:  2.898089587688446e-05\n",
      "Loss:  1.5221536159515381e-05\n",
      "Loss:  3.982335329055786e-05\n",
      "Loss:  1.8538907170295715e-05\n",
      "Loss:  4.324503242969513e-05\n",
      "Loss:  3.0377879738807678e-05\n",
      "Loss:  2.008490264415741e-05\n",
      "Loss:  4.284083843231201e-05\n",
      "Loss:  5.757436156272888e-06\n",
      "Loss:  8.033588528633118e-06\n",
      "Loss:  1.2112781405448914e-05\n",
      "Loss:  1.1937692761421204e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.4934688806533813e-05\n",
      "Loss:  9.978190064430237e-06\n",
      "Loss:  1.2749806046485901e-05\n",
      "Loss:  1.6925856471061707e-05\n",
      "Loss:  4.980713129043579e-06\n",
      "Loss:  3.0100345611572266e-06\n",
      "Loss:  1.0933727025985718e-05\n",
      "Loss:  5.5108219385147095e-05\n",
      "Loss:  3.388151526451111e-05\n",
      "Loss:  5.668029189109802e-06\n",
      "Loss:  4.721805453300476e-06\n",
      "Loss:  1.348927617073059e-05\n",
      "Loss:  9.86953546089353e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.7802376747131348\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.8166378140449524e-05\n",
      "Loss:  5.6939199566841125e-05\n",
      "Loss:  1.3580545783042908e-05\n",
      "Loss:  4.289671778678894e-06\n",
      "Loss:  5.462951958179474e-05\n",
      "Loss:  9.734183549880981e-06\n",
      "Loss:  3.512948751449585e-06\n",
      "Loss:  7.253140211105347e-06\n",
      "Loss:  1.928582787513733e-05\n",
      "Loss:  1.6314908862113953e-05\n",
      "Loss:  6.703659892082214e-06\n",
      "Loss:  1.4277175068855286e-05\n",
      "Loss:  2.7494505047798157e-05\n",
      "Loss:  2.8505921363830566e-05\n",
      "Loss:  8.480623364448547e-06\n",
      "Loss:  9.121373295783997e-06\n",
      "Loss:  5.416572093963623e-06\n",
      "Loss:  1.5217810869216919e-05\n",
      "Loss:  8.01868736743927e-06\n",
      "Loss:  1.749396324157715e-05\n",
      "Loss:  1.6180798411369324e-05\n",
      "Loss:  4.597194492816925e-05\n",
      "Loss:  2.0675361156463623e-05\n",
      "Loss:  3.0508264899253845e-05\n",
      "Loss:  1.0259449481964111e-05\n",
      "Loss:  7.092952728271484e-06\n",
      "Loss:  4.179216921329498e-05\n",
      "Loss:  1.3396143913269043e-05\n",
      "Loss:  2.6598572731018066e-06\n",
      "Loss:  1.436471939086914e-05\n",
      "Loss:  9.08970832824707e-06\n",
      "Loss:  1.977570354938507e-05\n",
      "Loss:  1.4137476682662964e-05\n",
      "Loss:  8.7786465883255e-06\n",
      "Loss:  2.1548941731452942e-05\n",
      "Loss:  1.2326985597610474e-05\n",
      "Loss:  3.078952431678772e-06\n",
      "Loss:  5.1800161600112915e-06\n",
      "Loss:  7.422640919685364e-06\n",
      "Loss:  7.717125117778778e-05\n",
      "Loss:  7.160007953643799e-06\n",
      "Epoch:  32\n",
      "Loss:  3.2372772693634033e-06\n",
      "Loss:  9.540468454360962e-06\n",
      "Loss:  2.3229047656059265e-05\n",
      "Loss:  1.9185245037078857e-06\n",
      "Loss:  4.0084123611450195e-06\n",
      "Loss:  4.403293132781982e-06\n",
      "Loss:  6.113015115261078e-05\n",
      "Loss:  9.700655937194824e-06\n",
      "Loss:  3.1888484954833984e-06\n",
      "Loss:  1.121126115322113e-05\n",
      "Loss:  1.7253682017326355e-05\n",
      "Loss:  8.296221494674683e-06\n",
      "Loss:  1.1473894119262695e-05\n",
      "Loss:  8.851289749145508e-06\n",
      "Loss:  1.1799857020378113e-05\n",
      "Loss:  2.1509826183319092e-05\n",
      "Loss:  9.976327419281006e-06\n",
      "Loss:  8.228607475757599e-05\n",
      "Loss:  1.5093013644218445e-05\n",
      "Loss:  4.673376679420471e-06\n",
      "Loss:  1.0598450899124146e-05\n",
      "Loss:  5.973502993583679e-06\n",
      "Loss:  8.44523310661316e-06\n",
      "Loss:  5.999580025672913e-06\n",
      "Loss:  2.5112181901931763e-05\n",
      "Loss:  3.0238181352615356e-05\n",
      "Loss:  1.4906749129295349e-05\n",
      "Loss:  6.504356861114502e-06\n",
      "Loss:  2.5369226932525635e-06\n",
      "Loss:  2.575106918811798e-05\n",
      "Loss:  3.628432750701904e-05\n",
      "Loss:  4.380755126476288e-05\n",
      "Loss:  7.776543498039246e-06\n",
      "Loss:  6.113201379776001e-06\n",
      "Loss:  4.345551133155823e-06\n",
      "Loss:  1.8768012523651123e-05\n",
      "Loss:  1.1119991540908813e-05\n",
      "Loss:  1.9822269678115845e-05\n",
      "Loss:  2.207234501838684e-05\n",
      "Loss:  3.175996243953705e-05\n",
      "Loss:  1.1652708053588867e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.789062261581421\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.8508325815200806e-05\n",
      "Loss:  7.33695924282074e-06\n",
      "Loss:  1.3167038559913635e-05\n",
      "Loss:  3.2410025596618652e-06\n",
      "Loss:  1.381710171699524e-05\n",
      "Loss:  3.7202611565589905e-05\n",
      "Loss:  1.189112663269043e-05\n",
      "Loss:  7.247552275657654e-06\n",
      "Loss:  3.187358379364014e-05\n",
      "Loss:  3.585591912269592e-05\n",
      "Loss:  4.300661385059357e-05\n",
      "Loss:  3.829970955848694e-05\n",
      "Loss:  3.4943222999572754e-06\n",
      "Loss:  5.00120222568512e-06\n",
      "Loss:  1.0963529348373413e-05\n",
      "Loss:  4.129484295845032e-06\n",
      "Loss:  1.5564262866973877e-05\n",
      "Loss:  5.733594298362732e-05\n",
      "Loss:  2.3646280169487e-05\n",
      "Loss:  9.737908840179443e-06\n",
      "Loss:  4.209578037261963e-06\n",
      "Loss:  9.298324584960938e-06\n",
      "Loss:  3.8584694266319275e-05\n",
      "Loss:  6.12996518611908e-06\n",
      "Loss:  3.8351863622665405e-06\n",
      "Loss:  1.280568540096283e-05\n",
      "Loss:  1.6359612345695496e-05\n",
      "Loss:  1.337006688117981e-05\n",
      "Loss:  5.198642611503601e-06\n",
      "Loss:  4.490837454795837e-06\n",
      "Loss:  2.164579927921295e-05\n",
      "Loss:  1.1479482054710388e-05\n",
      "Loss:  9.42125916481018e-06\n",
      "Loss:  1.7562881112098694e-05\n",
      "Loss:  4.258006811141968e-06\n",
      "Loss:  1.7136335372924805e-06\n",
      "Loss:  1.4504417777061462e-05\n",
      "Loss:  7.759779691696167e-06\n",
      "Loss:  6.798654794692993e-06\n",
      "Loss:  2.417340874671936e-05\n",
      "Loss:  3.1888484954833984e-06\n",
      "Epoch:  33\n",
      "Loss:  8.97236168384552e-06\n",
      "Loss:  7.02589750289917e-06\n",
      "Loss:  1.6001984477043152e-05\n",
      "Loss:  1.8367543816566467e-05\n",
      "Loss:  4.120171070098877e-06\n",
      "Loss:  1.0967254638671875e-05\n",
      "Loss:  1.150183379650116e-05\n",
      "Loss:  1.4415010809898376e-05\n",
      "Loss:  1.3152137398719788e-05\n",
      "Loss:  4.932284355163574e-06\n",
      "Loss:  4.5746564865112305e-06\n",
      "Loss:  7.029622793197632e-06\n",
      "Loss:  2.1012499928474426e-05\n",
      "Loss:  8.065253496170044e-06\n",
      "Loss:  5.7443976402282715e-06\n",
      "Loss:  1.4940276741981506e-05\n",
      "Loss:  3.1776726245880127e-06\n",
      "Loss:  3.8817524909973145e-06\n",
      "Loss:  2.4586915969848633e-05\n",
      "Loss:  3.522634506225586e-05\n",
      "Loss:  1.6070902347564697e-05\n",
      "Loss:  2.8235837817192078e-05\n",
      "Loss:  7.029622793197632e-06\n",
      "Loss:  1.1706724762916565e-05\n",
      "Loss:  3.0353665351867676e-05\n",
      "Loss:  3.4011900424957275e-06\n",
      "Loss:  1.298077404499054e-05\n",
      "Loss:  2.086162567138672e-06\n",
      "Loss:  2.0872801542282104e-05\n",
      "Loss:  1.0475516319274902e-05\n",
      "Loss:  1.3615936040878296e-05\n",
      "Loss:  3.0063092708587646e-06\n",
      "Loss:  3.1171366572380066e-05\n",
      "Loss:  3.858096897602081e-05\n",
      "Loss:  1.4917925000190735e-05\n",
      "Loss:  2.3398548364639282e-05\n",
      "Loss:  5.949288606643677e-06\n",
      "Loss:  3.724731504917145e-05\n",
      "Loss:  2.049282193183899e-05\n",
      "Loss:  4.850327968597412e-06\n",
      "Loss:  2.1805366486660205e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.824665904045105\n",
      "Validation accuracy: 81%\n",
      "Loss:  9.329989552497864e-06\n",
      "Loss:  7.651746273040771e-06\n",
      "Loss:  1.0017305612564087e-05\n",
      "Loss:  1.6734004020690918e-05\n",
      "Loss:  3.72864305973053e-05\n",
      "Loss:  4.317611455917358e-06\n",
      "Loss:  2.146884799003601e-05\n",
      "Loss:  2.507120370864868e-06\n",
      "Loss:  1.2587755918502808e-05\n",
      "Loss:  6.290152668952942e-06\n",
      "Loss:  3.248639404773712e-05\n",
      "Loss:  3.5762786865234375e-06\n",
      "Loss:  3.1962990760803223e-06\n",
      "Loss:  1.6024336218833923e-05\n",
      "Loss:  4.328787326812744e-06\n",
      "Loss:  1.3407319784164429e-05\n",
      "Loss:  1.206621527671814e-05\n",
      "Loss:  1.1947005987167358e-05\n",
      "Loss:  1.6646459698677063e-05\n",
      "Loss:  2.3407861590385437e-05\n",
      "Loss:  5.142763257026672e-06\n",
      "Loss:  2.141110599040985e-05\n",
      "Loss:  2.98917293548584e-05\n",
      "Loss:  3.993511199951172e-05\n",
      "Loss:  3.56137752532959e-06\n",
      "Loss:  1.3414770364761353e-05\n",
      "Loss:  1.547299325466156e-05\n",
      "Loss:  5.2209943532943726e-06\n",
      "Loss:  4.751235246658325e-05\n",
      "Loss:  2.201646566390991e-06\n",
      "Loss:  1.1688098311424255e-05\n",
      "Loss:  3.7457793951034546e-06\n",
      "Loss:  1.7350539565086365e-05\n",
      "Loss:  1.3597309589385986e-06\n",
      "Loss:  1.2720003724098206e-05\n",
      "Loss:  1.3057142496109009e-05\n",
      "Loss:  1.0484829545021057e-05\n",
      "Loss:  3.1966716051101685e-05\n",
      "Loss:  1.0810792446136475e-05\n",
      "Loss:  2.5816261768341064e-06\n",
      "Loss:  1.0843078598554712e-05\n",
      "Epoch:  34\n",
      "Loss:  4.373490810394287e-06\n",
      "Loss:  2.2433698177337646e-05\n",
      "Loss:  9.858980774879456e-06\n",
      "Loss:  8.342787623405457e-06\n",
      "Loss:  1.0542571544647217e-05\n",
      "Loss:  8.89599323272705e-06\n",
      "Loss:  3.653019666671753e-05\n",
      "Loss:  2.685748040676117e-05\n",
      "Loss:  2.562999725341797e-06\n",
      "Loss:  9.20705497264862e-06\n",
      "Loss:  4.298985004425049e-06\n",
      "Loss:  7.055699825286865e-06\n",
      "Loss:  2.7865171432495117e-06\n",
      "Loss:  2.78521329164505e-05\n",
      "Loss:  9.099021553993225e-06\n",
      "Loss:  4.783272743225098e-06\n",
      "Loss:  5.986541509628296e-06\n",
      "Loss:  1.2030825018882751e-05\n",
      "Loss:  2.808310091495514e-05\n",
      "Loss:  1.436285674571991e-05\n",
      "Loss:  2.436339855194092e-06\n",
      "Loss:  1.3889744877815247e-05\n",
      "Loss:  4.924461245536804e-05\n",
      "Loss:  7.878988981246948e-06\n",
      "Loss:  1.0751187801361084e-05\n",
      "Loss:  8.884817361831665e-06\n",
      "Loss:  9.3020498752594e-06\n",
      "Loss:  3.655068576335907e-05\n",
      "Loss:  6.418675184249878e-06\n",
      "Loss:  2.682209014892578e-06\n",
      "Loss:  1.3163313269615173e-05\n",
      "Loss:  1.802295446395874e-05\n",
      "Loss:  1.1451542377471924e-05\n",
      "Loss:  8.584931492805481e-06\n",
      "Loss:  7.2196125984191895e-06\n",
      "Loss:  1.4182180166244507e-05\n",
      "Loss:  2.2357329726219177e-05\n",
      "Loss:  8.674338459968567e-06\n",
      "Loss:  1.1026859283447266e-05\n",
      "Loss:  1.8328428268432617e-06\n",
      "Loss:  1.492847968620481e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8353631496429443\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.0001083612442017e-05\n",
      "Loss:  5.301088094711304e-06\n",
      "Loss:  2.942606806755066e-05\n",
      "Loss:  1.887977123260498e-05\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  2.9355287551879883e-06\n",
      "Loss:  6.798654794692993e-06\n",
      "Loss:  7.418915629386902e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.2882595658302307e-05\n",
      "Loss:  2.0489096641540527e-06\n",
      "Loss:  6.023794412612915e-06\n",
      "Loss:  2.2416934370994568e-05\n",
      "Loss:  1.0522082448005676e-05\n",
      "Loss:  1.961737871170044e-05\n",
      "Loss:  7.227063179016113e-06\n",
      "Loss:  1.3597309589385986e-05\n",
      "Loss:  1.8483027815818787e-05\n",
      "Loss:  1.4727935194969177e-05\n",
      "Loss:  1.3438984751701355e-05\n",
      "Loss:  1.385621726512909e-05\n",
      "Loss:  4.464760422706604e-06\n",
      "Loss:  7.927417755126953e-06\n",
      "Loss:  1.1458992958068848e-05\n",
      "Loss:  2.765469253063202e-05\n",
      "Loss:  3.948807716369629e-06\n",
      "Loss:  3.399699926376343e-05\n",
      "Loss:  2.1276995539665222e-05\n",
      "Loss:  5.185604095458984e-06\n",
      "Loss:  3.032386302947998e-06\n",
      "Loss:  1.0382384061813354e-05\n",
      "Loss:  3.9987266063690186e-05\n",
      "Loss:  2.8703361749649048e-05\n",
      "Loss:  8.715316653251648e-06\n",
      "Loss:  8.217990398406982e-06\n",
      "Loss:  1.2023374438285828e-05\n",
      "Loss:  4.556030035018921e-06\n",
      "Loss:  2.291053533554077e-05\n",
      "Loss:  3.5706907510757446e-06\n",
      "Loss:  4.716217517852783e-06\n",
      "Loss:  8.903443813323975e-07\n",
      "Loss:  6.38266419628053e-06\n",
      "Epoch:  35\n",
      "Loss:  1.2638047337532043e-05\n",
      "Loss:  3.0156224966049194e-06\n",
      "Loss:  1.7480924725532532e-05\n",
      "Loss:  3.0044466257095337e-05\n",
      "Loss:  5.170702934265137e-06\n",
      "Loss:  8.609145879745483e-06\n",
      "Loss:  5.319714546203613e-06\n",
      "Loss:  1.0164454579353333e-05\n",
      "Loss:  5.202367901802063e-06\n",
      "Loss:  2.983957529067993e-06\n",
      "Loss:  4.205852746963501e-06\n",
      "Loss:  1.6072764992713928e-05\n",
      "Loss:  2.3564323782920837e-05\n",
      "Loss:  1.2205913662910461e-05\n",
      "Loss:  9.959563612937927e-06\n",
      "Loss:  2.865120768547058e-05\n",
      "Loss:  1.1403113603591919e-05\n",
      "Loss:  7.398426532745361e-06\n",
      "Loss:  5.209818482398987e-06\n",
      "Loss:  4.189088940620422e-06\n",
      "Loss:  8.219853043556213e-06\n",
      "Loss:  5.340203642845154e-06\n",
      "Loss:  4.98257577419281e-06\n",
      "Loss:  1.1134892702102661e-05\n",
      "Loss:  3.1774863600730896e-05\n",
      "Loss:  3.6712735891342163e-06\n",
      "Loss:  2.3907050490379333e-05\n",
      "Loss:  1.3094395399093628e-05\n",
      "Loss:  1.7609447240829468e-05\n",
      "Loss:  1.3070181012153625e-05\n",
      "Loss:  2.1532177925109863e-06\n",
      "Loss:  1.9477680325508118e-05\n",
      "Loss:  1.5635043382644653e-05\n",
      "Loss:  8.204951882362366e-06\n",
      "Loss:  7.292255759239197e-06\n",
      "Loss:  2.2528693079948425e-05\n",
      "Loss:  1.6110017895698547e-05\n",
      "Loss:  1.0458752512931824e-05\n",
      "Loss:  7.836148142814636e-06\n",
      "Loss:  3.0994415283203125e-06\n",
      "Loss:  1.241763402504148e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8446869850158691\n",
      "Validation accuracy: 81%\n",
      "Loss:  6.016343832015991e-06\n",
      "Loss:  7.595866918563843e-06\n",
      "Loss:  1.0766088962554932e-06\n",
      "Loss:  2.4478882551193237e-05\n",
      "Loss:  1.73225998878479e-06\n",
      "Loss:  9.11019742488861e-06\n",
      "Loss:  3.835931420326233e-05\n",
      "Loss:  1.4046207070350647e-05\n",
      "Loss:  6.852671504020691e-06\n",
      "Loss:  6.6570937633514404e-06\n",
      "Loss:  9.814277291297913e-06\n",
      "Loss:  1.294352114200592e-05\n",
      "Loss:  3.145076334476471e-05\n",
      "Loss:  2.4177134037017822e-06\n",
      "Loss:  6.0498714447021484e-06\n",
      "Loss:  3.7532299757003784e-06\n",
      "Loss:  4.578381776809692e-06\n",
      "Loss:  2.689659595489502e-05\n",
      "Loss:  2.5952234864234924e-05\n",
      "Loss:  1.7024576663970947e-05\n",
      "Loss:  1.8924474716186523e-06\n",
      "Loss:  1.187250018119812e-05\n",
      "Loss:  3.6098062992095947e-06\n",
      "Loss:  2.9690563678741455e-06\n",
      "Loss:  1.7976388335227966e-05\n",
      "Loss:  3.0882656574249268e-06\n",
      "Loss:  8.339062333106995e-06\n",
      "Loss:  4.5299530029296875e-06\n",
      "Loss:  4.339590668678284e-05\n",
      "Loss:  9.10833477973938e-06\n",
      "Loss:  6.670132279396057e-06\n",
      "Loss:  1.66427344083786e-05\n",
      "Loss:  9.30391252040863e-06\n",
      "Loss:  2.870708703994751e-05\n",
      "Loss:  1.50240957736969e-05\n",
      "Loss:  2.3115426301956177e-06\n",
      "Loss:  9.01147723197937e-06\n",
      "Loss:  7.614493370056152e-06\n",
      "Loss:  1.948326826095581e-06\n",
      "Loss:  2.130866050720215e-06\n",
      "Loss:  1.7459193486502045e-06\n",
      "Epoch:  36\n",
      "Loss:  2.063252031803131e-05\n",
      "Loss:  1.8514692783355713e-06\n",
      "Loss:  9.702518582344055e-06\n",
      "Loss:  1.6763806343078613e-06\n",
      "Loss:  7.798895239830017e-06\n",
      "Loss:  1.5672296285629272e-05\n",
      "Loss:  3.416091203689575e-06\n",
      "Loss:  7.95908272266388e-06\n",
      "Loss:  1.3396143913269043e-05\n",
      "Loss:  3.118067979812622e-06\n",
      "Loss:  2.8870999813079834e-06\n",
      "Loss:  2.7416273951530457e-05\n",
      "Loss:  1.4310702681541443e-05\n",
      "Loss:  7.450580596923828e-06\n",
      "Loss:  1.0598450899124146e-05\n",
      "Loss:  3.87243926525116e-05\n",
      "Loss:  1.7132610082626343e-05\n",
      "Loss:  3.6098062992095947e-06\n",
      "Loss:  4.926696419715881e-06\n",
      "Loss:  3.128126263618469e-05\n",
      "Loss:  2.592802047729492e-06\n",
      "Loss:  2.1792948246002197e-06\n",
      "Loss:  5.751848220825195e-06\n",
      "Loss:  8.89413058757782e-06\n",
      "Loss:  8.042901754379272e-06\n",
      "Loss:  4.8354268074035645e-06\n",
      "Loss:  4.801899194717407e-06\n",
      "Loss:  3.63960862159729e-06\n",
      "Loss:  5.453824996948242e-06\n",
      "Loss:  1.2772157788276672e-05\n",
      "Loss:  1.8738210201263428e-06\n",
      "Loss:  3.2782554626464844e-06\n",
      "Loss:  3.581121563911438e-05\n",
      "Loss:  1.0319054126739502e-05\n",
      "Loss:  6.841495633125305e-06\n",
      "Loss:  9.57585871219635e-06\n",
      "Loss:  2.036057412624359e-05\n",
      "Loss:  3.2689422369003296e-06\n",
      "Loss:  2.5583431124687195e-05\n",
      "Loss:  2.6114284992218018e-06\n",
      "Loss:  1.5253822311933618e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8668991327285767\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.4085864424705505e-05\n",
      "Loss:  8.217990398406982e-06\n",
      "Loss:  1.545436680316925e-05\n",
      "Loss:  1.1518597602844238e-05\n",
      "Loss:  7.510185241699219e-06\n",
      "Loss:  5.2675604820251465e-06\n",
      "Loss:  1.7920508980751038e-05\n",
      "Loss:  6.375834345817566e-06\n",
      "Loss:  5.29363751411438e-06\n",
      "Loss:  1.8924474716186523e-06\n",
      "Loss:  7.366761565208435e-06\n",
      "Loss:  5.634501576423645e-06\n",
      "Loss:  4.637986421585083e-06\n",
      "Loss:  1.6532838344573975e-05\n",
      "Loss:  2.2727996110916138e-05\n",
      "Loss:  4.123896360397339e-06\n",
      "Loss:  6.455928087234497e-06\n",
      "Loss:  7.158145308494568e-06\n",
      "Loss:  1.5236437320709229e-05\n",
      "Loss:  1.8998980522155762e-06\n",
      "Loss:  1.1323019862174988e-05\n",
      "Loss:  7.674098014831543e-07\n",
      "Loss:  5.496665835380554e-06\n",
      "Loss:  2.4586915969848633e-06\n",
      "Loss:  2.041645348072052e-05\n",
      "Loss:  1.2069940567016602e-05\n",
      "Loss:  2.596154808998108e-05\n",
      "Loss:  3.7122517824172974e-06\n",
      "Loss:  1.62515789270401e-05\n",
      "Loss:  2.3404136300086975e-05\n",
      "Loss:  6.6366046667099e-06\n",
      "Loss:  3.509223461151123e-06\n",
      "Loss:  3.8053840398788452e-06\n",
      "Loss:  2.115964889526367e-06\n",
      "Loss:  2.868473529815674e-06\n",
      "Loss:  2.641044557094574e-05\n",
      "Loss:  5.321577191352844e-06\n",
      "Loss:  4.546716809272766e-06\n",
      "Loss:  3.688596189022064e-05\n",
      "Loss:  6.278976798057556e-06\n",
      "Loss:  7.616976745339343e-06\n",
      "Epoch:  37\n",
      "Loss:  1.2226402759552002e-05\n",
      "Loss:  2.682209014892578e-06\n",
      "Loss:  3.295019268989563e-06\n",
      "Loss:  2.123415470123291e-06\n",
      "Loss:  2.5294721126556396e-06\n",
      "Loss:  7.1730464696884155e-06\n",
      "Loss:  6.066635251045227e-06\n",
      "Loss:  1.3340264558792114e-05\n",
      "Loss:  1.0075047612190247e-05\n",
      "Loss:  8.106231689453125e-06\n",
      "Loss:  2.823770046234131e-06\n",
      "Loss:  3.071501851081848e-06\n",
      "Loss:  1.9991770386695862e-05\n",
      "Loss:  3.55597585439682e-05\n",
      "Loss:  1.9570812582969666e-05\n",
      "Loss:  3.576651215553284e-05\n",
      "Loss:  3.127753734588623e-05\n",
      "Loss:  2.333708107471466e-05\n",
      "Loss:  2.2798776626586914e-06\n",
      "Loss:  6.649643182754517e-06\n",
      "Loss:  5.844980478286743e-06\n",
      "Loss:  4.433095455169678e-06\n",
      "Loss:  3.475695848464966e-06\n",
      "Loss:  4.190951585769653e-06\n",
      "Loss:  7.661059498786926e-06\n",
      "Loss:  1.2256205081939697e-05\n",
      "Loss:  1.3167038559913635e-05\n",
      "Loss:  4.084780812263489e-06\n",
      "Loss:  4.980713129043579e-06\n",
      "Loss:  3.974884748458862e-06\n",
      "Loss:  6.807968020439148e-06\n",
      "Loss:  4.295259714126587e-06\n",
      "Loss:  1.241825520992279e-05\n",
      "Loss:  2.421438694000244e-06\n",
      "Loss:  1.0935589671134949e-05\n",
      "Loss:  3.255903720855713e-06\n",
      "Loss:  2.684071660041809e-06\n",
      "Loss:  4.932284355163574e-06\n",
      "Loss:  1.0302290320396423e-05\n",
      "Loss:  2.998858690261841e-06\n",
      "Loss:  3.0199686079868115e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8746821880340576\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.987069845199585e-05\n",
      "Loss:  8.432194590568542e-06\n",
      "Loss:  1.7471611499786377e-06\n",
      "Loss:  5.66430389881134e-06\n",
      "Loss:  1.222267746925354e-05\n",
      "Loss:  2.1589919924736023e-05\n",
      "Loss:  4.86522912979126e-06\n",
      "Loss:  1.3826414942741394e-05\n",
      "Loss:  6.183981895446777e-06\n",
      "Loss:  2.9671937227249146e-06\n",
      "Loss:  4.621222615242004e-06\n",
      "Loss:  2.034008502960205e-06\n",
      "Loss:  2.489425241947174e-05\n",
      "Loss:  1.012161374092102e-05\n",
      "Loss:  2.2970139980316162e-05\n",
      "Loss:  7.996335625648499e-06\n",
      "Loss:  1.4780089259147644e-05\n",
      "Loss:  2.7194619178771973e-06\n",
      "Loss:  2.635642886161804e-06\n",
      "Loss:  3.9245933294296265e-06\n",
      "Loss:  5.243346095085144e-06\n",
      "Loss:  7.964670658111572e-06\n",
      "Loss:  4.511326551437378e-06\n",
      "Loss:  2.7103349566459656e-05\n",
      "Loss:  4.9658119678497314e-06\n",
      "Loss:  4.336237907409668e-06\n",
      "Loss:  9.620562195777893e-06\n",
      "Loss:  2.32551246881485e-05\n",
      "Loss:  3.809109330177307e-06\n",
      "Loss:  8.631497621536255e-06\n",
      "Loss:  1.580268144607544e-05\n",
      "Loss:  6.07222318649292e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.3709068298339844e-06\n",
      "Loss:  5.552545189857483e-06\n",
      "Loss:  6.3497573137283325e-06\n",
      "Loss:  4.2673200368881226e-06\n",
      "Loss:  1.401267945766449e-05\n",
      "Loss:  1.0836869478225708e-05\n",
      "Loss:  4.455447196960449e-06\n",
      "Loss:  1.821480691432953e-05\n",
      "Loss:  1.5904506653896533e-05\n",
      "Epoch:  38\n",
      "Loss:  7.33695924282074e-06\n",
      "Loss:  1.0661780834197998e-05\n",
      "Loss:  3.084540367126465e-06\n",
      "Loss:  6.3087791204452515e-06\n",
      "Loss:  3.2141804695129395e-05\n",
      "Loss:  2.137012779712677e-05\n",
      "Loss:  7.601454854011536e-06\n",
      "Loss:  2.131424844264984e-05\n",
      "Loss:  2.298504114151001e-06\n",
      "Loss:  1.1105090379714966e-05\n",
      "Loss:  3.1385570764541626e-06\n",
      "Loss:  3.946945071220398e-06\n",
      "Loss:  3.680586814880371e-06\n",
      "Loss:  1.989305019378662e-06\n",
      "Loss:  2.7194619178771973e-05\n",
      "Loss:  3.7159770727157593e-06\n",
      "Loss:  4.7050416469573975e-06\n",
      "Loss:  2.5853514671325684e-06\n",
      "Loss:  4.9211084842681885e-06\n",
      "Loss:  9.164214134216309e-06\n",
      "Loss:  3.417953848838806e-06\n",
      "Loss:  9.000301361083984e-06\n",
      "Loss:  7.202848792076111e-06\n",
      "Loss:  3.825873136520386e-06\n",
      "Loss:  2.603977918624878e-06\n",
      "Loss:  3.4421682357788086e-06\n",
      "Loss:  1.817941665649414e-06\n",
      "Loss:  2.1360814571380615e-05\n",
      "Loss:  2.4784356355667114e-05\n",
      "Loss:  1.1363998055458069e-05\n",
      "Loss:  3.9711594581604e-06\n",
      "Loss:  8.940696716308594e-06\n",
      "Loss:  5.677342414855957e-06\n",
      "Loss:  1.218169927597046e-06\n",
      "Loss:  1.2740492820739746e-06\n",
      "Loss:  9.389594197273254e-06\n",
      "Loss:  2.8870999813079834e-06\n",
      "Loss:  3.468245267868042e-06\n",
      "Loss:  4.636123776435852e-06\n",
      "Loss:  1.0592862963676453e-05\n",
      "Loss:  4.1613977373344824e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.873970627784729\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.1920928955078125e-06\n",
      "Loss:  7.826834917068481e-06\n",
      "Loss:  5.9138983488082886e-06\n",
      "Loss:  2.9809772968292236e-05\n",
      "Loss:  2.3692846298217773e-06\n",
      "Loss:  1.989305019378662e-06\n",
      "Loss:  2.130866050720215e-06\n",
      "Loss:  8.411705493927002e-06\n",
      "Loss:  4.1212886571884155e-05\n",
      "Loss:  7.618218660354614e-06\n",
      "Loss:  5.902722477912903e-06\n",
      "Loss:  2.0097941160202026e-06\n",
      "Loss:  4.988163709640503e-06\n",
      "Loss:  5.478039383888245e-06\n",
      "Loss:  1.2842938303947449e-05\n",
      "Loss:  2.394244074821472e-05\n",
      "Loss:  4.697591066360474e-06\n",
      "Loss:  2.0038336515426636e-05\n",
      "Loss:  5.301088094711304e-06\n",
      "Loss:  1.0915100574493408e-05\n",
      "Loss:  5.880370736122131e-06\n",
      "Loss:  5.630776286125183e-06\n",
      "Loss:  3.368407487869263e-05\n",
      "Loss:  9.695068001747131e-06\n",
      "Loss:  6.0033053159713745e-06\n",
      "Loss:  1.3114884495735168e-05\n",
      "Loss:  3.427267074584961e-06\n",
      "Loss:  6.990507245063782e-06\n",
      "Loss:  2.859160304069519e-06\n",
      "Loss:  9.94652509689331e-07\n",
      "Loss:  2.9355287551879883e-06\n",
      "Loss:  1.8440186977386475e-06\n",
      "Loss:  7.107853889465332e-06\n",
      "Loss:  4.537403583526611e-06\n",
      "Loss:  1.0356307029724121e-06\n",
      "Loss:  3.5427510738372803e-06\n",
      "Loss:  5.679205060005188e-06\n",
      "Loss:  4.621222615242004e-06\n",
      "Loss:  1.0710209608078003e-05\n",
      "Loss:  1.69314444065094e-05\n",
      "Loss:  2.066294428004767e-06\n",
      "Epoch:  39\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  4.537403583526611e-06\n",
      "Loss:  5.455687642097473e-06\n",
      "Loss:  8.89599323272705e-06\n",
      "Loss:  2.4586915969848633e-06\n",
      "Loss:  2.0286068320274353e-05\n",
      "Loss:  4.06801700592041e-06\n",
      "Loss:  1.3932585716247559e-06\n",
      "Loss:  5.597248673439026e-06\n",
      "Loss:  2.3134052753448486e-06\n",
      "Loss:  1.942180097103119e-05\n",
      "Loss:  8.508563041687012e-06\n",
      "Loss:  1.1626631021499634e-05\n",
      "Loss:  9.648501873016357e-07\n",
      "Loss:  7.08363950252533e-06\n",
      "Loss:  5.148351192474365e-06\n",
      "Loss:  9.624287486076355e-06\n",
      "Loss:  8.907169103622437e-06\n",
      "Loss:  1.4923512935638428e-05\n",
      "Loss:  3.6619603633880615e-06\n",
      "Loss:  1.8496066331863403e-06\n",
      "Loss:  2.8405338525772095e-06\n",
      "Loss:  8.66129994392395e-06\n",
      "Loss:  9.784474968910217e-06\n",
      "Loss:  2.723187208175659e-06\n",
      "Loss:  2.678483724594116e-06\n",
      "Loss:  1.066550612449646e-05\n",
      "Loss:  3.7048012018203735e-06\n",
      "Loss:  2.9392540454864502e-05\n",
      "Loss:  2.2158026695251465e-05\n",
      "Loss:  8.879229426383972e-06\n",
      "Loss:  7.698312401771545e-06\n",
      "Loss:  2.627633512020111e-05\n",
      "Loss:  3.904104232788086e-06\n",
      "Loss:  6.590038537979126e-06\n",
      "Loss:  1.605600118637085e-06\n",
      "Loss:  5.692243576049805e-06\n",
      "Loss:  1.0067597031593323e-05\n",
      "Loss:  7.82310962677002e-07\n",
      "Loss:  5.085021257400513e-06\n",
      "Loss:  1.4243026271287818e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8883824348449707\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.7511268854141235e-06\n",
      "Loss:  1.3560056686401367e-06\n",
      "Loss:  9.424984455108643e-07\n",
      "Loss:  3.863126039505005e-06\n",
      "Loss:  1.3127923011779785e-05\n",
      "Loss:  6.407499313354492e-06\n",
      "Loss:  1.9049271941184998e-05\n",
      "Loss:  1.1801719665527344e-05\n",
      "Loss:  9.754672646522522e-06\n",
      "Loss:  9.767711162567139e-06\n",
      "Loss:  9.5367431640625e-07\n",
      "Loss:  1.0909512639045715e-05\n",
      "Loss:  2.946704626083374e-06\n",
      "Loss:  1.6205012798309326e-06\n",
      "Loss:  3.3080577850341797e-06\n",
      "Loss:  3.4086406230926514e-06\n",
      "Loss:  6.254762411117554e-06\n",
      "Loss:  5.3942203521728516e-06\n",
      "Loss:  6.407499313354492e-06\n",
      "Loss:  5.170702934265137e-06\n",
      "Loss:  2.0684674382209778e-05\n",
      "Loss:  4.999339580535889e-06\n",
      "Loss:  1.911073923110962e-06\n",
      "Loss:  1.6186386346817017e-05\n",
      "Loss:  1.817941665649414e-06\n",
      "Loss:  2.140924334526062e-05\n",
      "Loss:  5.939975380897522e-06\n",
      "Loss:  1.584365963935852e-05\n",
      "Loss:  4.520639777183533e-06\n",
      "Loss:  6.018206477165222e-06\n",
      "Loss:  8.473172783851624e-06\n",
      "Loss:  2.3953616619110107e-06\n",
      "Loss:  5.17629086971283e-06\n",
      "Loss:  1.0170042514801025e-05\n",
      "Loss:  7.761642336845398e-06\n",
      "Loss:  5.763024091720581e-06\n",
      "Loss:  1.7425045371055603e-05\n",
      "Loss:  1.566484570503235e-05\n",
      "Loss:  1.537799835205078e-05\n",
      "Loss:  7.398426532745361e-06\n",
      "Loss:  2.096096750392462e-06\n",
      "Epoch:  40\n",
      "Loss:  1.0015442967414856e-05\n",
      "Loss:  7.934868335723877e-07\n",
      "Loss:  1.4975666999816895e-06\n",
      "Loss:  2.4065375328063965e-06\n",
      "Loss:  2.507120370864868e-06\n",
      "Loss:  3.6712735891342163e-06\n",
      "Loss:  9.085983037948608e-06\n",
      "Loss:  1.7100945115089417e-05\n",
      "Loss:  1.5385448932647705e-06\n",
      "Loss:  2.1725893020629883e-05\n",
      "Loss:  1.2896955013275146e-05\n",
      "Loss:  6.2212347984313965e-06\n",
      "Loss:  6.707385182380676e-06\n",
      "Loss:  2.520158886909485e-06\n",
      "Loss:  1.7955899238586426e-06\n",
      "Loss:  1.2375414371490479e-05\n",
      "Loss:  1.0283663868904114e-05\n",
      "Loss:  1.210719347000122e-06\n",
      "Loss:  1.817941665649414e-06\n",
      "Loss:  1.3154000043869019e-05\n",
      "Loss:  6.182119250297546e-06\n",
      "Loss:  7.947906851768494e-06\n",
      "Loss:  1.296401023864746e-06\n",
      "Loss:  9.55536961555481e-06\n",
      "Loss:  5.943700671195984e-06\n",
      "Loss:  8.113682270050049e-06\n",
      "Loss:  5.898997187614441e-06\n",
      "Loss:  3.0677765607833862e-06\n",
      "Loss:  2.644956111907959e-06\n",
      "Loss:  3.226287662982941e-05\n",
      "Loss:  5.60469925403595e-06\n",
      "Loss:  1.955777406692505e-06\n",
      "Loss:  2.4028122425079346e-06\n",
      "Loss:  8.085742592811584e-06\n",
      "Loss:  7.195398211479187e-06\n",
      "Loss:  1.9183382391929626e-05\n",
      "Loss:  1.5478581190109253e-05\n",
      "Loss:  3.302469849586487e-06\n",
      "Loss:  2.389773726463318e-06\n",
      "Loss:  8.828938007354736e-06\n",
      "Loss:  5.25514269611449e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.8964518308639526\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.7293339371681213e-05\n",
      "Loss:  3.0919909477233887e-07\n",
      "Loss:  1.3470649719238281e-05\n",
      "Loss:  2.3130327463150024e-05\n",
      "Loss:  1.9779428839683533e-05\n",
      "Loss:  1.53370201587677e-05\n",
      "Loss:  6.1336904764175415e-06\n",
      "Loss:  2.5238841772079468e-06\n",
      "Loss:  9.624287486076355e-06\n",
      "Loss:  5.910173058509827e-06\n",
      "Loss:  4.304572939872742e-06\n",
      "Loss:  5.720183253288269e-06\n",
      "Loss:  8.648261427879333e-06\n",
      "Loss:  9.775161743164062e-06\n",
      "Loss:  2.4102628231048584e-06\n",
      "Loss:  9.201467037200928e-07\n",
      "Loss:  4.990026354789734e-06\n",
      "Loss:  8.67992639541626e-06\n",
      "Loss:  3.293156623840332e-06\n",
      "Loss:  1.3081356883049011e-05\n",
      "Loss:  1.9073486328125e-06\n",
      "Loss:  6.1355531215667725e-06\n",
      "Loss:  5.48921525478363e-06\n",
      "Loss:  1.3511627912521362e-05\n",
      "Loss:  8.458271622657776e-06\n",
      "Loss:  3.5651028156280518e-06\n",
      "Loss:  5.066394805908203e-07\n",
      "Loss:  2.9746443033218384e-06\n",
      "Loss:  2.0526349544525146e-06\n",
      "Loss:  3.416091203689575e-06\n",
      "Loss:  8.23289155960083e-06\n",
      "Loss:  2.566725015640259e-06\n",
      "Loss:  2.522021532058716e-06\n",
      "Loss:  7.964670658111572e-06\n",
      "Loss:  5.578622221946716e-06\n",
      "Loss:  7.973983883857727e-06\n",
      "Loss:  1.9527971744537354e-05\n",
      "Loss:  7.115304470062256e-07\n",
      "Loss:  1.8514692783355713e-06\n",
      "Loss:  2.08243727684021e-06\n",
      "Loss:  1.430511474609375e-06\n",
      "Epoch:  41\n",
      "Loss:  1.166015863418579e-06\n",
      "Loss:  1.4156103134155273e-06\n",
      "Loss:  2.5626271963119507e-05\n",
      "Loss:  3.5408884286880493e-06\n",
      "Loss:  1.5087425708770752e-06\n",
      "Loss:  4.163011908531189e-06\n",
      "Loss:  1.4826655387878418e-06\n",
      "Loss:  3.6526471376419067e-06\n",
      "Loss:  6.6943466663360596e-06\n",
      "Loss:  1.5795230865478516e-06\n",
      "Loss:  9.993091225624084e-06\n",
      "Loss:  6.789341568946838e-06\n",
      "Loss:  2.4884939193725586e-06\n",
      "Loss:  5.370005965232849e-06\n",
      "Loss:  2.4586915969848633e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.288290023803711e-06\n",
      "Loss:  5.446374416351318e-06\n",
      "Loss:  6.765127182006836e-06\n",
      "Loss:  7.929280400276184e-06\n",
      "Loss:  3.850087523460388e-06\n",
      "Loss:  4.8857182264328e-06\n",
      "Loss:  6.413087248802185e-06\n",
      "Loss:  2.023950219154358e-05\n",
      "Loss:  3.734603524208069e-06\n",
      "Loss:  1.1593103408813477e-05\n",
      "Loss:  4.023313522338867e-06\n",
      "Loss:  1.895800232887268e-05\n",
      "Loss:  3.2391399145126343e-06\n",
      "Loss:  2.477318048477173e-06\n",
      "Loss:  3.0584633350372314e-06\n",
      "Loss:  1.7102807760238647e-05\n",
      "Loss:  5.930662155151367e-06\n",
      "Loss:  2.473779022693634e-05\n",
      "Loss:  3.0938535928726196e-06\n",
      "Loss:  3.857538104057312e-06\n",
      "Loss:  7.739290595054626e-06\n",
      "Loss:  5.692243576049805e-06\n",
      "Loss:  1.5459954738616943e-06\n",
      "Loss:  1.2272968888282776e-05\n",
      "Loss:  4.017725586891174e-06\n",
      "Loss:  1.0232130307485932e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9216629266738892\n",
      "Validation accuracy: 81%\n",
      "Loss:  6.3963234424591064e-06\n",
      "Loss:  5.435198545455933e-06\n",
      "Loss:  1.150369644165039e-05\n",
      "Loss:  7.852911949157715e-06\n",
      "Loss:  1.3150274753570557e-06\n",
      "Loss:  6.4373016357421875e-06\n",
      "Loss:  5.250796675682068e-06\n",
      "Loss:  2.9375776648521423e-05\n",
      "Loss:  1.2032687664031982e-06\n",
      "Loss:  6.627291440963745e-06\n",
      "Loss:  3.816559910774231e-06\n",
      "Loss:  1.6119331121444702e-05\n",
      "Loss:  1.189298927783966e-05\n",
      "Loss:  2.524442970752716e-05\n",
      "Loss:  7.409602403640747e-06\n",
      "Loss:  1.9408762454986572e-06\n",
      "Loss:  2.115964889526367e-06\n",
      "Loss:  6.370246410369873e-06\n",
      "Loss:  6.206333637237549e-06\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  1.7853453755378723e-05\n",
      "Loss:  4.565343260765076e-06\n",
      "Loss:  1.5087425708770752e-06\n",
      "Loss:  3.954395651817322e-06\n",
      "Loss:  2.896413207054138e-06\n",
      "Loss:  4.587695002555847e-06\n",
      "Loss:  1.033395528793335e-05\n",
      "Loss:  3.691762685775757e-06\n",
      "Loss:  2.5723129510879517e-06\n",
      "Loss:  1.1838972568511963e-05\n",
      "Loss:  2.248212695121765e-06\n",
      "Loss:  4.973262548446655e-06\n",
      "Loss:  1.0721385478973389e-05\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  2.4512410163879395e-06\n",
      "Loss:  2.4065375328063965e-06\n",
      "Loss:  1.1995434761047363e-06\n",
      "Loss:  9.24617052078247e-06\n",
      "Loss:  2.1457672119140625e-06\n",
      "Loss:  4.809349775314331e-06\n",
      "Loss:  1.0629495363900787e-06\n",
      "Epoch:  42\n",
      "Loss:  6.008893251419067e-06\n",
      "Loss:  6.077811121940613e-06\n",
      "Loss:  1.4102086424827576e-05\n",
      "Loss:  7.966533303260803e-06\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  2.9150396585464478e-06\n",
      "Loss:  5.956739187240601e-06\n",
      "Loss:  3.676861524581909e-06\n",
      "Loss:  7.413327693939209e-06\n",
      "Loss:  1.2448057532310486e-05\n",
      "Loss:  3.991648554801941e-06\n",
      "Loss:  7.834285497665405e-06\n",
      "Loss:  1.3597309589385986e-06\n",
      "Loss:  1.5895813703536987e-05\n",
      "Loss:  1.0497868061065674e-05\n",
      "Loss:  5.00120222568512e-06\n",
      "Loss:  3.552064299583435e-06\n",
      "Loss:  8.23289155960083e-07\n",
      "Loss:  2.5313347578048706e-06\n",
      "Loss:  7.152557373046875e-07\n",
      "Loss:  4.392117261886597e-06\n",
      "Loss:  1.6130506992340088e-06\n",
      "Loss:  2.127140760421753e-06\n",
      "Loss:  5.798414349555969e-06\n",
      "Loss:  3.4403055906295776e-06\n",
      "Loss:  3.511086106300354e-06\n",
      "Loss:  1.0654330253601074e-06\n",
      "Loss:  4.734843969345093e-06\n",
      "Loss:  1.3886019587516785e-05\n",
      "Loss:  1.9008293747901917e-05\n",
      "Loss:  1.8142163753509521e-06\n",
      "Loss:  8.46758484840393e-06\n",
      "Loss:  1.710467040538788e-05\n",
      "Loss:  1.3794749975204468e-05\n",
      "Loss:  2.0265579223632812e-06\n",
      "Loss:  2.086162567138672e-07\n",
      "Loss:  8.419156074523926e-07\n",
      "Loss:  4.248693585395813e-06\n",
      "Loss:  1.2692064046859741e-05\n",
      "Loss:  2.93925404548645e-06\n",
      "Loss:  1.1436641216278076e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9300916194915771\n",
      "Validation accuracy: 81%\n",
      "Loss:  7.294118404388428e-06\n",
      "Loss:  2.127140760421753e-06\n",
      "Loss:  6.299465894699097e-06\n",
      "Loss:  3.4086406230926514e-06\n",
      "Loss:  8.642673492431641e-07\n",
      "Loss:  5.5655837059021e-06\n",
      "Loss:  1.1663883924484253e-05\n",
      "Loss:  2.203136682510376e-05\n",
      "Loss:  6.055459380149841e-06\n",
      "Loss:  2.0563602447509766e-06\n",
      "Loss:  1.5256926417350769e-05\n",
      "Loss:  4.6156346797943115e-06\n",
      "Loss:  1.2889504432678223e-06\n",
      "Loss:  2.53971666097641e-05\n",
      "Loss:  3.378838300704956e-06\n",
      "Loss:  1.7434358596801758e-06\n",
      "Loss:  2.7175992727279663e-06\n",
      "Loss:  1.7620623111724854e-06\n",
      "Loss:  1.0656192898750305e-05\n",
      "Loss:  3.0081719160079956e-06\n",
      "Loss:  1.924857497215271e-05\n",
      "Loss:  4.814937710762024e-06\n",
      "Loss:  3.8333237171173096e-06\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  6.789341568946838e-06\n",
      "Loss:  4.131346940994263e-06\n",
      "Loss:  4.991888999938965e-07\n",
      "Loss:  2.8498470783233643e-06\n",
      "Loss:  3.958120942115784e-06\n",
      "Loss:  8.605420589447021e-07\n",
      "Loss:  1.2116506695747375e-05\n",
      "Loss:  4.589557647705078e-06\n",
      "Loss:  1.5050172805786133e-06\n",
      "Loss:  3.905966877937317e-06\n",
      "Loss:  1.5776604413986206e-05\n",
      "Loss:  5.446374416351318e-06\n",
      "Loss:  4.3995678424835205e-06\n",
      "Loss:  6.407499313354492e-07\n",
      "Loss:  4.548579454421997e-06\n",
      "Loss:  5.660578608512878e-06\n",
      "Loss:  4.765888206748059e-06\n",
      "Epoch:  43\n",
      "Loss:  1.9371509552001953e-06\n",
      "Loss:  6.292015314102173e-06\n",
      "Loss:  4.1387975215911865e-06\n",
      "Loss:  4.122033715248108e-06\n",
      "Loss:  1.2952834367752075e-05\n",
      "Loss:  5.329027771949768e-06\n",
      "Loss:  3.343448042869568e-06\n",
      "Loss:  2.592802047729492e-06\n",
      "Loss:  4.8317015171051025e-06\n",
      "Loss:  1.2394040822982788e-05\n",
      "Loss:  1.0559335350990295e-05\n",
      "Loss:  4.347413778305054e-06\n",
      "Loss:  3.03611159324646e-06\n",
      "Loss:  5.969777703285217e-06\n",
      "Loss:  1.5407800674438477e-05\n",
      "Loss:  9.834766387939453e-07\n",
      "Loss:  1.6881152987480164e-05\n",
      "Loss:  6.780028343200684e-07\n",
      "Loss:  6.407499313354492e-06\n",
      "Loss:  3.4905970096588135e-06\n",
      "Loss:  4.490837454795837e-06\n",
      "Loss:  3.4868717193603516e-06\n",
      "Loss:  3.596767783164978e-06\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  1.2032687664031982e-06\n",
      "Loss:  1.3319775462150574e-05\n",
      "Loss:  8.890405297279358e-06\n",
      "Loss:  6.4391642808914185e-06\n",
      "Loss:  3.764405846595764e-06\n",
      "Loss:  1.2639909982681274e-05\n",
      "Loss:  7.197260856628418e-06\n",
      "Loss:  1.255422830581665e-06\n",
      "Loss:  3.345310688018799e-06\n",
      "Loss:  1.2619420886039734e-05\n",
      "Loss:  6.603077054023743e-06\n",
      "Loss:  4.4330954551696777e-07\n",
      "Loss:  6.081536412239075e-06\n",
      "Loss:  5.60469925403595e-06\n",
      "Loss:  2.562999725341797e-06\n",
      "Loss:  7.711350917816162e-07\n",
      "Loss:  1.9818544387817383e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9305216073989868\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.043081283569336e-05\n",
      "Loss:  4.7888606786727905e-06\n",
      "Loss:  1.7844140529632568e-06\n",
      "Loss:  3.820285201072693e-06\n",
      "Loss:  1.3746321201324463e-05\n",
      "Loss:  9.236857295036316e-06\n",
      "Loss:  8.083879947662354e-06\n",
      "Loss:  1.4677643775939941e-06\n",
      "Loss:  1.432374119758606e-06\n",
      "Loss:  4.345551133155823e-06\n",
      "Loss:  4.149973392486572e-06\n",
      "Loss:  1.8440186977386475e-06\n",
      "Loss:  1.9334256649017334e-06\n",
      "Loss:  6.996095180511475e-06\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  1.5385448932647705e-06\n",
      "Loss:  5.349516868591309e-06\n",
      "Loss:  1.718848943710327e-05\n",
      "Loss:  1.8734484910964966e-05\n",
      "Loss:  4.362314939498901e-06\n",
      "Loss:  6.554648280143738e-06\n",
      "Loss:  4.388391971588135e-06\n",
      "Loss:  1.7154961824417114e-05\n",
      "Loss:  4.1816383600234985e-06\n",
      "Loss:  7.115304470062256e-07\n",
      "Loss:  2.4959444999694824e-06\n",
      "Loss:  3.339722752571106e-06\n",
      "Loss:  5.5655837059021e-06\n",
      "Loss:  3.6582350730895996e-06\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  2.421438694000244e-06\n",
      "Loss:  2.298504114151001e-06\n",
      "Loss:  5.2265822887420654e-06\n",
      "Loss:  2.522021532058716e-06\n",
      "Loss:  3.7066638469696045e-06\n",
      "Loss:  4.250556230545044e-06\n",
      "Loss:  2.2329390048980713e-05\n",
      "Loss:  4.26173210144043e-06\n",
      "Loss:  5.198642611503601e-06\n",
      "Loss:  3.462657332420349e-06\n",
      "Loss:  2.294778823852539e-06\n",
      "Epoch:  44\n",
      "Loss:  2.5033950805664062e-06\n",
      "Loss:  2.954155206680298e-06\n",
      "Loss:  1.5124678611755371e-06\n",
      "Loss:  1.905485987663269e-06\n",
      "Loss:  4.433095455169678e-06\n",
      "Loss:  6.582587957382202e-06\n",
      "Loss:  1.0766088962554932e-05\n",
      "Loss:  1.8673017621040344e-05\n",
      "Loss:  1.2444332242012024e-05\n",
      "Loss:  1.5422701835632324e-05\n",
      "Loss:  4.3213367462158203e-07\n",
      "Loss:  2.089887857437134e-06\n",
      "Loss:  2.6617199182510376e-06\n",
      "Loss:  1.043081283569336e-06\n",
      "Loss:  8.530914783477783e-07\n",
      "Loss:  5.558133125305176e-06\n",
      "Loss:  3.688037395477295e-06\n",
      "Loss:  2.9355287551879883e-06\n",
      "Loss:  1.0170042514801025e-06\n",
      "Loss:  7.383525371551514e-06\n",
      "Loss:  1.0736286640167236e-05\n",
      "Loss:  1.0989606380462646e-06\n",
      "Loss:  3.8743019104003906e-07\n",
      "Loss:  6.115064024925232e-06\n",
      "Loss:  7.413327693939209e-06\n",
      "Loss:  1.0207295417785645e-05\n",
      "Loss:  2.814456820487976e-06\n",
      "Loss:  9.909272193908691e-06\n",
      "Loss:  8.696690201759338e-06\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  1.4696270227432251e-05\n",
      "Loss:  1.8924474716186523e-06\n",
      "Loss:  3.7830322980880737e-06\n",
      "Loss:  5.420297384262085e-06\n",
      "Loss:  6.431713700294495e-06\n",
      "Loss:  1.8514692783355713e-06\n",
      "Loss:  1.7620623111724854e-06\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  4.10713255405426e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.112480044364929e-06\n",
      "Loss:  1.074373722076416e-05\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9478791952133179\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.2207776308059692e-05\n",
      "Loss:  1.4098361134529114e-05\n",
      "Loss:  2.678483724594116e-06\n",
      "Loss:  8.588656783103943e-06\n",
      "Loss:  1.5906989574432373e-06\n",
      "Loss:  1.3638287782669067e-05\n",
      "Loss:  4.7013163566589355e-06\n",
      "Loss:  8.745118975639343e-06\n",
      "Loss:  6.891787052154541e-07\n",
      "Loss:  5.695968866348267e-06\n",
      "Loss:  6.673857569694519e-06\n",
      "Loss:  1.1537224054336548e-05\n",
      "Loss:  1.300126314163208e-06\n",
      "Loss:  3.7103891372680664e-06\n",
      "Loss:  1.2405216693878174e-06\n",
      "Loss:  5.600973963737488e-06\n",
      "Loss:  4.190951585769653e-06\n",
      "Loss:  2.902001142501831e-06\n",
      "Loss:  6.254762411117554e-06\n",
      "Loss:  1.0244548320770264e-06\n",
      "Loss:  1.9297003746032715e-06\n",
      "Loss:  8.568167686462402e-07\n",
      "Loss:  7.599592208862305e-07\n",
      "Loss:  4.844740033149719e-06\n",
      "Loss:  1.3224780559539795e-06\n",
      "Loss:  1.5385448932647705e-06\n",
      "Loss:  1.5832483768463135e-06\n",
      "Loss:  9.188428521156311e-06\n",
      "Loss:  1.944601535797119e-06\n",
      "Loss:  4.28222119808197e-06\n",
      "Loss:  6.536021828651428e-06\n",
      "Loss:  4.04752790927887e-06\n",
      "Loss:  8.126720786094666e-06\n",
      "Loss:  7.405877113342285e-06\n",
      "Loss:  1.1065974831581116e-05\n",
      "Loss:  4.291534423828125e-06\n",
      "Loss:  9.47713851928711e-06\n",
      "Loss:  9.909272193908691e-07\n",
      "Loss:  8.577480912208557e-06\n",
      "Loss:  1.3262033462524414e-06\n",
      "Loss:  3.4570693969726562e-06\n",
      "Epoch:  45\n",
      "Loss:  4.9211084842681885e-06\n",
      "Loss:  5.2247196435928345e-06\n",
      "Loss:  4.781410098075867e-06\n",
      "Loss:  4.259869456291199e-06\n",
      "Loss:  9.391456842422485e-06\n",
      "Loss:  2.9709190130233765e-06\n",
      "Loss:  5.252659320831299e-07\n",
      "Loss:  1.7508864402770996e-06\n",
      "Loss:  6.658956408500671e-06\n",
      "Loss:  3.7513673305511475e-06\n",
      "Loss:  2.3171305656433105e-06\n",
      "Loss:  4.557892680168152e-06\n",
      "Loss:  3.241002559661865e-07\n",
      "Loss:  6.51925802230835e-06\n",
      "Loss:  1.4174729585647583e-06\n",
      "Loss:  1.944601535797119e-06\n",
      "Loss:  9.313225746154785e-07\n",
      "Loss:  4.190951585769653e-06\n",
      "Loss:  1.3262033462524414e-05\n",
      "Loss:  1.6987323760986328e-06\n",
      "Loss:  3.6135315895080566e-07\n",
      "Loss:  4.306435585021973e-06\n",
      "Loss:  1.0574236512184143e-05\n",
      "Loss:  6.6962093114852905e-06\n",
      "Loss:  1.1779367923736572e-05\n",
      "Loss:  1.8142163753509521e-06\n",
      "Loss:  1.4491379261016846e-06\n",
      "Loss:  4.86522912979126e-06\n",
      "Loss:  1.3113021850585938e-06\n",
      "Loss:  7.320195436477661e-06\n",
      "Loss:  1.1228024959564209e-05\n",
      "Loss:  1.0019168257713318e-05\n",
      "Loss:  9.30391252040863e-06\n",
      "Loss:  2.982094883918762e-06\n",
      "Loss:  2.5704503059387207e-06\n",
      "Loss:  2.289190888404846e-06\n",
      "Loss:  4.187226295471191e-06\n",
      "Loss:  7.735565304756165e-06\n",
      "Loss:  6.260350346565247e-06\n",
      "Loss:  4.693865776062012e-06\n",
      "Loss:  6.919105999259045e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.947338342666626\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.5688281059265137e-06\n",
      "Loss:  3.641471266746521e-06\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  1.8738210201263428e-06\n",
      "Loss:  2.305954694747925e-06\n",
      "Loss:  9.190291166305542e-06\n",
      "Loss:  1.1473894119262695e-06\n",
      "Loss:  4.144385457038879e-06\n",
      "Loss:  2.2333115339279175e-06\n",
      "Loss:  1.012161374092102e-05\n",
      "Loss:  7.398426532745361e-06\n",
      "Loss:  3.4887343645095825e-06\n",
      "Loss:  1.084059476852417e-06\n",
      "Loss:  5.127862095832825e-06\n",
      "Loss:  8.121132850646973e-07\n",
      "Loss:  2.8349459171295166e-06\n",
      "Loss:  4.030764102935791e-06\n",
      "Loss:  8.702278137207031e-06\n",
      "Loss:  1.0244548320770264e-06\n",
      "Loss:  6.7427754402160645e-06\n",
      "Loss:  3.293156623840332e-06\n",
      "Loss:  9.46037471294403e-06\n",
      "Loss:  2.7976930141448975e-06\n",
      "Loss:  4.373490810394287e-06\n",
      "Loss:  5.921348929405212e-06\n",
      "Loss:  1.4089047908782959e-05\n",
      "Loss:  3.1013041734695435e-06\n",
      "Loss:  7.074326276779175e-06\n",
      "Loss:  5.418434739112854e-06\n",
      "Loss:  5.206093192100525e-06\n",
      "Loss:  1.2675300240516663e-05\n",
      "Loss:  1.4521181583404541e-05\n",
      "Loss:  2.2444874048233032e-06\n",
      "Loss:  1.255422830581665e-06\n",
      "Loss:  2.512708306312561e-06\n",
      "Loss:  9.98377799987793e-07\n",
      "Loss:  4.239380359649658e-06\n",
      "Loss:  4.4330954551696777e-07\n",
      "Loss:  5.660578608512878e-06\n",
      "Loss:  8.378177881240845e-06\n",
      "Loss:  1.2119611483285553e-06\n",
      "Epoch:  46\n",
      "Loss:  2.0693987607955933e-06\n",
      "Loss:  8.028000593185425e-06\n",
      "Loss:  2.780929207801819e-06\n",
      "Loss:  3.5315752029418945e-06\n",
      "Loss:  2.041459083557129e-06\n",
      "Loss:  1.1960044503211975e-05\n",
      "Loss:  1.3299286365509033e-06\n",
      "Loss:  4.071742296218872e-06\n",
      "Loss:  8.530914783477783e-07\n",
      "Loss:  6.92903995513916e-07\n",
      "Loss:  7.4338167905807495e-06\n",
      "Loss:  1.453980803489685e-05\n",
      "Loss:  2.0135194063186646e-06\n",
      "Loss:  3.3099204301834106e-06\n",
      "Loss:  3.7532299757003784e-06\n",
      "Loss:  3.8780272006988525e-06\n",
      "Loss:  1.173466444015503e-06\n",
      "Loss:  1.5681609511375427e-05\n",
      "Loss:  5.532056093215942e-06\n",
      "Loss:  6.934627890586853e-06\n",
      "Loss:  7.797032594680786e-06\n",
      "Loss:  3.8016587495803833e-06\n",
      "Loss:  4.552304744720459e-06\n",
      "Loss:  8.270144462585449e-07\n",
      "Loss:  2.9802322387695312e-06\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  2.2351741790771484e-06\n",
      "Loss:  3.037974238395691e-06\n",
      "Loss:  3.3974647521972656e-06\n",
      "Loss:  1.0170042514801025e-06\n",
      "Loss:  8.29063355922699e-06\n",
      "Loss:  2.0638108253479004e-06\n",
      "Loss:  2.4922192096710205e-06\n",
      "Loss:  9.052455425262451e-07\n",
      "Loss:  4.723668098449707e-06\n",
      "Loss:  1.8309801816940308e-06\n",
      "Loss:  6.3907355070114136e-06\n",
      "Loss:  1.633167266845703e-05\n",
      "Loss:  2.8721988201141357e-06\n",
      "Loss:  6.051734089851379e-06\n",
      "Loss:  1.5397866093280754e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9683868885040283\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.6391277313232422e-06\n",
      "Loss:  5.815178155899048e-06\n",
      "Loss:  9.782612323760986e-06\n",
      "Loss:  1.4156103134155273e-06\n",
      "Loss:  2.0153820514678955e-06\n",
      "Loss:  4.041939973831177e-06\n",
      "Loss:  5.170702934265137e-06\n",
      "Loss:  6.556510925292969e-07\n",
      "Loss:  2.205371856689453e-06\n",
      "Loss:  3.0249357223510742e-06\n",
      "Loss:  5.9623271226882935e-06\n",
      "Loss:  3.3527612686157227e-06\n",
      "Loss:  6.90855085849762e-06\n",
      "Loss:  1.6167759895324707e-06\n",
      "Loss:  7.269904017448425e-06\n",
      "Loss:  1.5430152416229248e-05\n",
      "Loss:  2.035871148109436e-06\n",
      "Loss:  1.039355993270874e-05\n",
      "Loss:  9.275972843170166e-07\n",
      "Loss:  6.258487701416016e-07\n",
      "Loss:  5.9586018323898315e-06\n",
      "Loss:  1.6469508409500122e-05\n",
      "Loss:  9.797513484954834e-07\n",
      "Loss:  6.103888154029846e-06\n",
      "Loss:  1.2330710887908936e-06\n",
      "Loss:  4.816800355911255e-06\n",
      "Loss:  4.889443516731262e-06\n",
      "Loss:  3.6600977182388306e-06\n",
      "Loss:  1.2740492820739746e-05\n",
      "Loss:  2.086162567138672e-07\n",
      "Loss:  1.1399388313293457e-06\n",
      "Loss:  2.080574631690979e-06\n",
      "Loss:  3.5837292671203613e-06\n",
      "Loss:  3.375113010406494e-06\n",
      "Loss:  1.1026859283447266e-06\n",
      "Loss:  1.4565885066986084e-06\n",
      "Loss:  5.8766454458236694e-06\n",
      "Loss:  2.1830201148986816e-06\n",
      "Loss:  2.332031726837158e-06\n",
      "Loss:  6.671994924545288e-06\n",
      "Loss:  3.3477942906756653e-06\n",
      "Epoch:  47\n",
      "Loss:  2.8815120458602905e-06\n",
      "Loss:  7.711350917816162e-07\n",
      "Loss:  3.7811696529388428e-06\n",
      "Loss:  1.1548399925231934e-06\n",
      "Loss:  2.089887857437134e-06\n",
      "Loss:  8.964911103248596e-06\n",
      "Loss:  3.0882656574249268e-06\n",
      "Loss:  4.76837158203125e-06\n",
      "Loss:  4.24310564994812e-06\n",
      "Loss:  3.330409526824951e-06\n",
      "Loss:  1.389533281326294e-06\n",
      "Loss:  5.289912223815918e-07\n",
      "Loss:  8.009374141693115e-07\n",
      "Loss:  3.1944364309310913e-06\n",
      "Loss:  1.2088567018508911e-06\n",
      "Loss:  5.48921525478363e-06\n",
      "Loss:  4.667788743972778e-06\n",
      "Loss:  5.02169132232666e-06\n",
      "Loss:  1.1064112186431885e-06\n",
      "Loss:  1.3746321201324463e-06\n",
      "Loss:  2.073124051094055e-06\n",
      "Loss:  4.7478824853897095e-06\n",
      "Loss:  4.891306161880493e-06\n",
      "Loss:  5.200505256652832e-06\n",
      "Loss:  1.6577541828155518e-06\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  7.6051801443099976e-06\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  9.004026651382446e-06\n",
      "Loss:  4.49642539024353e-06\n",
      "Loss:  3.956258296966553e-06\n",
      "Loss:  1.570768654346466e-05\n",
      "Loss:  6.448477506637573e-06\n",
      "Loss:  1.798383891582489e-05\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  1.773238182067871e-06\n",
      "Loss:  3.0212104320526123e-06\n",
      "Loss:  6.744638085365295e-06\n",
      "Loss:  5.9157609939575195e-06\n",
      "Loss:  2.946704626083374e-06\n",
      "Loss:  6.087124347686768e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9654021263122559\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.8980354070663452e-06\n",
      "Loss:  1.6987323760986328e-06\n",
      "Loss:  1.735985279083252e-06\n",
      "Loss:  1.3522803783416748e-06\n",
      "Loss:  3.078952431678772e-06\n",
      "Loss:  8.605420589447021e-07\n",
      "Loss:  3.5688281059265137e-06\n",
      "Loss:  5.017966032028198e-06\n",
      "Loss:  1.3355165719985962e-05\n",
      "Loss:  2.864748239517212e-06\n",
      "Loss:  1.7192214727401733e-06\n",
      "Loss:  3.3117830753326416e-06\n",
      "Loss:  1.8700957298278809e-06\n",
      "Loss:  3.334134817123413e-06\n",
      "Loss:  5.479902029037476e-06\n",
      "Loss:  1.2330710887908936e-06\n",
      "Loss:  3.9227306842803955e-06\n",
      "Loss:  5.325302481651306e-06\n",
      "Loss:  4.98257577419281e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.3319775462150574e-05\n",
      "Loss:  2.853572368621826e-06\n",
      "Loss:  8.376315236091614e-06\n",
      "Loss:  9.760260581970215e-07\n",
      "Loss:  3.5855919122695923e-06\n",
      "Loss:  4.069879651069641e-06\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  6.467103958129883e-06\n",
      "Loss:  9.702518582344055e-06\n",
      "Loss:  1.3083219528198242e-05\n",
      "Loss:  1.3597309589385986e-06\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  1.5534460544586182e-06\n",
      "Loss:  3.5651028156280518e-06\n",
      "Loss:  3.1962990760803223e-06\n",
      "Loss:  3.825873136520386e-06\n",
      "Loss:  2.773478627204895e-06\n",
      "Loss:  2.730637788772583e-06\n",
      "Loss:  2.2426247596740723e-06\n",
      "Loss:  3.373250365257263e-06\n",
      "Loss:  9.715557098388672e-06\n",
      "Loss:  3.697971578731085e-06\n",
      "Epoch:  48\n",
      "Loss:  5.474314093589783e-06\n",
      "Loss:  4.852190613746643e-06\n",
      "Loss:  1.0190531611442566e-05\n",
      "Loss:  1.0766088962554932e-06\n",
      "Loss:  1.2679025530815125e-05\n",
      "Loss:  8.456408977508545e-07\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  2.1792948246002197e-06\n",
      "Loss:  3.1869858503341675e-06\n",
      "Loss:  6.221234798431396e-07\n",
      "Loss:  1.0846182703971863e-05\n",
      "Loss:  5.731359124183655e-06\n",
      "Loss:  2.4121254682540894e-06\n",
      "Loss:  4.092231392860413e-06\n",
      "Loss:  1.8104910850524902e-06\n",
      "Loss:  6.51925802230835e-07\n",
      "Loss:  7.973983883857727e-06\n",
      "Loss:  3.63960862159729e-06\n",
      "Loss:  2.9802322387695312e-06\n",
      "Loss:  1.210719347000122e-06\n",
      "Loss:  8.568167686462402e-07\n",
      "Loss:  2.168118953704834e-06\n",
      "Loss:  8.903443813323975e-07\n",
      "Loss:  2.25752592086792e-06\n",
      "Loss:  2.592802047729492e-06\n",
      "Loss:  8.121132850646973e-07\n",
      "Loss:  2.769753336906433e-06\n",
      "Loss:  6.487593054771423e-06\n",
      "Loss:  8.083879947662354e-07\n",
      "Loss:  1.8700957298278809e-06\n",
      "Loss:  1.1019408702850342e-05\n",
      "Loss:  9.078532457351685e-06\n",
      "Loss:  2.7213245630264282e-06\n",
      "Loss:  3.3508986234664917e-06\n",
      "Loss:  3.3527612686157227e-07\n",
      "Loss:  3.693625330924988e-06\n",
      "Loss:  8.361414074897766e-06\n",
      "Loss:  1.0570511221885681e-05\n",
      "Loss:  3.8780272006988525e-06\n",
      "Loss:  7.251898637150589e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9971359968185425\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.0021030902862549e-06\n",
      "Loss:  5.327165126800537e-07\n",
      "Loss:  3.961846232414246e-06\n",
      "Loss:  3.1944364309310913e-06\n",
      "Loss:  1.4603137969970703e-06\n",
      "Loss:  1.8309801816940308e-06\n",
      "Loss:  5.386769771575928e-06\n",
      "Loss:  5.327165126800537e-07\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  7.338821887969971e-07\n",
      "Loss:  1.296401023864746e-06\n",
      "Loss:  4.2691826820373535e-06\n",
      "Loss:  9.281560778617859e-06\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  1.8104910850524902e-06\n",
      "Loss:  3.4458935260772705e-06\n",
      "Loss:  6.493180990219116e-06\n",
      "Loss:  1.4156103134155273e-06\n",
      "Loss:  1.950189471244812e-06\n",
      "Loss:  1.039355993270874e-06\n",
      "Loss:  7.838010787963867e-06\n",
      "Loss:  9.350478649139404e-07\n",
      "Loss:  9.039416909217834e-06\n",
      "Loss:  3.2652169466018677e-06\n",
      "Loss:  8.23289155960083e-06\n",
      "Loss:  2.771615982055664e-06\n",
      "Loss:  1.0617077350616455e-05\n",
      "Loss:  8.756294846534729e-06\n",
      "Loss:  4.274770617485046e-06\n",
      "Loss:  1.2794509530067444e-05\n",
      "Loss:  8.940696716308594e-07\n",
      "Loss:  1.183338463306427e-05\n",
      "Loss:  8.717179298400879e-07\n",
      "Loss:  4.155561327934265e-06\n",
      "Loss:  4.766508936882019e-06\n",
      "Loss:  1.3299286365509033e-06\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  1.3336539268493652e-06\n",
      "Loss:  4.088506102561951e-06\n",
      "Loss:  2.902001142501831e-06\n",
      "Loss:  1.927216771946405e-06\n",
      "Epoch:  49\n",
      "Loss:  1.9837170839309692e-06\n",
      "Loss:  3.91155481338501e-06\n",
      "Loss:  7.351860404014587e-06\n",
      "Loss:  2.3730099201202393e-06\n",
      "Loss:  2.9336661100387573e-06\n",
      "Loss:  9.818002581596375e-06\n",
      "Loss:  2.864748239517212e-06\n",
      "Loss:  1.6164034605026245e-05\n",
      "Loss:  9.82917845249176e-06\n",
      "Loss:  8.286908268928528e-06\n",
      "Loss:  1.3224780559539795e-06\n",
      "Loss:  1.1257827281951904e-05\n",
      "Loss:  4.49642539024353e-06\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  4.3548643589019775e-06\n",
      "Loss:  1.2367963790893555e-06\n",
      "Loss:  5.725771188735962e-06\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  6.183981895446777e-07\n",
      "Loss:  1.6130506992340088e-06\n",
      "Loss:  2.8442591428756714e-06\n",
      "Loss:  5.21540641784668e-07\n",
      "Loss:  2.302229404449463e-06\n",
      "Loss:  9.797513484954834e-07\n",
      "Loss:  3.3080577850341797e-06\n",
      "Loss:  9.462237358093262e-07\n",
      "Loss:  1.4007091522216797e-06\n",
      "Loss:  4.7441571950912476e-06\n",
      "Loss:  3.1050294637680054e-06\n",
      "Loss:  4.880130290985107e-07\n",
      "Loss:  1.776963472366333e-06\n",
      "Loss:  2.1047890186309814e-06\n",
      "Loss:  4.08664345741272e-06\n",
      "Loss:  4.539266228675842e-06\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  1.730397343635559e-06\n",
      "Loss:  4.129484295845032e-06\n",
      "Loss:  8.307397365570068e-07\n",
      "Loss:  2.4512410163879395e-06\n",
      "Loss:  6.331130862236023e-06\n",
      "Loss:  4.6690306021446304e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  1.9963278770446777\n",
      "Validation accuracy: 81%\n",
      "Loss:  4.2244791984558105e-06\n",
      "Loss:  2.3581087589263916e-06\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  2.039596438407898e-06\n",
      "Loss:  1.8998980522155762e-06\n",
      "Loss:  3.868713974952698e-06\n",
      "Loss:  6.11133873462677e-06\n",
      "Loss:  5.3998082876205444e-06\n",
      "Loss:  9.08970832824707e-07\n",
      "Loss:  2.561137080192566e-06\n",
      "Loss:  8.67992639541626e-07\n",
      "Loss:  2.25752592086792e-06\n",
      "Loss:  1.5087425708770752e-06\n",
      "Loss:  1.0088086128234863e-05\n",
      "Loss:  3.7960708141326904e-06\n",
      "Loss:  7.82310962677002e-07\n",
      "Loss:  5.600973963737488e-06\n",
      "Loss:  7.189810276031494e-07\n",
      "Loss:  1.7881393432617188e-06\n",
      "Loss:  4.76837158203125e-07\n",
      "Loss:  3.1925737857818604e-06\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  1.4448538422584534e-05\n",
      "Loss:  9.12882387638092e-06\n",
      "Loss:  1.9185245037078857e-06\n",
      "Loss:  4.041939973831177e-06\n",
      "Loss:  1.6279518604278564e-06\n",
      "Loss:  9.909272193908691e-07\n",
      "Loss:  6.530433893203735e-06\n",
      "Loss:  1.2405216693878174e-06\n",
      "Loss:  1.1920928955078125e-06\n",
      "Loss:  1.5459954738616943e-06\n",
      "Loss:  3.5837292671203613e-06\n",
      "Loss:  1.5459954738616943e-06\n",
      "Loss:  8.849427103996277e-06\n",
      "Loss:  1.344829797744751e-06\n",
      "Loss:  9.16793942451477e-06\n",
      "Loss:  4.304572939872742e-06\n",
      "Loss:  3.676861524581909e-06\n",
      "Loss:  1.5292316675186157e-06\n",
      "Loss:  5.712111601496872e-07\n",
      "Epoch:  50\n",
      "Loss:  2.3245811462402344e-06\n",
      "Loss:  8.67992639541626e-07\n",
      "Loss:  2.682209014892578e-06\n",
      "Loss:  1.039355993270874e-06\n",
      "Loss:  7.264316082000732e-07\n",
      "Loss:  6.161630153656006e-06\n",
      "Loss:  2.7026981115341187e-06\n",
      "Loss:  5.811452865600586e-07\n",
      "Loss:  3.980472683906555e-06\n",
      "Loss:  2.5033950805664062e-06\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  6.368383765220642e-06\n",
      "Loss:  7.16187059879303e-06\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  5.602836608886719e-06\n",
      "Loss:  4.189088940620422e-06\n",
      "Loss:  1.5795230865478516e-06\n",
      "Loss:  1.080334186553955e-06\n",
      "Loss:  2.5294721126556396e-06\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  1.959502696990967e-06\n",
      "Loss:  1.1362135410308838e-06\n",
      "Loss:  3.894791007041931e-06\n",
      "Loss:  7.748603820800781e-07\n",
      "Loss:  5.057081580162048e-06\n",
      "Loss:  9.6932053565979e-06\n",
      "Loss:  3.4496188163757324e-06\n",
      "Loss:  1.7527490854263306e-06\n",
      "Loss:  7.394701242446899e-06\n",
      "Loss:  9.462237358093262e-07\n",
      "Loss:  2.5015324354171753e-06\n",
      "Loss:  7.92182981967926e-06\n",
      "Loss:  7.119029760360718e-06\n",
      "Loss:  1.430511474609375e-06\n",
      "Loss:  3.252178430557251e-06\n",
      "Loss:  1.8961727619171143e-06\n",
      "Loss:  2.2873282432556152e-06\n",
      "Loss:  1.6279518604278564e-06\n",
      "Loss:  8.940696716308594e-07\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  2.1457672119140625e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.005563974380493\n",
      "Validation accuracy: 81%\n",
      "Loss:  8.380040526390076e-06\n",
      "Loss:  1.6689300537109375e-06\n",
      "Loss:  2.253800630569458e-06\n",
      "Loss:  5.923211574554443e-07\n",
      "Loss:  9.015202522277832e-07\n",
      "Loss:  5.347654223442078e-06\n",
      "Loss:  1.864507794380188e-06\n",
      "Loss:  2.1420419216156006e-06\n",
      "Loss:  2.1830201148986816e-06\n",
      "Loss:  1.773238182067871e-06\n",
      "Loss:  3.7178397178649902e-06\n",
      "Loss:  1.6707926988601685e-06\n",
      "Loss:  3.1795352697372437e-06\n",
      "Loss:  4.392117261886597e-06\n",
      "Loss:  6.742775440216064e-07\n",
      "Loss:  1.0989606380462646e-06\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  2.3636966943740845e-06\n",
      "Loss:  4.3585896492004395e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  2.2295862436294556e-06\n",
      "Loss:  1.3820827007293701e-06\n",
      "Loss:  1.0319054126739502e-06\n",
      "Loss:  1.8998980522155762e-06\n",
      "Loss:  1.687556505203247e-06\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  7.729977369308472e-06\n",
      "Loss:  4.205852746963501e-06\n",
      "Loss:  5.850568413734436e-06\n",
      "Loss:  1.6726553440093994e-06\n",
      "Loss:  9.275972843170166e-07\n",
      "Loss:  2.9671937227249146e-06\n",
      "Loss:  5.550682544708252e-07\n",
      "Loss:  2.8833746910095215e-06\n",
      "Loss:  6.658956408500671e-06\n",
      "Loss:  9.834766387939453e-07\n",
      "Loss:  1.4826655387878418e-06\n",
      "Loss:  5.070120096206665e-06\n",
      "Loss:  3.876164555549622e-06\n",
      "Loss:  1.8961727619171143e-06\n",
      "Loss:  6.308158049250778e-07\n",
      "Epoch:  51\n",
      "Loss:  3.961846232414246e-06\n",
      "Loss:  1.3783574104309082e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.456408977508545e-07\n",
      "Loss:  5.5693089962005615e-06\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  5.198642611503601e-06\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  2.594664692878723e-06\n",
      "Loss:  1.3615936040878296e-06\n",
      "Loss:  1.8868595361709595e-06\n",
      "Loss:  8.121132850646973e-07\n",
      "Loss:  1.3969838619232178e-06\n",
      "Loss:  6.4820051193237305e-06\n",
      "Loss:  4.805624485015869e-07\n",
      "Loss:  1.344829797744751e-06\n",
      "Loss:  5.736947059631348e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  2.868473529815674e-06\n",
      "Loss:  1.1809170246124268e-06\n",
      "Loss:  1.5534460544586182e-06\n",
      "Loss:  4.172325134277344e-07\n",
      "Loss:  9.164214134216309e-07\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  4.366040229797363e-06\n",
      "Loss:  1.044943928718567e-06\n",
      "Loss:  1.6801059246063232e-06\n",
      "Loss:  1.0561197996139526e-06\n",
      "Loss:  1.0766088962554932e-06\n",
      "Loss:  3.8370490074157715e-06\n",
      "Loss:  5.027279257774353e-06\n",
      "Loss:  2.479180693626404e-06\n",
      "Loss:  1.4081597328186035e-06\n",
      "Loss:  9.201467037200928e-07\n",
      "Loss:  3.203749656677246e-07\n",
      "Loss:  9.888783097267151e-06\n",
      "Loss:  1.6316771507263184e-06\n",
      "Loss:  2.378597855567932e-06\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  3.248453140258789e-06\n",
      "Loss:  4.3213367462158203e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.0160677433013916\n",
      "Validation accuracy: 81%\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.087784767150879e-06\n",
      "Loss:  2.428889274597168e-06\n",
      "Loss:  1.1174008250236511e-05\n",
      "Loss:  1.1473894119262695e-06\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.380460500717163e-06\n",
      "Loss:  2.9169023036956787e-06\n",
      "Loss:  1.039355993270874e-06\n",
      "Loss:  2.5331974029541016e-06\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  4.3585896492004395e-07\n",
      "Loss:  7.450580596923828e-07\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  7.674098014831543e-07\n",
      "Loss:  4.332512617111206e-06\n",
      "Loss:  3.7997961044311523e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  1.9166618585586548e-06\n",
      "Loss:  7.936730980873108e-06\n",
      "Loss:  1.4118850231170654e-06\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  8.21426510810852e-07\n",
      "Loss:  3.5390257835388184e-07\n",
      "Loss:  2.7082860469818115e-06\n",
      "Loss:  2.956017851829529e-06\n",
      "Loss:  3.1497329473495483e-06\n",
      "Loss:  1.6521662473678589e-06\n",
      "Loss:  6.258487701416016e-07\n",
      "Loss:  5.438923835754395e-07\n",
      "Loss:  3.546476364135742e-06\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  1.1809170246124268e-06\n",
      "Loss:  1.2479722499847412e-06\n",
      "Loss:  1.0281801223754883e-06\n",
      "Loss:  6.686896085739136e-07\n",
      "Loss:  7.301568984985352e-07\n",
      "Loss:  1.9017606973648071e-06\n",
      "Loss:  6.742775440216064e-07\n",
      "Loss:  1.5385448932647705e-06\n",
      "Loss:  7.301568984985352e-07\n",
      "Epoch:  52\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  2.3134052753448486e-06\n",
      "Loss:  7.115304470062256e-07\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  2.132728695869446e-06\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  2.345070242881775e-06\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  5.081295967102051e-06\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  4.3585896492004395e-07\n",
      "Loss:  1.6745179891586304e-06\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  3.166496753692627e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  1.1026859283447266e-06\n",
      "Loss:  8.717179298400879e-07\n",
      "Loss:  1.86823308467865e-06\n",
      "Loss:  3.2670795917510986e-06\n",
      "Loss:  4.880130290985107e-07\n",
      "Loss:  5.62518835067749e-07\n",
      "Loss:  2.0619481801986694e-06\n",
      "Loss:  5.25452196598053e-06\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  2.3543834686279297e-06\n",
      "Loss:  1.8868595361709595e-06\n",
      "Loss:  2.7064234018325806e-06\n",
      "Loss:  2.8405338525772095e-06\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  6.891787052154541e-07\n",
      "Loss:  5.736947059631348e-07\n",
      "Loss:  7.450580596923828e-07\n",
      "Loss:  6.791204214096069e-06\n",
      "Loss:  8.307397365570068e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  1.3820827007293701e-06\n",
      "Loss:  1.4156103134155273e-06\n",
      "Loss:  2.7815501653094543e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.047840118408203\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  3.0547380447387695e-07\n",
      "Loss:  1.1101365089416504e-06\n",
      "Loss:  2.380460500717163e-06\n",
      "Loss:  9.052455425262451e-07\n",
      "Loss:  4.3958425521850586e-07\n",
      "Loss:  8.083879947662354e-07\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  8.717179298400879e-07\n",
      "Loss:  3.2670795917510986e-06\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  3.160908818244934e-06\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  2.9243528842926025e-06\n",
      "Loss:  4.341825842857361e-06\n",
      "Loss:  2.173706889152527e-06\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  4.470348358154297e-07\n",
      "Loss:  2.2370368242263794e-06\n",
      "Loss:  1.862645149230957e-07\n",
      "Loss:  3.1739473342895508e-06\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  3.501772880554199e-07\n",
      "Loss:  5.401670932769775e-07\n",
      "Loss:  1.214444637298584e-06\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  2.60770320892334e-06\n",
      "Loss:  8.344650268554688e-06\n",
      "Loss:  9.164214134216309e-07\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  9.126961231231689e-07\n",
      "Loss:  9.760260581970215e-07\n",
      "Loss:  1.5478581190109253e-06\n",
      "Loss:  3.0919909477233887e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  1.8272548913955688e-06\n",
      "Loss:  1.1362135410308838e-06\n",
      "Loss:  2.433856423067482e-07\n",
      "Epoch:  53\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  2.3599714040756226e-06\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  1.862645149230957e-06\n",
      "Loss:  4.220753908157349e-06\n",
      "Loss:  2.5872141122817993e-06\n",
      "Loss:  1.2665987014770508e-06\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  2.1494925022125244e-06\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  1.3373792171478271e-06\n",
      "Loss:  7.152557373046875e-07\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  1.471489667892456e-06\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  2.5779008865356445e-06\n",
      "Loss:  1.3262033462524414e-06\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  1.5906989574432373e-06\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  1.4808028936386108e-06\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  1.173466444015503e-06\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  8.866190910339355e-07\n",
      "Loss:  7.152557373046875e-07\n",
      "Loss:  1.3578683137893677e-06\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  1.3299286365509033e-06\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  1.862645149230957e-07\n",
      "Loss:  6.258487701416016e-07\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  5.289912223815918e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  9.462237358093262e-07\n",
      "Loss:  4.51505184173584e-06\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  1.3615936040878296e-06\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  2.86102294921875e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.078467845916748\n",
      "Validation accuracy: 81%\n",
      "Loss:  6.891787052154541e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  2.1476298570632935e-06\n",
      "Loss:  8.158385753631592e-07\n",
      "Loss:  3.507360816001892e-06\n",
      "Loss:  6.780028343200684e-07\n",
      "Loss:  1.1902302503585815e-06\n",
      "Loss:  3.084540367126465e-06\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  4.209578037261963e-07\n",
      "Loss:  1.6093254089355469e-06\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  2.175569534301758e-06\n",
      "Loss:  1.3187527656555176e-06\n",
      "Loss:  1.6614794731140137e-06\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.821666955947876e-06\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  5.513429641723633e-07\n",
      "Loss:  4.408881068229675e-06\n",
      "Loss:  9.909272193908691e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  2.1923333406448364e-06\n",
      "Loss:  4.470348358154297e-07\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  1.1138617992401123e-06\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  5.62518835067749e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  5.289912223815918e-07\n",
      "Loss:  9.797513484954834e-07\n",
      "Loss:  9.126961231231689e-07\n",
      "Loss:  9.164214134216309e-07\n",
      "Loss:  9.816139936447144e-07\n",
      "Loss:  3.4086406230926514e-06\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.4835267709022446e-07\n",
      "Epoch:  54\n",
      "Loss:  9.164214134216309e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  2.1420419216156006e-06\n",
      "Loss:  1.9725412130355835e-06\n",
      "Loss:  2.086162567138672e-06\n",
      "Loss:  1.1436641216278076e-06\n",
      "Loss:  4.544854164123535e-07\n",
      "Loss:  7.711350917816162e-07\n",
      "Loss:  1.9259750843048096e-06\n",
      "Loss:  4.954636096954346e-07\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  6.817281246185303e-07\n",
      "Loss:  6.034970283508301e-07\n",
      "Loss:  3.635883331298828e-06\n",
      "Loss:  3.241002559661865e-07\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  2.93925404548645e-06\n",
      "Loss:  8.568167686462402e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  7.338821887969971e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  3.2782554626464844e-07\n",
      "Loss:  2.08243727684021e-06\n",
      "Loss:  1.1436641216278076e-06\n",
      "Loss:  6.668269634246826e-07\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  1.7434358596801758e-06\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  1.0766088962554932e-06\n",
      "Loss:  2.4605542421340942e-06\n",
      "Loss:  9.98377799987793e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  2.0811955891986145e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.093733787536621\n",
      "Validation accuracy: 81%\n",
      "Loss:  7.748603820800781e-07\n",
      "Loss:  1.996755599975586e-06\n",
      "Loss:  6.221234798431396e-07\n",
      "Loss:  8.344650268554688e-07\n",
      "Loss:  8.828938007354736e-07\n",
      "Loss:  8.754432201385498e-07\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  3.563240170478821e-06\n",
      "Loss:  1.6540288925170898e-06\n",
      "Loss:  9.611248970031738e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  4.4330954551696777e-07\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  1.3634562492370605e-06\n",
      "Loss:  2.2780150175094604e-06\n",
      "Loss:  2.1606683731079102e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  2.1830201148986816e-06\n",
      "Loss:  8.791685104370117e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  3.0472874641418457e-06\n",
      "Loss:  1.521781086921692e-06\n",
      "Loss:  4.6938657760620117e-07\n",
      "Loss:  1.0170042514801025e-06\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  7.487833499908447e-07\n",
      "Loss:  7.972121238708496e-07\n",
      "Loss:  3.0919909477233887e-07\n",
      "Loss:  7.487833499908447e-07\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  7.37607479095459e-07\n",
      "Loss:  6.35782896551973e-07\n",
      "Epoch:  55\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  8.046627044677734e-07\n",
      "Loss:  1.341104507446289e-06\n",
      "Loss:  2.1532177925109863e-06\n",
      "Loss:  4.3213367462158203e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  9.5367431640625e-07\n",
      "Loss:  6.444752216339111e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  2.7865171432495117e-06\n",
      "Loss:  1.7620623111724854e-06\n",
      "Loss:  2.086162567138672e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  7.338821887969971e-07\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  1.5273690223693848e-06\n",
      "Loss:  6.407499313354492e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  9.5367431640625e-07\n",
      "Loss:  2.2333115339279175e-06\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  3.3490359783172607e-06\n",
      "Loss:  1.0766088962554932e-06\n",
      "Loss:  1.341104507446289e-06\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  1.0542571544647217e-06\n",
      "Loss:  1.1473894119262695e-06\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  4.3958425521850586e-07\n",
      "Loss:  8.456408977508545e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  3.3155083656311035e-07\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  9.040037980412308e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.1188247203826904\n",
      "Validation accuracy: 81%\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  6.556510925292969e-07\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  1.944601535797119e-06\n",
      "Loss:  2.8312206268310547e-07\n",
      "Loss:  4.3213367462158203e-07\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  1.3932585716247559e-06\n",
      "Loss:  1.30385160446167e-06\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  2.6207417249679565e-06\n",
      "Loss:  1.0356307029724121e-06\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  1.2665987014770508e-06\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  3.166496753692627e-07\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  2.8312206268310547e-07\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  9.872019290924072e-07\n",
      "Loss:  2.8312206268310547e-07\n",
      "Loss:  1.0021030902862549e-06\n",
      "Loss:  9.313225746154785e-07\n",
      "Loss:  3.7997961044311523e-07\n",
      "Loss:  5.029141902923584e-07\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  8.270144462585449e-07\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  9.499490261077881e-07\n",
      "Loss:  2.1345913410186768e-06\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  3.905966877937317e-06\n",
      "Loss:  1.4100223779678345e-06\n",
      "Loss:  1.0927518445669193e-07\n",
      "Epoch:  56\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  1.862645149230957e-07\n",
      "Loss:  6.51925802230835e-07\n",
      "Loss:  6.817281246185303e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.0505318641662598e-06\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  1.8868595361709595e-06\n",
      "Loss:  9.611248970031738e-07\n",
      "Loss:  2.7194619178771973e-07\n",
      "Loss:  1.087784767150879e-06\n",
      "Loss:  3.3527612686157227e-06\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  4.76837158203125e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  6.854534149169922e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  9.723007678985596e-07\n",
      "Loss:  1.1511147022247314e-06\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  2.0638108253479004e-06\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  2.0489096641540527e-06\n",
      "Loss:  5.513429641723633e-07\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  3.762543201446533e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  8.493661880493164e-07\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  2.391636371612549e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.1358907222747803\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  5.736947059631348e-07\n",
      "Loss:  6.556510925292969e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  1.5422701835632324e-06\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  9.238719940185547e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  2.384185791015625e-06\n",
      "Loss:  1.603737473487854e-06\n",
      "Loss:  1.3727694749832153e-06\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  6.92903995513916e-07\n",
      "Loss:  6.295740604400635e-07\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  2.2761523723602295e-06\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  9.126961231231689e-07\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  2.5443732738494873e-06\n",
      "Loss:  6.146728992462158e-07\n",
      "Loss:  3.6135315895080566e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  5.997717380523682e-07\n",
      "Loss:  5.513429641723633e-07\n",
      "Loss:  8.791685104370117e-07\n",
      "Loss:  3.986060619354248e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  7.94728578057402e-07\n",
      "Epoch:  57\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  1.0915100574493408e-06\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.300126314163208e-06\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  1.6987323760986328e-06\n",
      "Loss:  6.370246410369873e-07\n",
      "Loss:  7.674098014831543e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  7.562339305877686e-07\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  1.1138617992401123e-06\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  5.066394805908203e-07\n",
      "Loss:  5.923211574554443e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  7.82310962677002e-07\n",
      "Loss:  1.125037670135498e-06\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.5816261768341064e-06\n",
      "Loss:  1.8049031496047974e-06\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  5.438923835754395e-07\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  4.991888999938965e-07\n",
      "Loss:  3.3527612686157227e-07\n",
      "Loss:  5.62518835067749e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  4.5821070671081543e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  1.4156103134155273e-06\n",
      "Loss:  1.4404456294414558e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.163134813308716\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  4.76837158203125e-07\n",
      "Loss:  4.805624485015869e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  1.605600118637085e-06\n",
      "Loss:  2.0116567611694336e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  1.4603137969970703e-06\n",
      "Loss:  3.8743019104003906e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  1.650303602218628e-06\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  8.67992639541626e-07\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  1.7043203115463257e-06\n",
      "Loss:  1.0095536708831787e-06\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  6.817281246185303e-07\n",
      "Loss:  3.3527612686157227e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  5.513429641723633e-07\n",
      "Loss:  2.3543834686279297e-06\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  4.3213367462158203e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.210719347000122e-06\n",
      "Loss:  3.2782554626464844e-07\n",
      "Loss:  2.25752592086792e-06\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  1.862645149230957e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Epoch:  58\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  4.954636096954346e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  2.5480985641479492e-06\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  4.991888999938965e-07\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  6.48200511932373e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  4.6938657760620117e-07\n",
      "Loss:  3.6135315895080566e-07\n",
      "Loss:  8.046627044677734e-07\n",
      "Loss:  4.954636096954346e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  6.854534149169922e-07\n",
      "Loss:  5.699694156646729e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  1.4360994100570679e-06\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  4.731118679046631e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  6.407499313354492e-07\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  1.4528632164001465e-06\n",
      "Loss:  1.1585652828216553e-06\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  2.878407713069464e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.1685070991516113\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  9.797513484954834e-07\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  3.7401914596557617e-06\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  1.087784767150879e-06\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  4.4330954551696777e-07\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  1.9315630197525024e-06\n",
      "Loss:  4.544854164123535e-07\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  3.650784492492676e-07\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  1.1473894119262695e-06\n",
      "Loss:  1.3075768947601318e-06\n",
      "Loss:  8.158385753631592e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  4.6938657760620117e-07\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  4.6938657760620117e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  5.327165126800537e-07\n",
      "Loss:  4.3213367462158203e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  4.172325134277344e-07\n",
      "Loss:  1.2417633854511223e-07\n",
      "Epoch:  59\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.4808028936386108e-06\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  6.966292858123779e-07\n",
      "Loss:  4.3958425521850586e-07\n",
      "Loss:  3.8743019104003906e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  2.518296241760254e-06\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  2.436339855194092e-06\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.1473894119262695e-06\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  3.501772880554199e-07\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  4.209578037261963e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  3.6135315895080566e-07\n",
      "Loss:  6.183981895446777e-07\n",
      "Loss:  5.066394805908203e-07\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  5.029141902923584e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  9.437401899958786e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.192103385925293\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.7639249563217163e-06\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  7.264316082000732e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  1.0170042514801025e-06\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  1.0337680578231812e-06\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  7.562339305877686e-07\n",
      "Loss:  3.2782554626464844e-07\n",
      "Loss:  6.817281246185303e-07\n",
      "Loss:  1.3150274753570557e-06\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  9.424984455108643e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  2.8386712074279785e-06\n",
      "Epoch:  60\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  1.4416873455047607e-06\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  1.2032687664031982e-06\n",
      "Loss:  5.289912223815918e-07\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  4.544854164123535e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  1.1418014764785767e-06\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  4.544854164123535e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  8.903443813323975e-07\n",
      "Loss:  2.1141022443771362e-06\n",
      "Loss:  1.8421560525894165e-06\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  3.0547380447387695e-07\n",
      "Loss:  1.8874804652568855e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.2005162239074707\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  2.7194619178771973e-06\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  5.401670932769775e-07\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  3.7997961044311523e-07\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  6.92903995513916e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  1.0505318641662598e-06\n",
      "Loss:  8.456408977508545e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  5.364418029785156e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  5.327165126800537e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  3.0547380447387695e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  9.499490261077881e-07\n",
      "Loss:  9.331852197647095e-07\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  1.5534460544586182e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  7.525086402893066e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  8.44399110633276e-08\n",
      "Epoch:  61\n",
      "Loss:  2.6542693376541138e-06\n",
      "Loss:  9.350478649139404e-07\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  5.587935447692871e-07\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  6.92903995513916e-07\n",
      "Loss:  1.1771917343139648e-06\n",
      "Loss:  3.203749656677246e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  4.023313522338867e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  8.456408977508545e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  7.152557373046875e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  9.015202522277832e-07\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  1.1064112186431885e-06\n",
      "Loss:  4.991888999938965e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  8.44399110633276e-08\n",
      "Training accuracy: 100%\n",
      "Loss:  2.219759225845337\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  1.1138617992401123e-06\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  5.811452865600586e-07\n",
      "Loss:  3.725290298461914e-09\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  5.401670932769775e-07\n",
      "Loss:  3.762543201446533e-07\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  1.2628734111785889e-06\n",
      "Loss:  1.0654330253601074e-06\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  4.023313522338867e-07\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  4.470348358154297e-07\n",
      "Loss:  2.261251211166382e-06\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  6.48200511932373e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  3.5390257835388184e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  1.1424223345102291e-07\n",
      "Epoch:  62\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  1.1511147022247314e-06\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  3.5390257835388184e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  4.731118679046631e-07\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  9.57399606704712e-07\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  5.029141902923584e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  1.1362135410308838e-06\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  1.1846423149108887e-06\n",
      "Loss:  1.6354024410247803e-06\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  5.066394805908203e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  7.860362529754639e-07\n",
      "Loss:  3.725290298461914e-09\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  4.1350722312927246e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  6.457169661189255e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.2336769104003906\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  3.501772880554199e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  1.7527490854263306e-06\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  3.3155083656311035e-07\n",
      "Loss:  8.195638656616211e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  5.587935447692871e-07\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  7.599592208862305e-07\n",
      "Loss:  2.1606683731079102e-07\n",
      "Loss:  1.043081283569336e-06\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  5.848705768585205e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  2.7194619178771973e-07\n",
      "Loss:  3.7997961044311523e-07\n",
      "Loss:  1.1511147022247314e-06\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  5.736947059631348e-07\n",
      "Loss:  5.252659320831299e-07\n",
      "Loss:  8.642673492431641e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  5.960464477539062e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  1.8378098332050286e-07\n",
      "Epoch:  63\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  1.2516975402832031e-06\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  3.948807716369629e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  9.723007678985596e-07\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  4.172325134277344e-07\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  4.5821070671081543e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  1.5050172805786133e-06\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  7.37607479095459e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  7.860362529754639e-07\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  5.401670932769775e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  7.487833499908447e-07\n",
      "Loss:  7.053216108943161e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.237172842025757\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  3.8370490074157715e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  7.748603820800781e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  1.4007091522216797e-06\n",
      "Loss:  1.4118850231170654e-06\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  6.631016731262207e-07\n",
      "Loss:  1.0691583156585693e-06\n",
      "Loss:  8.530914783477783e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  1.3560056686401367e-06\n",
      "Epoch:  64\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  1.959502696990967e-06\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  3.166496753692627e-07\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  5.587935447692871e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  8.940696716308594e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.227063179016113e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  5.476176738739014e-07\n",
      "Loss:  2.5331974029541016e-07\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  4.4330954551696777e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  8.754432201385498e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  1.5813857316970825e-06\n",
      "Loss:  4.42067772610244e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.248647928237915\n",
      "Validation accuracy: 81%\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  5.923211574554443e-07\n",
      "Loss:  9.462237358093262e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  4.172325134277344e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  4.842877388000488e-07\n",
      "Loss:  8.270144462585449e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  2.045184373855591e-06\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  7.450580596923828e-07\n",
      "Loss:  3.46451997756958e-07\n",
      "Loss:  4.023313522338867e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  9.499490261077881e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  6.804863801335159e-07\n",
      "Epoch:  65\n",
      "Loss:  1.0095536708831787e-06\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  4.470348358154297e-07\n",
      "Loss:  1.9073486328125e-06\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  5.923211574554443e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  3.166496753692627e-07\n",
      "Loss:  6.332993507385254e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  4.5821070671081543e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  3.762543201446533e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  8.828938007354736e-07\n",
      "Loss:  7.152557373046875e-07\n",
      "Loss:  3.8743019104003906e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.9868215517249155e-08\n",
      "Training accuracy: 100%\n",
      "Loss:  2.268016815185547\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  4.917383193969727e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  3.8370490074157715e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  6.146728992462158e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  1.0505318641662598e-06\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  7.562339305877686e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  4.3585896492004395e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.4603137969970703e-06\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  7.599592208862305e-07\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  6.109476089477539e-07\n",
      "Loss:  5.848705768585205e-07\n",
      "Loss:  4.76837158203125e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  4.967053879312289e-09\n",
      "Epoch:  66\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  8.493661880493164e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  5.848705768585205e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  3.986060619354248e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  3.0919909477233887e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  7.301568984985352e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  5.103647708892822e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  4.0605664253234863e-07\n",
      "Loss:  1.3951212167739868e-06\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  6.370246410369873e-07\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  6.593763828277588e-07\n",
      "Loss:  1.2417633854511223e-07\n",
      "Training accuracy: 100%\n",
      "Loss:  2.2773566246032715\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.043081283569336e-06\n",
      "Loss:  1.0728836059570312e-06\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  6.48200511932373e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  5.438923835754395e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  2.980232238769531e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  1.2330710887908936e-06\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  2.7194619178771973e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  8.009374141693115e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  3.476937706636818e-07\n",
      "Epoch:  67\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  6.183981895446777e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  2.086162567138672e-07\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  7.264316082000732e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  5.438923835754395e-07\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  1.3783574104309082e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  3.241002559661865e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  7.525086402893066e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.67992639541626e-07\n",
      "Loss:  1.7117708921432495e-06\n",
      "Loss:  1.9868215517249155e-08\n",
      "Training accuracy: 100%\n",
      "Loss:  2.28098726272583\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.725290298461914e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  1.601874828338623e-07\n",
      "Loss:  1.039355993270874e-06\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  1.1548399925231934e-06\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  5.811452865600586e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  8.419156074523926e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  1.7881393432617188e-07\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  4.954636096954346e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  4.731118679046631e-07\n",
      "Loss:  5.140900611877441e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  3.91155481338501e-07\n",
      "Loss:  2.123415470123291e-07\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.4404456294414558e-07\n",
      "Epoch:  68\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  2.8312206268310547e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  3.3155083656311035e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  2.0489096641540527e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  5.289912223815918e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  1.1865049600601196e-06\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  3.166496753692627e-07\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  5.438923835754395e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  2.2724270820617676e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  5.587935447692871e-07\n",
      "Loss:  8.977949619293213e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  2.5704503059387207e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  1.226862309522403e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.2955610752105713\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  2.60770320892334e-07\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  3.2782554626464844e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  3.688037395477295e-07\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  6.668269634246826e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  7.003545761108398e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  2.1606683731079102e-07\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  7.674098014831543e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  2.905726432800293e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  1.173466444015503e-06\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  6.258487701416016e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4901161193847656e-08\n",
      "Epoch:  69\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  1.0728836059570312e-06\n",
      "Loss:  1.7508864402770996e-07\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  7.450580596923828e-09\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  5.327165126800537e-07\n",
      "Loss:  2.7939677238464355e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  1.9371509552001953e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  1.2628734111785889e-06\n",
      "Loss:  1.8998980522155762e-07\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  0.0\n",
      "Loss:  1.862645149230957e-07\n",
      "Loss:  3.6135315895080566e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  4.805624485015869e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.825392246246338e-07\n",
      "Loss:  1.9744038581848145e-07\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  7.450580596923828e-07\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  8.940696716308594e-08\n",
      "Training accuracy: 100%\n",
      "Loss:  2.300957679748535\n",
      "Validation accuracy: 81%\n",
      "Loss:  3.427267074584961e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  2.3469328880310059e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  1.4230608940124512e-06\n",
      "Loss:  7.636845111846924e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  2.1979212760925293e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  3.203749656677246e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  5.774199962615967e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  2.8312206268310547e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  2.682209014892578e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  5.513429641723633e-07\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  1.8874804652568855e-07\n",
      "Epoch:  70\n",
      "Loss:  1.7136335372924805e-07\n",
      "Loss:  2.942979335784912e-07\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  3.3527612686157227e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  4.3958425521850586e-07\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  1.255422830581665e-06\n",
      "Loss:  7.450580596923828e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  3.725290298461914e-09\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  1.2293457984924316e-07\n",
      "Loss:  3.0174851417541504e-07\n",
      "Loss:  6.817281246185303e-07\n",
      "Loss:  2.644956111907959e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  1.043081283569336e-07\n",
      "Loss:  3.5762786865234375e-07\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  6.705522537231445e-07\n",
      "Loss:  6.07222318649292e-07\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  2.868473529815674e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Training accuracy: 100%\n",
      "Loss:  2.3128087520599365\n",
      "Validation accuracy: 81%\n",
      "Loss:  2.1606683731079102e-07\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Loss:  5.21540641784668e-08\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  3.725290298461914e-08\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  8.381903171539307e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  6.742775440216064e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  1.0207295417785645e-06\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  2.60770320892334e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.195638656616211e-08\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  1.4528632164001465e-07\n",
      "Loss:  1.1548399925231934e-07\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  2.0116567611694336e-07\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  4.6193599700927734e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  2.2351741790771484e-07\n",
      "Loss:  4.246830940246582e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  5.885958671569824e-07\n",
      "Loss:  2.7567148208618164e-07\n",
      "Loss:  1.6391277313232422e-07\n",
      "Loss:  5.662441253662109e-07\n",
      "Loss:  6.332993507385254e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  5.960464477539063e-08\n",
      "Epoch:  71\n",
      "Loss:  1.6763806343078613e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  5.587935447692871e-08\n",
      "Loss:  2.4959444999694824e-07\n",
      "Loss:  1.1175870895385742e-07\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  2.2351741790771484e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  4.0978193283081055e-08\n",
      "Loss:  1.4156103134155273e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  8.642673492431641e-07\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  9.313225746154785e-08\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  1.341104507446289e-07\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  3.501772880554199e-07\n",
      "Loss:  3.725290298461914e-09\n",
      "Loss:  2.4586915969848633e-07\n",
      "Loss:  4.172325134277344e-07\n",
      "Loss:  2.60770320892334e-08\n",
      "Loss:  2.9802322387695312e-08\n",
      "Loss:  5.029141902923584e-07\n",
      "Loss:  7.078051567077637e-08\n",
      "Loss:  5.62518835067749e-07\n",
      "Loss:  2.384185791015625e-07\n",
      "Loss:  9.685754776000977e-08\n",
      "Loss:  1.2665987014770508e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  6.92903995513916e-07\n",
      "Loss:  4.507601261138916e-07\n",
      "Loss:  2.3096799850463867e-07\n",
      "Loss:  1.4901161193847656e-07\n",
      "Loss:  3.3527612686157227e-07\n",
      "Loss:  1.2218952178955078e-06\n",
      "Training accuracy: 100%\n",
      "Loss:  2.3221802711486816\n",
      "Validation accuracy: 81%\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  8.195638656616211e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  4.805624485015869e-07\n",
      "Loss:  3.5390257835388184e-07\n",
      "Loss:  1.1920928955078125e-07\n",
      "Loss:  1.0803341865539551e-07\n",
      "Loss:  6.705522537231445e-08\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  1.4901161193847656e-08\n",
      "Loss:  1.3560056686401367e-06\n",
      "Loss:  3.3527612686157227e-08\n",
      "Loss:  4.0978193283081055e-07\n",
      "Loss:  1.0058283805847168e-07\n",
      "Loss:  3.129243850708008e-07\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  4.470348358154297e-08\n",
      "Loss:  2.086162567138672e-07\n",
      "Loss:  1.1175870895385742e-08\n",
      "Loss:  2.421438694000244e-07\n",
      "Loss:  1.862645149230957e-08\n",
      "Loss:  8.568167686462402e-08\n",
      "Loss:  1.564621925354004e-07\n",
      "Loss:  6.742775440216064e-07\n",
      "Loss:  8.940696716308594e-08\n",
      "Loss:  7.82310962677002e-08\n",
      "Loss:  1.30385160446167e-07\n",
      "Loss:  1.5273690223693848e-07\n",
      "Loss:  3.390014171600342e-07\n",
      "Loss:  4.842877388000488e-08\n",
      "Loss:  4.6566128730773926e-07\n",
      "Loss:  7.450580596923828e-08\n",
      "Loss:  2.644956111907959e-07\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_accuracy = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    gradients = dict()\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        print('Loss: ', loss.item())\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_loss / datasets.ImageFolder('chest_xray/train', transform=image_transforms['train']).__len__()\n",
    "\n",
    "    model.eval()\n",
    "    print('Epoch: ', epoch)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        print('Loss: ', loss.item())\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == target))\n",
    "        total += data.shape[0]\n",
    "    train_loss = train_loss / datasets.ImageFolder('chest_xray/train', transform=image_transforms['test']).__len__()\n",
    "    train_accuracy = total_correct/total\n",
    "    print('Training accuracy: {:.0%}'.format(total_correct/total))\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        print('Loss: ', loss.item())\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == target))\n",
    "        total += data.shape[0]\n",
    "    test_accuracy = total_correct/total\n",
    "    print('Validation accuracy: {:.0%}'.format(total_correct/total))\n",
    "    td = TrainingData(date_time=datetime.datetime.now())       \n",
    "    td.epoch = epoch\n",
    "    td.train_loss = train_loss\n",
    "    td.test_loss = test_loss\n",
    "    td.train_accuracy = train_accuracy\n",
    "    td.validation_accuracy = test_accuracy\n",
    "    gradients_dict = {k:v.cpu().numpy() for (k,v) in model.state_dict().items()}\n",
    "    td.gradients = pickle.dumps(gradients_dict)\n",
    "    td.save()\n",
    "#     if total_correct/total > 0.8:\n",
    "#         torch.save(model.state_dict(), 'pt/XRP_' + str(time.strftime(\"%Y%m%d_%H%M%S\"))+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
